[{"content":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.\nImagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one. In this setup, each arm provides a random reward from an unknown probability distribution. Our primary objective is to maximize the total reward obtained over a series of plays.\nAs we do not know the probability distributions, a straightforward strategy is to simply select the arm given a uniform distribution; that is, select each arm with the same probability. Over time, we will eventually manage to estimate the true reward probability according to the law of large numbers. But here\u0026rsquo;s the catch: we need to spend enormous time trying out every action. Why not we only focus on the most promising actions given the reward we received so far?\nExploration vs Exploitation We want to play only the good actions; so just keep playing the actions that have given us the best reward so far. However, at first, we do not have information to tell us what the best actions are. We need strategies that exploit what we think are the best actions so far, but still explore other actions.\nNow, the big question is: how much should we exploit and how much should we explore? This is known as the exploration vs exploitation dilemma. It\u0026rsquo;s tricky because we don\u0026rsquo;t have all the information we need. We want to gather enough data to make smart decisions overall while keeping the risks in check. Exploitation means using what we know works best, while exploration involves taking some risks to learn about actions we\u0026rsquo;re not familiar with.\nIn the context of the multi-armed bandit problem, we want exploration strategies that minimize the regret, which is the expected loss from not taking the best action. A zero-regret strategy is a strategy where the average regret of each round approaches zero as the number of rounds approaches infinity. This means, a zero-regret strategy will converge to an optimal strategy given enough rounds.\nBernoulli Bandit We are going to implement several exploration strategies for the simplest multi-armed bandit problem: Bernoulli Bandit. The bandit has $K$ actions. The action produces a reward, $r=1$, with probability $0 \\le \\theta_k \\le 1$, which is unknown to the agent, but fixed over time. The objective of the agent is to minimize regret over a fixed number of action selections, $T$.\n$$ \\rho = T \\theta^* - \\sum_{t=1}^T \\theta_{\\alpha_t} $$\n$$ \\text{where } \\theta^* = \\max_k{\\theta_k} \\text{; and } \\\\theta_{\\alpha_t} \\text{ corresponds to the chosen action } \\alpha_t \\text{ at each step} $$\nclass BernoulliBandit:\rdef __init__(self, n_actions=5):\rself._probs = np.random.random(n_actions)\r@property\rdef action_count(self):\rreturn len(self._probs)\rdef pull(self, action):\rif np.any(np.random.random() \u0026gt; self._probs[action]):\rreturn 0.0\rreturn 1.0\rdef optimal_reward(self):\rreturn np.max(self._probs)\rdef action_value(self, action):\rreturn self._probs[action]\rdef step(self):\rpass\rdef reset(self):\rpass\rThe implementation for each strategy that will be discuss inherits from the AbstractAgent class:\nclass AbstractAgent(metaclass=ABCMeta):\rdef init_actions(self, n_actions):\rself._successes = np.zeros(n_actions)\rself._failures = np.zeros(n_actions)\rself._total_pulls = 0\r@abstractmethod\rdef get_action(self):\rpass\rdef update(self, action, reward):\rself._total_pulls += 1\rif reward == 1:\rself._successes[action] += 1\relse:\rself._failures[action] += 1\r@property\rdef name(self):\rreturn self.__class__.__name__\rEpsilon-greedy The epsilon-greedy strategy is a simple and effective way to balance exploration and exploitation. The parameter $\\epsilon \\in [0,1]$ controls how much the agent explores and how much will it exploit.\nAccording to this strategy, with a small probability $\\epsilon$, the agent takes a random action, but most of the time, with probability $1 - \\epsilon$, the agent will pick the best action learned so far. The best $\\epsilon$ value depends on the particular problem, but typically, values around 0.05 to 0.1 work very well.\nclass EpsilonGreedyAgent(AbstractAgent):\rdef __init__(self, epsilon=0.01):\rself._epsilon = epsilon\rdef get_action(self):\rif np.random.random() \u0026lt; self._epsilon:\rreturn np.random.randint(len(self._successes))\relse:\rreturn np.argmax(self._successes / (self._successes + self._failures + 0.1))\rThe following plot shows the regret for each step, averaged over 10 trials.\nHigher values of epsilon tend to have a higher regret over time. Higher value means more exploration, so the agent spends more time exploring less valuable actions, even though it already has a good estimate of the value of actions. In this particular problem, the epsilon value of 0.05 to 0.1 is a reasonable choice.\nUpper Confidence Bound The epsilon-greedy strategy has no preference for actions and is inefficient in exploration. The agent might explore a bad action which is already been confirmed as a bad action in the past. It would be better to select among actions that are uncertain or have the potential to be optimal. One can come up with an idea of index for each action that represents optimality and uncertainty at the same time. One efficient way to do it is to use the UCB1 algorithm.\nIn each iteration, the agent assesses each available action\u0026rsquo;s potential by calculating a weight ($w_k$) that combines estimates of both optimality and uncertainty.\n$$ w_k = {\\alpha_k \\over \\alpha_k + \\beta_k} + \\sqrt{2 \\log t \\over \\alpha_k + \\beta_k} $$\nThe first term ${\\alpha_k \\over \\alpha_k + \\beta_k}$ represents the estimated success probability (optimality). The second term $\\sqrt{2 \\log t \\over \\alpha_k + \\beta_k}$ represents the uncertainty, encouraging exploration.\nAfter calculating weights for all actions, the agent then will choose with the maximum weight.\nclass UCBAgent(AbstractAgent):\rdef get_action(self):\rpulls = self._successes + self._failures + 0.1\rreturn np.argmax(self._successes / pulls + np.sqrt(2 * np.log(self._total_pulls + 0.1) / pulls))\rIn a static environment, epsilon-greedy might outperform UCB1 initially because epsilon-greedy is straightforward and tends to quickly focus on the arm with the highest estimated mean reward. UCB1, in contrast, might spend more time exploring and being cautious due to its confidence bounds.\nBut, in many real problems, the underlying probability distributions are not static. For example, suppose we employ a recommendation system for streaming content, using multi-armed bandit approach to decide which shows to suggest to users. In this scenario, the reward is measured by user engagement, specifically whether they watch the suggested show. The viewing preferences of our audience may evolve over time, influenced by factors such as trending genres, seasonal changes, and more.\nHere is an example of a nonstationary bandit where the reward probabilities change over time.\nclass DriftingBandit(BernoulliBandit):\rdef __init__(self, n_actions=5, gamma=0.01):\rsuper().__init__(n_actions)\rself._gamma = gamma\rself._successes = None\rself._failures = None\rself._steps = 0\rself.reset()\rdef reset(self):\rself._successes = np.zeros(self.action_count) + 1.0\rself._failures = np.zeros(self.action_count) + 1.0\rself._steps = 0\rdef step(self):\raction = np.random.randint(self.action_count)\rreward = self.pull(action)\rself._step(action, reward)\rdef _step(self, action, reward):\rself._successes = self._successes * (1 - self._gamma) + self._gamma\rself._failures = self._failures * (1 - self._gamma) + self._gamma\rself._steps += 1\rself._successes[action] += reward\rself._failures[action] += 1.0 - reward\rself._probs = np.random.beta(self._successes, self._failures)\rWe can see from the plot how the reward probabilities change over time.\nUCB1 shines in a changing environment because of its ability to adapt. As the distribution of rewards changes over time, UCB1 continues to explore arms with uncertain estimates, preventing it from getting stuck on a suboptimal arm.\nThompson Sampling Unlike the UCB1 algorithm, Thompson Sampling incorporates the actual distribution of rewards by sampling from a Beta distribution for each action. The Beta distribution is a flexible choice, as it is defined on the interval $[0, 1]$, making it suitable for representing probabilities.\nIn each iteration, the algorithm samples from a Beta distribution for each available action. These samples provide estimates of the true success probability for each action. The algorithm then selects the action with the highest sampled value. This approach allows Thompson Sampling to adapt to the true underlying distribution of rewards and make more informed decisions over time.\nclass ThompsonSamplingAgent(AbstractAgent):\rdef get_action(self):\rreturn np.argmax(np.random.beta(self._successes + 1, self._failures + 1))\rFrom these comparison plots, we can see that Thompson Sampling performs really well compared to epsilon-greedy and UCB1.\nIn a static environment, the algorithm continuously refines its probability distributions based on observed outcomes. As it converges to the true underlying distribution, the algorithm becomes adept at exploiting the arm with the highest expected reward.\nIn a dynamic environment, its ability to update beliefs in a Bayesian manner allows it to swiftly adapt to changes in the reward distribution.\n","permalink":"https://outlierblog.me/posts/multibandits/","summary":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.\nImagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one.","title":"Multi-Armed Bandit Problem and Its Solutions"},{"content":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return. RL methods are designed to help the agent learn and improve its behaviors to achieve its objectives.\nTo better understand RL, we need to explore some key terms such as states and observations, action spaces, policies, trajectories, different ways of measuring return, the optimization problem in RL, and value functions. By understanding these fundamental concepts, we build a strong foundation to understand how RL methods work and how they can be used to solve many different real-life problems.\nStates and Observations In RL, a state represents a complete description of the world at a given point in time. It contains all the information necessary to understand the current state of the environment, including any hidden information. On the other hand, an observation provides a partial description of the state and may omit certain details. In deep RL, observations are often represented as real-valued vectors, matrices, or higher-order tensors. For example, a visual observation can be represented by an RGB matrix of pixel values, while the state of a robot can be represented by its joint angles and velocities.\nAction Spaces Different environments allow different kinds of actions to be taken by the agent. The set of valid actions is referred to as the action space. In RL, there are two main types of action spaces: discrete and continuous.\nIn discrete action spaces, such as Atari games or the game of chess, only a finite number of moves are available to the agent. The agent selects an action from a predefined set of discrete choices.\nIn continuous action spaces, such as controlling a robot in the physical world, actions are represented as real-valued vectors. This allows for a wide range of precise actions to be taken by the agent.\nPolicies A policy is a rule or strategy used by an agent to decide what actions to take in a given state. In RL, policies can be deterministic or stochastic.\nA deterministic policy selects a single action for a given state:\n$$ a_t = \\mu (s_t) $$\nThe action is directly determined by the policy\u0026rsquo;s output, which can be a computable function based on a set of parameters, such as the weights and biases of a neural network.\nA stochastic policy selects actions probabilistically:\n$$ a_t \\sim \\pi( \\cdot \\mid s_t) $$\nThe policy outputs a probability distribution over actions for a given state, and the agent samples an action based on this distribution. Stochastic policies are often used when exploring different actions or dealing with uncertainty.\nIn deep RL, parameterized policies are commonly used, where the outputs are computable functions that depend on a set of parameters. These policies can be represented by neural networks, allowing for flexible and expressive policy representations.\nTrajectories A trajectory, also known as an episode or rollout, is a sequence of states and actions experienced by an agent in the environment. It captures the agent\u0026rsquo;s interactions with the environment over a specific period. A trajectory can be represented as follows:\n$$ \\tau = (s_0, a_0, s_1, a_1, \u0026hellip;) $$\nThe very first state of the trajectory, $s_0$, is randomly sampled from the start-state distribution, denoted as $s_0 \\sim \\rho_0 (\\cdot) $. The state transitions in an environment can be either deterministic or stochastic.\nIn deterministic state transitions, the next state, $s_{t+1}$, is solely determined by the current state and action:\n$$ s_{t+1} = f(s_t, a_t) $$\nIn stochastic state transitions, the next state, $s_{t+1}$, is sampled from a transition probability distribution:\n$$ s_{t+1} \\sim P(\\cdot|s_t, a_t) $$\nRewards and Return The reward function plays a crucial role in RL. It quantifies the immediate desirability or quality of a particular state-action-state transition. The reward function depends on the current state, the action taken, and the next state:\nThe agent\u0026rsquo;s goal is to maximize the cumulative reward over a trajectory, denoted as $R(\\tau)$. There are different types of returns in RL:\nFinite-horizon undiscounted return represents the sum of rewards obtained within a fixed window of steps:\n$$ R(\\tau) = \\sum^T_{t=0}r_t $$\nInfinite-horizon discounted return represents the sum of all rewards obtained by the agent, but discounted based on how far they are obtained in the future:\n$$ R(\\tau) = \\sum^\\infty_{t=0} \\gamma^tr_t, $$\nwhere $\\gamma \\in (0, 1)$\nThe RL Problem The primary objective in RL is to find a policy that maximizes the expected return when the agent acts according to it. Suppose the environment transitions and the policy are stochastic. In that case, the probability of a T-step trajectory can be expressed as:\n$$ P(\\tau | \\pi) = \\rho_0(s_0) \\prod_{t=0}^{T - 1} P(s_{t+1}|s_t, a_t) \\pi(a_t|s_t) $$\nThe expected return, or objective function, can be defined as:\n$$ J(\\pi) = \\int_\\tau P(\\tau | \\pi)R(\\tau) = E_{\\tau \\sim \\pi}[R(\\tau)] $$\nThe central optimization problem in RL is to find the optimal policy, denoted as π*:\n$$ \\pi^*=\\arg \\max_\\pi J(\\pi) $$\nValue Functions Value functions provide estimates of the expected return associated with states or state-action pairs. There are four main types of value functions:\nThe on-policy value function, $V^\\pi(s)$, estimates the expected return if we start in state s and always act according to policy $\\pi$:\n$$V ^\\pi(s) = E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe on-policy action-value function, $Q^\\pi(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to policy $\\pi$:\n$$ ^\\pi(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a] $$\nThe optimal value function, $V^*(s)$, estimates the expected return if we start in state s and always act according to the optimal policy:\n$$ V^\\pi(s) = \\max_\\pi E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe optimal action-value function, $Q^*(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to the optimal policy:\n$$ Q^\\pi(s,a) = \\max_\\pi E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a $$\nBellman Equations All four value functions above satisfy self-consistency equations known as the Bellman equations. These equations describe the relationship between the value of a state or state-action pair and the value of subsequent states.\nThe Bellman equations for on-policy value functions are:\n$$ V^\\pi(s)=E_{a \\sim \\pi, s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma V^\\pi(s\u0026rsquo;)], $$\n$$ Q^\\pi(s,a)=E_{s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma E_{a\u0026rsquo; \\sim \\pi}[Q^\\pi(s\u0026rsquo;, a\u0026rsquo;)] $$\nThe Bellman equations for optimal value functions are:\n$$ V^\\ast(s)=\\max_a E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma V^*(s\u0026rsquo;)], $$\n$$ Q^\\ast(s,a)= E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma \\max_a\u0026rsquo; Q^*(s\u0026rsquo;, a\u0026rsquo;)] $$\nThe crucial difference between the Bellman equations for on-policy value functions and optimal value functions is the absence or presence of the maximization over actions. The inclusion of this maximization reflects the fact that the agent must select the action that leads to the highest value in order to act optimally.\nAdvantage Functions In RL, there are situations where we are interested in understanding not just how good an action is in absolute terms but how much better it is compared to other available actions on average. The advantage function captures the relative advantage of taking a specified action in a state compared to randomly selecting an action. It can be defined as:\n$$ A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s) $$\nThe advantage function provides insights into the superiority of a specific action in a given state, considering the current policy\u0026rsquo;s performance.\nReferences [1] OpenAI Spinning Up. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n","permalink":"https://outlierblog.me/posts/key_concepts_rl/","summary":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return.","title":"Key Concepts In (Deep) Reinforcement Learning"},{"content":"","permalink":"https://outlierblog.me/projects/","summary":"","title":"Projects"}]