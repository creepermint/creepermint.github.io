[{"content":"Introduction In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?\nThat\u0026rsquo;s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs. By effectively segmenting customers, businesses gain valuable insights into their target audience, enabling them to tailor their marketing efforts to cater to the specific desires and preferences of each segment.\nCustomer segmentation allows businesses to go beyond a one-size-fits-all approach and dive deeper into understanding their customers\u0026rsquo; unique identities. By identifying similarities and patterns within different customer groups, organizations can develop targeted strategies that resonate on a more personal level.\nMoreover, segmentation enables businesses to allocate their resources more efficiently. Instead of scattering marketing efforts across the entire customer base, they can focus on specific segments that offer the greatest potential for growth and profitability. This approach maximizes the impact of marketing initiatives, ensuring that resources are utilized effectively to yield the best possible results.\nOne of the most widely used techniques for customer segmentation is K-means clustering. In this blog post, we will explore K-means clustering in more detail. We will learn about the assumptions of K-means clustering, how to apply it using Python, and some of the limitations of the algorithm. By the end of this post, you will have a clear understanding of what K-means clustering is and how to use it in your own customer segmentation projects.\nWhat is K-Means Clustering? K-means clustering is an unsupervised machine learning algorithm that groups data points into a predetermined number of clusters. It aims to group data points into distinct clusters based on their similarity. The \u0026ldquo;K\u0026rdquo; in K-means represents the number of clusters we want to create.\nThe algorithm works by iteratively assigning data points to the nearest cluster centroid and updating the centroids based on the newly formed clusters. It operates based on the assumption that similar data points should be closer to each other and that the centroid represents the center or average of the data points within its cluster.\nHere\u0026rsquo;s a simplified step-by-step explanation of how the K-means clustering algorithm works:\n Randomly select K centroids, which act as the initial centers of the clusters. Assign each data point to the nearest centroid based on a distance metric, often the Euclidean distance. Recalculate the centroids by taking the average of the data points within each cluster. Iterate the process of assignment and updating until convergence. Convergence occurs when the centroids no longer significantly change or a predefined number of iterations is reached. Once convergence is reached, the algorithm assigns each data point to its corresponding cluster based on the final centroids.  The outcome of K-means clustering is a partitioning of the data into K distinct clusters, where data points within the same cluster are more similar to each other than to those in other clusters.\nK-means clustering is widely used due to its simplicity, scalability, and interpretability. However, it is essential to choose the appropriate number of clusters (K) and pre-process the data adequately to obtain meaningful results. Additionally, it is worth noting that the algorithm assumes that clusters are spherical and have equal variance, which may not always hold true in real-world scenarios.\nK-Means Clustering in Python In this tutorial, we will be using Mall Customer Segmentation Data from Kaggle. This dataset contains some basic data about customers like age, gender, annual income and spending score. For this project we will only be using the numerical features (age, annual income, and spending score).\nData Preparation import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from kneed import KneeLocator from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler  # Read the CSV file containing customer data into a DataFrame df = pd.read_csv('data/Mall_Customers.csv') # Select the desired features (Age, Annual Income, and Spending Score) X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]  Now, lets visualize the distribution of the features with histograms\n# Create a 2x2 grid of subplots fig, axs = plt.subplots(2, 2) # Flatten the subplots array into a 1D array axs = axs.ravel() # Iterate over the columns of the X DataFrame and their corresponding subplot axes for i, col in enumerate(X.columns): # Plot a histogram of the current column on the corresponding subplot axis sns.histplot(X[col], ax=axs[i]) # Set the title of the subplot to indicate the column name axs[i].set_title('Histogram of ' + col) # Adjust the layout of the subplots to prevent overlapping plt.tight_layout() # Display the plot plt.show()  Since the features are on different scales, it is necessary to scale them to have zero mean and unit variance. We will use the StandardScaler from scikit-learn to perform this task.\n# Create a StandardScaler object scaler = StandardScaler() # Scale the data using the fit_transform method X_scaled = scaler.fit_transform(X)  Determining the Optimal Number of Clusters The next step is to determine the optimal number of clusters for the segmentation. For this, we will be utilizing the elbow method. The basic idea behind the elbow method is that increasing the number of clusters will result in a decrease in the within-cluster sum of squared distances (WCSS). Still, at some point, the decrease will no longer be significant enough to justify the increase in the number of clusters.\nThe elbow method works by plotting the WCSS against the number of clusters and finding the point where the decrease in WCSS slows down, creating an elbow-like shape in the plot. This elbow-like shape indicates the optimal number of clusters, with the region before the elbow being under-fitting and the region after being over-fitting.\nThe kneed library provides a simple and convenient way to determine the optimal number of clusters for a k-means clustering algorithm, without requiring manual inspection of the plot of WCSS against the number of clusters. The library provides a KneeLocator function that can be used to find the knee or elbow point in a given set of data, which can then be used to determine the optimal number of clusters.\n# compute WCSS for different values of k wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters=i, random_state=42) kmeans.fit(X_scaled) wcss.append(kmeans.inertia_) # plot WCSS vs no. of clusters sns.lineplot(x=range(1, 11), y=wcss) plt.title('Elbow Method') plt.xlabel('Number of clusters') plt.ylabel('WCSS') # find the knee location knee = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing') # plot the knee location plt.vlines(knee.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed') plt.scatter(knee.knee, knee.knee_y, color='red', s=30) plt.show()  The elbow method plot shows that the optimal number of clusters is 4, where the decrease in the within-cluster sum of squares (WCSS) is minimal.\nKMeans Clustering Then, we instantiate a KMeans object with 4 clusters and fit it to the scaled data.\n# Instantiate a KMeans object with 4 clusters and a random state of 42 kmeans = KMeans(n_clusters=4, random_state=42) # Fit the K-means model to the scaled data and obtain the cluster labels for each data point y_kmeans = kmeans.fit_predict(X_scaled)  Cluster Analysis To effectively analyze and interpret the results of the clustering process, it is essential to integrate the cluster labels obtained from the K-means algorithm with the original dataset. This integration allows us to gain a comprehensive understanding of how each data point is assigned to a specific cluster, providing valuable insights into customer segmentation. By examining average age, annual income, and spending score, we can identify distinct traits and tendencies that define each segment.\n# add the cluster labels to the original data df['cluster'] = y_kmeans # analyze the characteristics of each customer segment segment_summary = df.groupby('cluster').agg({ 'Age': 'mean', 'Annual Income (k$)': 'mean', 'Spending Score (1-100)': 'mean', 'CustomerID': 'count' }).rename(columns={ 'Age': 'Mean Age', 'Annual Income (k$)': 'Mean Annual Income (k$)', 'Spending Score (1-100)': 'Mean Spending Score (1-100)', 'CustomerID': 'Number of Customers' })  fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection='3d') # Create a 3D scatter plot of the data points ax.scatter3D(df['Age'], df['Annual Income (k$)'], df['Spending Score (1-100)'], c=df['cluster'], cmap='viridis') # Set labels for each axis ax.set_xlabel('Age') ax.set_ylabel('Annual Income (k$)') ax.set_zlabel('Spending Score (1-100)') # Set a title for the plot plt.title('K-means Clustering: 3D Plot') ax.set_box_aspect(aspect=None, zoom=0.9) plt.show()  We can now analyze and interpret the clustering results. We can explore the characteristics of each cluster, and evaluate their distinctiveness.\nFor this tutorial, we can label these 4 clusters as follows:\n Conservative Spenders: This segment includes customers with a higher average age and annual income, but a lower spending score. They may be more conservative with their spending and focused on saving their money. Young and Wealthy: This segment includes customers with a lower average age and a high annual income and spending score. They are likely younger, wealthier customers who are more likely to spend on luxury goods and experiences. Budget Shoppers: This segment includes customers with a lower average age and annual income, but a relatively high spending score. They may be focused on getting the most for their money and finding deals and discounts. Middle-Aged Moderates: This segment includes customers with a higher average age and a moderate annual income and spending score. They may be more conservative with their spending than the Young and Wealthy segment, but still have more disposable income than the Budget Shoppers segment.  This is just an example on how to interpret the clusters. Remember that the interpretation of the clusters is subjective and may require domain knowledge or further analysis.\nConclusion In this blog post, we learned about customer segmentation using K-means clustering in Python. We explored the significance of understanding customers\u0026rsquo; preferences and behaviors. Through the implementation of K-means clustering, we discovered a practical and widely-used technique to cluster customers effectively. We examined the step-by-step process of applying K-means clustering in Python. By preprocessing the data, choosing the appropriate number of clusters, and utilizing the K-means algorithm, we obtained valuable insights into our customer segments.\nIn conclusion, customer segmentation through K-means clustering equips businesses with the tools to unlock the potential within their customer data. By gaining insights into distinct customer groups, organizations can tailor their marketing strategies, make data-driven decisions, and build strong connections with their customers. As the business landscape continues to evolve, harnessing the power of data science and customer segmentation becomes increasingly crucial in achieving sustainable growth and success.\n","permalink":"https://outlierblog.me/posts/customer_segmentation/","summary":"Introduction In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?\nThat\u0026rsquo;s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs.","title":"Customer Segmentation using K-means Clustering in Python"},{"content":"Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.\nMarket Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors. By identifying patterns and relationships between products, businesses can improve sales and customer satisfaction.\nIn this blog post, we will explore the use of association rule mining in Market Basket Analysis, with a focus on one of the most popular algorithms - the Apriori Algorithm. We will delve into the inner workings of the algorithm and understand how it can be applied to reveal patterns in customer purchase behavior.\nAssociation Rule Mining One of the key techniques used in Market Basket Analysis is Association Rule Mining. Association Rule Mining is a powerful technique that can be used to identify interesting relationships between items in large datasets. It involves identifying frequent itemsets, which are sets of items that are often purchased or appear together in transactions, and then generating association rules that describe the relationships between these itemsets.\nAn association rule has two parts: an antecedent and a consequent. The antecedent is the set of items that appear on the left-hand side of the rule, while the consequent is the set of items that appear on the right-hand side. The antecedent and consequent are connected by the symbol \u0026ldquo;=\u0026gt;\u0026rdquo; which means \u0026ldquo;implies\u0026rdquo;.\nFor example, consider the following association rule: {🍞, 🍰} =\u0026gt; {☕}. In this rule, {🍞, 🍰} is the antecedent and {☕} is the consequent. This rule implies that customers who purchase bread and cake together are also likely to purchase coffee.\nTo determine which itemsets are frequent, we typically use three metrics: support, confidence, and lift\nSupport Support measures the frequency of occurrence of a particular itemset in the dataset. It is calculated as the number of transactions containing the itemset divided by the total number of transactions in the dataset. A high support value indicates that the itemset occurs frequently in the dataset and is therefore a good candidate for generating association rules.\nFor example, if we have 100 transactions in total and 20 of them include 🥐 and 🥖 together, the support for the association rule \u0026ldquo;🥐 -\u0026gt; 🥖\u0026rdquo; would be 20/100 = 0.2 or 20%.\nConfidence Confidence measures the strength of the relationship between two items in an association rule. It is calculated as the number of transactions containing both items in the rule divided by the total number of transactions containing the first item in the rule. A high confidence value indicates that the second item is likely to be purchased when the first item is purchased.\nFor example, if we have 50 transactions that include 🥐, and 20 of those transactions also include 🥖, the confidence for the association rule \u0026ldquo;🥐 -\u0026gt; 🥖\u0026rdquo; would be 20/50 = 0.4 or 40%.\nLift Lift measures the degree of association between two items in an association rule, relative to the frequency of occurrence of both items. It is calculated as the support of the itemset containing both items divided by the product of the supports of the individual items in the rule. A lift value greater than 1 indicates a positive association between the items, while a value less than 1 indicates a negative association.\nFor example, if the support for \u0026ldquo;🥐 -\u0026gt; 🥖\u0026rdquo; is 0.2 (as in the example above), and the support for 🥐 alone is 0.6 and the support for 🥖 alone is 0.5, then the lift for \u0026ldquo;🥐 -\u0026gt; 🥖\u0026rdquo; would be (0.2) / (0.6 x 0.5) = 0.67.\nApriori Algorithm The Apriori algorithm is widely used in association rule mining because of its efficiency in discovering interesting relationships between items in large datasets. It employs a bottom-up approach to mine frequent itemsets and generates association rules that describe the relationships between them.\nThe Apriori algorithm prunes the search space of itemsets to focus only on those that are frequent, which significantly reduces the computational resources and time required to find frequent itemsets. This pruning helps to avoid the generation of irrelevant rules that would not be useful in practical applications.\nThe algorithm has two phases, where the first phase involves identifying all frequent itemsets that occur above a minimum threshold, also known as the support threshold. The second phase involves generating association rules between these frequent itemsets based on their support and confidence.\nThe algorithm starts by identifying all individual items in the dataset and their frequency of occurrence. It then generates candidate itemsets of length two by joining pairs of frequent items. It continues to generate candidate itemsets of increasing length until no new frequent itemsets are found.\nIn the second phase, the algorithm generates association rules between the frequent itemsets. An association rule is a statement of the form A -\u0026gt; B, where A and B are itemsets. The support of an association rule is the proportion of transactions in the dataset that contain both A and B, while the confidence of the rule is the proportion of transactions containing A that also contain B.\nThe Apriori algorithm uses support and confidence thresholds to filter out weak association rules. A high support threshold ensures that only frequent itemsets are considered, while a high confidence threshold ensures that only strong association rules are generated.\nPython Implementation We will use “The Bread Basket” dataset from Kaggle. The dataset belongs to a bakery located in Edinburgh. The dataset has over 9000 transactions. For this analysis, we will only be using the Transaction and Item columns.\nimport pandas as pd\rfrom mlxtend.frequent_patterns import apriori, association_rules\rdf = pd.read_csv('data/bread basket.csv')\rdf = df[['Transaction', 'Item']]\r The resulting dataframe is pivoted to create a matrix where the rows are transactions, the columns are unique items, and the values are the counts of each item in each transaction.\n# group the df by transaction and item, count the number of each item in each transaction\rbasket = df.groupby(\rby=['Transaction', 'Item']\r)['Item'].count().reset_index(name='Count')\r# pivot the table to have transaction as rows and item names as columns\rbasket = basket.pivot_table(\rindex='Transaction', columns='Item', values='Count', aggfunc='sum').fillna(0).applymap(lambda x: 1 if x \u0026gt; 0 else 0)\r Next, we use the Apriori algorithm from the mlxtend library to generate frequent itemsets. The min_support parameter is set to 0.01, which means that an itemset must appear in at least 1% of all transactions to be considered frequent. After generating frequent itemsets, we use the association_rules function from the same library to generate association rules between the frequent itemsets. The min_threshold parameter is set to 0.5, which means that only association rules with a confidence score of 50% or higher will be considered. Finally, we sort the association rules by lift and select the top 10 rules based on the highest lift score.\n# create frequent itemsets with a support of at least 0.01\rfrequent_itemsets = apriori(basket, min_support=0.01, use_colnames=True)\r# generate association rules with a minimum confidence of 0.5\rassociation_rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\r# print the top 10 association rules by lift\rtop_10_rules = association_rules.sort_values(by='lift', ascending=False).head(10)\rprint(top_10_rules)\r Based on the market basket analysis performed on the dataset, we were able to generate a set of association rules that have a confidence greater than 0.5 and a positive lift value. Among the rules generated, we identified the top 10 rules that were particularly important and interesting.\nTakeaways 1. Market Basket Analysis is a powerful data mining technique that can reveal patterns and relationships between products and customer preferences by analyzing large datasets such as purchase history.\n2. Association Rule Mining is a key technique used in Market Basket Analysis to identify frequent itemsets and generate association rules that describe the relationships between them.\n3. An association rule has two parts: an antecedent and a consequent, which are connected by the symbol \u0026ldquo;=\u0026gt;\u0026rdquo;.\n4. The Apriori algorithm is a widely used algorithm in association rule mining, as it efficiently discovers interesting relationships between items in large datasets.\n5. The Apriori algorithm prunes the search space of itemsets to focus only on those that are frequent, which significantly reduces the computational resources and time required to find frequent itemsets.\n6. The algorithm has two phases: identifying all frequent itemsets that occur above a minimum threshold and generating association rules between these frequent itemsets based on their support and confidence.\n7. Support, confidence, and lift are metrics used to determine which itemsets are frequent and to filter out weak association rules.\n8. A high support value indicates that the itemset occurs frequently in the dataset, while a high confidence value indicates that the second item is likely to be purchased when the first item is purchased.\n9. Lift measures the degree of association between two items in an association rule, relative to the frequency of occurrence of both items. A lift value greater than 1 indicates a positive association between the items, while a value less than 1 indicates a negative association.\n10. The Apriori algorithm uses support and confidence thresholds to filter out weak association rules, ensuring that only frequent and strong association rules are generated.\n","permalink":"https://outlierblog.me/posts/market_basket_analysis/","summary":"Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.\nMarket Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors.","title":"Market Basket Analysis With Apriori Algorithm"},{"content":"","permalink":"https://outlierblog.me/projects/","summary":"","title":"Projects"}]