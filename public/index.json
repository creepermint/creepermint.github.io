[{"content":"The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment. Learning from actual experience doesn\u0026rsquo;t require prior knowledge of the environment\u0026rsquo;s dynamics and can lead to optimal behavior. Learning from simulated experience can also be powerful, requiring only the generation of sample transitions, not the complete probability distributions needed for dynamic programming (DP).\nMonte Carlo Prediction When estimating the value of a state under a policy $\\pi$, Monte Carlo methods typically average the returns observed after visiting that state. This averaging process converges to the expected value as more returns are collected. This concept forms the basis for all Monte Carlo methods.\nFor example, when estimating $v_\\pi(s)$, the value of state $s$ under policy $\\pi$, based on a set of episodes following $\\pi$ and passing through $s$. Each occurrence of state $s$ in an episode is called a visit to $s$. The first-visit MC method estimates $v_\\pi(s)$ as the average of the returns following first visits to $s$, whereas the every-visit MC method averages the returns following all visits to $s$. Both first-visit MC and every-visit MC converge to $v_\\pi(s)$ as the number of visits to $s$tends toward infinity.\nMonte Carlo Estimation For action values, the policy evaluation problem involves estimating $q_\\pi(s, a)$, the expected return when starting in state $s$, taking action $a$, and following policy $\\pi$. The methods for this are similar to those used for state values but now involve state-action pairs.\nOne complication of this method is that many state-action pairs may never be visited. If $\\pi$ is a deterministic policy, then in following $\\pi$, we will observe returns only for one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. This is a serious problem because the purpose of learning action values is to help in choosing among the actions available in each state. To compare alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor.\nFor policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. This is called the assumption of exploring starts.\nMonte Carlo Control Monte Carlo estimation can be applied to control problems to approximate optimal policies. This involves maintaining both an approximate policy and an approximate value function. The value function is iteratively updated to better approximate the value function for the current policy, while the policy is improved with respect to the current value function. These two processes may have opposing effects, but they collectively guide both policy and value function towards optimality.\n$$ \\pi_0 \\xrightarrow{\\text{E}} q_{\\pi0} \\xrightarrow{\\text{I}} \\pi_1 \\xrightarrow{\\text{E}} q_{\\pi1} \\xrightarrow{\\text{I}} \\pi_2 \\xrightarrow{\\text{E}} \u0026hellip; \\xrightarrow{\\text{I}} \\pi_* \\xrightarrow{\\text{E}} q_* $$\nwhere $\\xrightarrow{\\text{E}}$ denotes a complete policy evaluation and $\\xrightarrow{\\text{I}}$ denotes a complete policy improvement. Policy evaluation is done as described in the preceding section. Policy improvement is done by making the policy greedy with respect to the current value function. For any action-value function $q$, the corresponding greedy policy is the one that deterministically chooses an action with maximum action-value.\n$$ \\pi(s) = \\arg \\max_a q(s, a) $$\nPolicy improvement then can be done by constructing each $\\pi_{k+1}$ as the greedy policy with respect to $q_{\\pi_k}$. The policy improvement theorem then applies to $\\pi_k$ and $\\pi_{k+1}$ because, for all $s \\in S$,\n$$ \\begin{align} q_{\\pi_k}(s, \\pi_{k+1}(s)) \u0026amp;= q_{\\pi_k}(s, \\arg \\max_a q_{\\pi_k}(s, a)) \\\\ \u0026amp;= \\max_a q_{\\pi_k}(s, a) \\\\ \u0026amp;\\geq q_{\\pi_k}(s, \\pi_k(s)) \\\\ \u0026amp;\\geq v_{\\pi_k}(s) \\end{align} $$\nThe policy improvement theorem ensures that each updated policy ($\\pi_{k+1}$) is uniformly better or equally good as the previous one ($\\pi_k$). This process continues until convergence to the optimal policy and optimal value function is achieved.\nControl without Exploring Starts Control without the need for exploring initial states is a goal in reinforcement learning to avoid relying on the impractical assumption of starting from all possible initial conditions. Achieving this entails ensuring that the agent continues to select all possible actions throughout its learning process. There are two principal approaches to guaranteeing this: on-policy methods and off-policy methods. On-policy methods aim to evaluate or improve the policy used to make decisions, while off-policy methods involve evaluating or improving a policy that differs from the one used to collect the data.\nIn on-policy methods, the policy is typically \u0026ldquo;soft,”, meaning that $\\pi(a \\mid s) \\gt 0$ for all $s \\in S$ and $a \\in A(s)$. Over time, the policy gradually transitions closer to a deterministic optimal policy. An example of a soft policy is the $\\epsilon\\text{-greedy}$ policies, where most of the time, the agent selects the action with the highest estimated value, but occasionally, with a probability $\\epsilon$, the agent choose random action instead. All non-greedy actions are given the minimal probability of selection, $\\epsilon \\over \\left|A(s)\\right|$ , while the remaining probability, $1 - \\epsilon + {\\epsilon \\over \\left|A(s)\\right|}$, is given to the greedy action.\nOff-policy Prediction via Importance Sampling All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? A more straightforward approach is to employ two policies, one that is being learned (the target policy) and becomes optimal over time, and another that is more exploratory and generates the agent\u0026rsquo;s behavior (the behavior policy). Learning occurs \u0026ldquo;off\u0026rdquo; the target policy, and this overall process is termed off-policy learning.\nWe start by considering the prediction problem, in which both target and behavior policies are fixed. Suppose we wish to estimate $v_\\pi$ or $q_\\pi$, but all we have are episodes following another policy $b$, where $b \\neq \\pi$. In this case, $\\pi$ is the target policy, and $b$ is the behavior policy. In order to use episodes from $b$ to estimate values for $\\pi$, we require that every action taken under $\\pi$ is also taken, at least occasionally, under $b$. This is called the assumption of coverage. It follows from coverage that $b$ must be stochastic in states where it is not identical to $\\pi$. The target policy $\\pi$, on the other hand, may be deterministic.\nAlmost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state $S_t$, the probability of the subsequent state-action trajectory, $A_t, S_{t+1}, A_{t+1}, \u0026hellip;, S_T$, occurring under any policy $\\pi$ is,\n$$ \\begin{align} Pr{A_t, S_{t+1}, A_{t+1}, \u0026hellip;, S_T \\mid S_t, A_{t:T-1} \\sim \\pi} \u0026amp;= \\pi(A_t \\mid S_t)p(S_{t+1} \\mid S_t, A_t)\\pi(A_{t+1} \\mid S_{t+1})\u0026hellip;p(S_T \\mid S_{T-1}, A_{T-1}) \\\\ \u0026amp;= \\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k) \\end{align} $$\nwhere $p$ is the state-transition probability function. Thus, the importance sampling ratio is,\n$$ \\begin{align} \\rho_{t:T-1} \u0026amp;= {\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k) \\over \\prod_{k=t}^{T-1} b(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k)} \\\\ \u0026amp;= \\prod_{k=t}^{T-1}{\\pi(A_k \\mid S_k) \\over b(A_k \\mid S_k)} \\end{align} $$\nRecall that we wish to estimate the expected returns (values) under the target policy, but all we have are returns $G_t$ due to the behavior policy. These returns have the wrong expectation and cannot be averaged to obtain $V_\\pi$. This is where importance sampling comes in. The ratio $\\rho_{t:T-1}$transforms the returns to have the right expected value:\n$$ \\mathbb{E}[\\rho_{t:T-1}G_t \\mid S_t=s] = v_\\pi(s) $$\nThere are two main types of importance sampling: ordinary importance sampling and weighted importance sampling. In ordinary importance sampling, we use the sampled values to estimate expected values or probabilities under the target distribution. Let $\\mathcal{T}(s)$ denote the set of all time steps in which state $s$ is visited. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up through $T(t)$. Then ${G_t}{t \\in \\mathcal{T}(s)}$ are the returns that pertain to state $s$, and ${\\rho{t:T(t)-1}}{t \\in \\mathcal{T}(s)}$ are the importance-sampling ratios. To estimate $v\\pi(s)$, we scale the returns by the ratios and average the results.\n$$ V(S) = {\\sum_{t \\in \\mathcal{T}(s) } \\rho_{t:T(t)-1} G_t \\over \\left|\\mathcal{T}(s)\\right|} $$\nWeighted importance sampling improves upon ordinary importance sampling by introducing weights that adjust for the discrepancies between the importance distribution and the target distribution. These weights ensure that samples drawn from the importance distribution contribute appropriately to the estimate.\n$$ V(S) = {\\sum_{t \\in \\mathcal{T}(s) } \\rho_{t:T(t)-1} G_t \\over \\sum_{t \\in \\mathcal{T}(s) } \\rho_{t:T(t)-1}} $$\nOrdinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one.\nReferences [1] Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto\n","permalink":"https://outlierblog.me/posts/monte_carlo_method/","summary":"The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment.","title":"Monte Carlo Method"},{"content":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.\nImagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one. In this setup, each machine provides a random reward from an unknown probability distribution. The primary objective of the gambler (or agent) is to maximize the total reward obtained over a series of plays.\nAt each trial, the agent faces a crucial tradeoff: exploitation or exploration. In exploration, the agent tries out different arms to gather information about their rewards and estimate their true potential. The more exploration is done, the better the agent can learn about each arm\u0026rsquo;s payoffs. In contrast, exploitation involves making decisions based on the current knowledge to choose the arm expected to yield the highest reward.\nEffectively balancing exploration and exploitation is the key challenge to maximize cumulative rewards over time. If the agent exploits too much, it may miss out on higher-reward arms. On the other hand, excessive exploration could lead to missed opportunities to gain higher rewards from known better arms.\nTo address the multi-armed bandit problem, various strategies and algorithms have been developed, including epsilon-greedy, optimistic initialization, upper confidence bound (UCB), Thompson sampling, and gradient bandit methods. Each of these approaches aims to tackle the exploration-exploitation dilemma and optimize the agent\u0026rsquo;s decision-making.\nEpsilon-Greedy The action value is estimated according to past experience by averaging the rewards associated with the target action $a$ that we have observed so far (up to the current time step t).\n$$ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^t r_\\tau \\mathbb{1}[a_\\tau = a] $$\nwhere $\\mathbb{1}$ is a binary indicator function and $N_t(a)$ is how many times the action a has been selected so far, $N_t(a) = \\sum_{\\tau=1}^t \\mathbb{1}[a_\\tau = a]$.\nThe simplest action that the agent can make is to select one of the actions with the highest estimated value, that is, one of the greedy actions defined as:\n$$ A_t = \\arg \\max_a \\hat{Q}_t(a) $$\nThe greedy action prioritizes exploiting existing knowledge to achieve the maximum immediate reward and does not invest any time in sampling other actions to explore if they could yield better results.\nOne simple alternative to this is to take greedy action most of the time, but with a small probability (say $\\epsilon$), select random action instead. This method is known as the epsilon-greedy method.\n$$ A_t = \\begin{cases} \\arg \\max_a \\hat{Q}_t(a) \u0026amp; \\text{with probability 1 - } \\epsilon \\\\ \\text{a random action} \u0026amp; \\text{with probability } \\epsilon \\ \\end{cases} $$\nOptimistic Initialization The idea behind this method is to initialize the estimated action value, $Q_0(a)$ to a high value, higher than their actual estimated action value. The agent then updates action value by incremental averaging, starting with $N_0(a) \\ge 0$ for all $a \\in \\mathcal{A}$,\n$$ \\hat{Q}_t(A_t) = \\hat{Q}_{t-1}(A_t) + {1 \\over N_t(a)}(r_t - \\hat{Q}_{t-1}(A_t)), \\text{ and} $$ $$ \\hat{Q}_t(a) = \\hat{Q}_{t-1}(a) \\text{ for all } a \\ne A_t $$\nThis method promotes systematic exploration in the initial stages. When the agent chooses actions initially, the rewards received are lower than the starting estimates. Consequently, the agent switches to other actions as it becomes \u0026ldquo;disappointed\u0026rdquo; with the received rewards. This leads to repeated trials of all actions before the value estimates eventually converge.\nUpper Confidence Bound Exploration is important because we are not always sure how accurate our action-value estimates are. Greedy method choose an action that looks best at time $t$, but there might be better options among the other actions. Epsilon-greedy forces us to try non-greedy actions, but it does so randomly, without considering which non-greedy actions are almost as good as the greedy ones or which ones have more uncertainty.\nA better approach would be to choose non-greedy actions based on their potential to be the best choices. This means considering how close their estimates are to the highest possible values and how uncertain those estimates are.\nOne effective way of doing this is to select actions according to:\n$$ A_t = \\arg \\max_a [Q_t(a) + c \\sqrt{\\ln t \\over N_t(a)}] $$\nwhere $\\ln t$ denotes the natural logarithm of t, and the number $c \\gt 0$ control degree of exploration.\nThe square-root term is a measure of the uncertainty in the estimate of action $a$’s value. Each time action $a$ selected, the uncertainty decrease and, conversely, each time action other than $a$ is selected, the uncertainty increase. The use of the natural logarithm means that the increments become progressively smaller over time, but they have no upper limit. As a result, all actions will eventually be chosen, but actions with lower value estimates or those that have been frequently selected before will be picked less frequently as time goes on.\nThompson Sampling Thompson sampling model the uncertainty in the reward distributions of arms using probability distributions, particularly the Beta distribution. Each arm is associated with a Beta distribution that represents the agent\u0026rsquo;s belief or uncertainty about the true mean reward of that arm.\nThe algorithm work as follow:\nFor each arm, initialize the parameters of the Beta distribution based on some prior belief. The choice of prior can influence the algorithm\u0026rsquo;s behavior, but common choices are uniform or optimistic priors. For example, we can set α = 1 and β = 1, where we expect the reward probability to be 50% but we are not very confident.\nAt each time step, sample an expected reward from each arm\u0026rsquo;s $\\text{Beta}(\\alpha_i, \\beta_i)$ distribution independently. the best action is selected among samples: $a_t = \\arg\\max \\tilde{Q}(a)$\nThe sampled reward is used to update the parameters of the corresponding Beta distribution for that arm, incorporating the new information.\n$$ \\alpha_i \\leftarrow \\alpha_i + r_t \\mathbb{1}[a_t = a_i] $$ $$ \\beta_i \\leftarrow \\beta_i + (1-r_t) \\mathbb{1}[a_t = a_i] $$\nContinue the process by selecting arms, pulling, observing rewards, and updating the Beta distributions at each time step.\nThompson sampling implements the idea of probability matching. Because its reward estimations $\\tilde{Q}$ are sampled from posterior distributions, each of these probabilities is equivalent to the probability that the corresponding action is optimal, conditioned on observed history.\nGradient Bandit Algorithm So far, we have explored methods that estimate action values and use those estimates to select actions. Another approach is to consider learning a numerical preference for each action $a$, which we denote as $H_t(a)$. A higher preference value corresponds to a more frequent selection of the action, but the preference itself does not hold any direct reward interpretation.\nInitially, all action preferences are set equally, ensuring that all actions have an equal probability of being chosen. After each step (where action $A_t$ is selected, and reward $R_t$ is received), the action preferences are updated by:\n$$ H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R_t})(1 - \\pi_t(A_t)), \\text{ and} $$ $$ H_{t+1}(a) = H_t(a) - \\alpha(R_t, -\\bar{R_t})\\pi_t(a),\\text{ for all } \\ a \\ne A_t $$\nwhere $\\pi_t(a) = {e^{H_t(a)} \\over \\sum_{b=1}^ke^{H_t(b)}}$ is the probability of taking action $a$ at time $t$, $\\alpha \\gt 0$ is a step-size parameter, and $\\bar{R_t} \\in \\mathbb{R}$ is the average of all the rewards up to but not including time $t$.\nIf the reward ($R_t$) is higher than the baseline ($\\bar{R_t}$), the probability of taking $A_t$ in the future increase and vice versa. The non-selected actions move in the opposite direction.\nReferences [1] RL Course by David Silver - Lecture 9: Exploration and Exploitation\n[2] Stanford CME 241 slide - Multi-Armed Bandits: Exploration versus Exploitation\n[3] Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto\n[4] Lilian Weng - The Multi-Armed Bandit Problem and Its Solutions\n","permalink":"https://outlierblog.me/posts/multibandits/","summary":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.\nImagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one.","title":"Multi-Armed Bandit Problem and Its Solutions"},{"content":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return. RL methods are designed to help the agent learn and improve its behaviors to achieve its objectives.\nTo better understand RL, we need to explore some key terms such as states and observations, action spaces, policies, trajectories, different ways of measuring return, the optimization problem in RL, and value functions. By understanding these fundamental concepts, we build a strong foundation to understand how RL methods work and how they can be used to solve many different real-life problems.\nStates and Observations In RL, a state represents a complete description of the world at a given point in time. It contains all the information necessary to understand the current state of the environment, including any hidden information. On the other hand, an observation provides a partial description of the state and may omit certain details. In deep RL, observations are often represented as real-valued vectors, matrices, or higher-order tensors. For example, a visual observation can be represented by an RGB matrix of pixel values, while the state of a robot can be represented by its joint angles and velocities.\nAction Spaces Different environments allow different kinds of actions to be taken by the agent. The set of valid actions is referred to as the action space. In RL, there are two main types of action spaces: discrete and continuous.\nIn discrete action spaces, such as Atari games or the game of chess, only a finite number of moves are available to the agent. The agent selects an action from a predefined set of discrete choices.\nIn continuous action spaces, such as controlling a robot in the physical world, actions are represented as real-valued vectors. This allows for a wide range of precise actions to be taken by the agent.\nPolicies A policy is a rule or strategy used by an agent to decide what actions to take in a given state. In RL, policies can be deterministic or stochastic.\nA deterministic policy selects a single action for a given state:\n$$ a_t = \\mu (s_t) $$\nThe action is directly determined by the policy\u0026rsquo;s output, which can be a computable function based on a set of parameters, such as the weights and biases of a neural network.\nA stochastic policy selects actions probabilistically:\n$$ a_t \\sim \\pi( \\cdot \\mid s_t) $$\nThe policy outputs a probability distribution over actions for a given state, and the agent samples an action based on this distribution. Stochastic policies are often used when exploring different actions or dealing with uncertainty.\nIn deep RL, parameterized policies are commonly used, where the outputs are computable functions that depend on a set of parameters. These policies can be represented by neural networks, allowing for flexible and expressive policy representations.\nTrajectories A trajectory, also known as an episode or rollout, is a sequence of states and actions experienced by an agent in the environment. It captures the agent\u0026rsquo;s interactions with the environment over a specific period. A trajectory can be represented as follows:\n$$ \\tau = (s_0, a_0, s_1, a_1, \u0026hellip;) $$\nThe very first state of the trajectory, $s_0$, is randomly sampled from the start-state distribution, denoted as $s_0 \\sim \\rho_0 (\\cdot) $. The state transitions in an environment can be either deterministic or stochastic.\nIn deterministic state transitions, the next state, $s_{t+1}$, is solely determined by the current state and action:\n$$ s_{t+1} = f(s_t, a_t) $$\nIn stochastic state transitions, the next state, $s_{t+1}$, is sampled from a transition probability distribution:\n$$ s_{t+1} \\sim P(\\cdot|s_t, a_t) $$\nRewards and Return The reward function plays a crucial role in RL. It quantifies the immediate desirability or quality of a particular state-action-state transition. The reward function depends on the current state, the action taken, and the next state:\nThe agent\u0026rsquo;s goal is to maximize the cumulative reward over a trajectory, denoted as $R(\\tau)$. There are different types of returns in RL:\nFinite-horizon undiscounted return represents the sum of rewards obtained within a fixed window of steps:\n$$ R(\\tau) = \\sum^T_{t=0}r_t $$\nInfinite-horizon discounted return represents the sum of all rewards obtained by the agent, but discounted based on how far they are obtained in the future:\n$$ R(\\tau) = \\sum^\\infty_{t=0} \\gamma^tr_t, $$\nwhere $\\gamma \\in (0, 1)$\nThe RL Problem The primary objective in RL is to find a policy that maximizes the expected return when the agent acts according to it. Suppose the environment transitions and the policy are stochastic. In that case, the probability of a T-step trajectory can be expressed as:\n$$ P(\\tau | \\pi) = \\rho_0(s_0) \\prod_{t=0}^{T - 1} P(s_{t+1}|s_t, a_t) \\pi(a_t|s_t) $$\nThe expected return, or objective function, can be defined as:\n$$ J(\\pi) = \\int_\\tau P(\\tau | \\pi)R(\\tau) = E_{\\tau \\sim \\pi}[R(\\tau)] $$\nThe central optimization problem in RL is to find the optimal policy, denoted as π*:\n$$ \\pi^*=\\arg \\max_\\pi J(\\pi) $$\nValue Functions Value functions provide estimates of the expected return associated with states or state-action pairs. There are four main types of value functions:\nThe on-policy value function, $V^\\pi(s)$, estimates the expected return if we start in state s and always act according to policy $\\pi$:\n$$V ^\\pi(s) = E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe on-policy action-value function, $Q^\\pi(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to policy $\\pi$:\n$$ ^\\pi(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a] $$\nThe optimal value function, $V^*(s)$, estimates the expected return if we start in state s and always act according to the optimal policy:\n$$ V^\\pi(s) = \\max_\\pi E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe optimal action-value function, $Q^*(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to the optimal policy:\n$$ Q^\\pi(s,a) = \\max_\\pi E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a $$\nBellman Equations All four value functions above satisfy self-consistency equations known as the Bellman equations. These equations describe the relationship between the value of a state or state-action pair and the value of subsequent states.\nThe Bellman equations for on-policy value functions are:\n$$ V^\\pi(s)=E_{a \\sim \\pi, s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma V^\\pi(s\u0026rsquo;)], $$\n$$ Q^\\pi(s,a)=E_{s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma E_{a\u0026rsquo; \\sim \\pi}[Q^\\pi(s\u0026rsquo;, a\u0026rsquo;)] $$\nThe Bellman equations for optimal value functions are:\n$$ V^\\ast(s)=\\max_a E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma V^*(s\u0026rsquo;)], $$\n$$ Q^\\ast(s,a)= E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma \\max_a\u0026rsquo; Q^*(s\u0026rsquo;, a\u0026rsquo;)] $$\nThe crucial difference between the Bellman equations for on-policy value functions and optimal value functions is the absence or presence of the maximization over actions. The inclusion of this maximization reflects the fact that the agent must select the action that leads to the highest value in order to act optimally.\nAdvantage Functions In RL, there are situations where we are interested in understanding not just how good an action is in absolute terms but how much better it is compared to other available actions on average. The advantage function captures the relative advantage of taking a specified action in a state compared to randomly selecting an action. It can be defined as:\n$$ A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s) $$\nThe advantage function provides insights into the superiority of a specific action in a given state, considering the current policy\u0026rsquo;s performance.\nReferences [1] OpenAI Spinning Up. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n","permalink":"https://outlierblog.me/posts/key_concepts_rl/","summary":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return.","title":"Key Concepts In (Deep) Reinforcement Learning"},{"content":"","permalink":"https://outlierblog.me/projects/","summary":"","title":"Projects"}]