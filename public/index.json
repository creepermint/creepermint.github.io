[{"content":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.\nImagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one. In this setup, each machine provides a random reward from an unknown probability distribution. The primary objective of the gambler (or agent) is to maximize the total reward obtained over a series of plays.\nAt each trial, the agent faces a crucial tradeoff: exploitation or exploration. In exploration, the agent tries out different arms to gather information about their rewards and estimate their true potential. The more exploration is done, the better the agent can learn about each arm\u0026rsquo;s payoffs. In contrast, exploitation involves making decisions based on the current knowledge to choose the arm expected to yield the highest reward.\nEffectively balancing exploration and exploitation is the key challenge to maximize cumulative rewards over time. If the agent exploits too much, it may miss out on higher-reward arms. On the other hand, excessive exploration could lead to missed opportunities to gain higher rewards from known better arms.\nTo address the multi-armed bandit problem, various strategies and algorithms have been developed, including epsilon-greedy, optimistic initialization, upper confidence bound (UCB), Thompson sampling, and gradient bandit methods. Each of these approaches aims to tackle the exploration-exploitation dilemma and optimize the agent\u0026rsquo;s decision-making.\nEpsilon-Greedy The action value is estimated according to past experience by averaging the rewards associated with the target action $a$ that we have observed so far (up to the current time step t).\n$$ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^t r_\\tau \\mathbb{1}[a_\\tau = a] $$\nwhere $\\mathbb{1}$ is a binary indicator function and $N_t(a)$ is how many times the action a has been selected so far, $N_t(a) = \\sum_{\\tau=1}^t \\mathbb{1}[a_\\tau = a]$.\nThe simplest action that the agent can make is to select one of the actions with the highest estimated value, that is, one of the greedy actions defined as:\n$$ A_t = \\arg \\max_a \\hat{Q}_t(a) $$\nThe greedy action prioritizes exploiting existing knowledge to achieve the maximum immediate reward and does not invest any time in sampling other actions to explore if they could yield better results.\nOne simple alternative to this is to take greedy action most of the time, but with a small probability (say $\\epsilon$), select random action instead. This method is known as the epsilon-greedy method.\n$$ A_t = \\begin{cases} \\arg \\max_a \\hat{Q}_t(a) \u0026amp; \\text{with probability 1 - } \\epsilon \\\\ \\text{a random action} \u0026amp; \\text{with probability } \\epsilon \\ \\end{cases} $$\nOptimistic Initialization The idea behind this method is to initialize the estimated action value, $Q_0(a)$ to a high value, higher than their actual estimated action value. The agent then updates action value by incremental averaging, starting with $N_0(a) \\ge 0$ for all $a \\in \\mathcal{A}$,\n$$ \\hat{Q}_t(A_t) = \\hat{Q}_{t-1}(A_t) + {1 \\over N_t(a)}(r_t - \\hat{Q}_{t-1}(A_t)), \\text{ and} $$ $$ \\hat{Q}_t(a) = \\hat{Q}_{t-1}(a) \\text{ for all } a \\ne A_t $$\nThis method promotes systematic exploration in the initial stages. When the agent chooses actions initially, the rewards received are lower than the starting estimates. Consequently, the agent switches to other actions as it becomes \u0026ldquo;disappointed\u0026rdquo; with the received rewards. This leads to repeated trials of all actions before the value estimates eventually converge.\nUpper Confidence Bound Exploration is important because we are not always sure how accurate our action-value estimates are. Greedy method choose an action that looks best at time $t$, but there might be better options among the other actions. Epsilon-greedy forces us to try non-greedy actions, but it does so randomly, without considering which non-greedy actions are almost as good as the greedy ones or which ones have more uncertainty.\nA better approach would be to choose non-greedy actions based on their potential to be the best choices. This means considering how close their estimates are to the highest possible values and how uncertain those estimates are.\nOne effective way of doing this is to select actions according to:\n$$ A_t = \\arg \\max_a [Q_t(a) + c \\sqrt{\\ln t \\over N_t(a)}] $$\nwhere $\\ln t$ denotes the natural logarithm of t, and the number $c \\gt 0$ control degree of exploration.\nThe square-root term is a measure of the uncertainty in the estimate of action $a$’s value. Each time action $a$ selected, the uncertainty decrease and, conversely, each time action other than $a$ is selected, the uncertainty increase. The use of the natural logarithm means that the increments become progressively smaller over time, but they have no upper limit. As a result, all actions will eventually be chosen, but actions with lower value estimates or those that have been frequently selected before will be picked less frequently as time goes on.\nThompson Sampling Thompson sampling model the uncertainty in the reward distributions of arms using probability distributions, particularly the Beta distribution. Each arm is associated with a Beta distribution that represents the agent\u0026rsquo;s belief or uncertainty about the true mean reward of that arm.\nThe algorithm work as follow:\nFor each arm, initialize the parameters of the Beta distribution based on some prior belief. The choice of prior can influence the algorithm\u0026rsquo;s behavior, but common choices are uniform or optimistic priors. For example, we can set α = 1 and β = 1, where we expect the reward probability to be 50% but we are not very confident.\nAt each time step, sample an expected reward from each arm\u0026rsquo;s $\\text{Beta}(\\alpha_i, \\beta_i)$ distribution independently. the best action is selected among samples: $a_t = \\arg\\max \\tilde{Q}(a)$\nThe sampled reward is used to update the parameters of the corresponding Beta distribution for that arm, incorporating the new information.\n$$ \\alpha_i \\leftarrow \\alpha_i + r_t \\mathbb{1}[a_t = a_i] $$ $$ \\beta_i \\leftarrow \\beta_i + (1-r_t) \\mathbb{1}[a_t = a_i] $$\nContinue the process by selecting arms, pulling, observing rewards, and updating the Beta distributions at each time step.\nThompson sampling implements the idea of probability matching. Because its reward estimations $\\tilde{Q}$ are sampled from posterior distributions, each of these probabilities is equivalent to the probability that the corresponding action is optimal, conditioned on observed history.\nGradient Bandit Algorithm So far, we have explored methods that estimate action values and use those estimates to select actions. Another approach is to consider learning a numerical preference for each action $a$, which we denote as $H_t(a)$. A higher preference value corresponds to a more frequent selection of the action, but the preference itself does not hold any direct reward interpretation.\nInitially, all action preferences are set equally, ensuring that all actions have an equal probability of being chosen. After each step (where action $A_t$ is selected, and reward $R_t$ is received), the action preferences are updated by:\n$$ H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R_t})(1 - \\pi_t(A_t)), \\text{ and} $$ $$ H_{t+1}(a) = H_t(a) - \\alpha(R_t, -\\bar{R_t})\\pi_t(a),\\text{ for all } \\ a \\ne A_t $$\nwhere $\\pi_t(a) = {e^{H_t(a)} \\over \\sum_{b=1}^ke^{H_t(b)}}$ is the probability of taking action $a$ at time $t$, $\\alpha \\gt 0$ is a step-size parameter, and $\\bar{R_t} \\in \\mathbb{R}$ is the average of all the rewards up to but not including time $t$.\nIf the reward ($R_t$) is higher than the baseline ($\\bar{R_t}$), the probability of taking $A_t$ in the future increase and vice versa. The non-selected actions move in the opposite direction.\nReferences [1] RL Course by David Silver - Lecture 9: Exploration and Exploitation\n[2] Stanford CME 241 slide - Multi-Armed Bandits: Exploration versus Exploitation\n[3] Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto\n[4] Lilian Weng - The Multi-Armed Bandit Problem and Its Solutions\n","permalink":"https://outlierblog.me/posts/multibandits/","summary":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.\nImagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one.","title":"Multi-Armed Bandit Problem and Its Solutions"},{"content":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return. RL methods are designed to help the agent learn and improve its behaviors to achieve its objectives.\nTo better understand RL, we need to explore some key terms such as states and observations, action spaces, policies, trajectories, different ways of measuring return, the optimization problem in RL, and value functions. By understanding these fundamental concepts, we build a strong foundation to understand how RL methods work and how they can be used to solve many different real-life problems.\nStates and Observations In RL, a state represents a complete description of the world at a given point in time. It contains all the information necessary to understand the current state of the environment, including any hidden information. On the other hand, an observation provides a partial description of the state and may omit certain details. In deep RL, observations are often represented as real-valued vectors, matrices, or higher-order tensors. For example, a visual observation can be represented by an RGB matrix of pixel values, while the state of a robot can be represented by its joint angles and velocities.\nAction Spaces Different environments allow different kinds of actions to be taken by the agent. The set of valid actions is referred to as the action space. In RL, there are two main types of action spaces: discrete and continuous.\nIn discrete action spaces, such as Atari games or the game of chess, only a finite number of moves are available to the agent. The agent selects an action from a predefined set of discrete choices.\nIn continuous action spaces, such as controlling a robot in the physical world, actions are represented as real-valued vectors. This allows for a wide range of precise actions to be taken by the agent.\nPolicies A policy is a rule or strategy used by an agent to decide what actions to take in a given state. In RL, policies can be deterministic or stochastic.\nA deterministic policy selects a single action for a given state:\n$$ a_t = \\mu (s_t) $$\nThe action is directly determined by the policy\u0026rsquo;s output, which can be a computable function based on a set of parameters, such as the weights and biases of a neural network.\nA stochastic policy selects actions probabilistically:\n$$ a_t \\sim \\pi( \\cdot \\mid s_t) $$\nThe policy outputs a probability distribution over actions for a given state, and the agent samples an action based on this distribution. Stochastic policies are often used when exploring different actions or dealing with uncertainty.\nIn deep RL, parameterized policies are commonly used, where the outputs are computable functions that depend on a set of parameters. These policies can be represented by neural networks, allowing for flexible and expressive policy representations.\nTrajectories A trajectory, also known as an episode or rollout, is a sequence of states and actions experienced by an agent in the environment. It captures the agent\u0026rsquo;s interactions with the environment over a specific period. A trajectory can be represented as follows:\n$$ \\tau = (s_0, a_0, s_1, a_1, \u0026hellip;) $$\nThe very first state of the trajectory, $s_0$, is randomly sampled from the start-state distribution, denoted as $s_0 \\sim \\rho_0 (\\cdot) $. The state transitions in an environment can be either deterministic or stochastic.\nIn deterministic state transitions, the next state, $s_{t+1}$, is solely determined by the current state and action:\n$$ s_{t+1} = f(s_t, a_t) $$\nIn stochastic state transitions, the next state, $s_{t+1}$, is sampled from a transition probability distribution:\n$$ s_{t+1} \\sim P(\\cdot|s_t, a_t) $$\nRewards and Return The reward function plays a crucial role in RL. It quantifies the immediate desirability or quality of a particular state-action-state transition. The reward function depends on the current state, the action taken, and the next state:\nThe agent\u0026rsquo;s goal is to maximize the cumulative reward over a trajectory, denoted as $R(\\tau)$. There are different types of returns in RL:\nFinite-horizon undiscounted return represents the sum of rewards obtained within a fixed window of steps:\n$$ R(\\tau) = \\sum^T_{t=0}r_t $$\nInfinite-horizon discounted return represents the sum of all rewards obtained by the agent, but discounted based on how far they are obtained in the future:\n$$ R(\\tau) = \\sum^\\infty_{t=0} \\gamma^tr_t, $$\nwhere $\\gamma \\in (0, 1)$\nThe RL Problem The primary objective in RL is to find a policy that maximizes the expected return when the agent acts according to it. Suppose the environment transitions and the policy are stochastic. In that case, the probability of a T-step trajectory can be expressed as:\n$$ P(\\tau | \\pi) = \\rho_0(s_0) \\prod_{t=0}^{T - 1} P(s_{t+1}|s_t, a_t) \\pi(a_t|s_t) $$\nThe expected return, or objective function, can be defined as:\n$$ J(\\pi) = \\int_\\tau P(\\tau | \\pi)R(\\tau) = E_{\\tau \\sim \\pi}[R(\\tau)] $$\nThe central optimization problem in RL is to find the optimal policy, denoted as π*:\n$$ \\pi^*=\\arg \\max_\\pi J(\\pi) $$\nValue Functions Value functions provide estimates of the expected return associated with states or state-action pairs. There are four main types of value functions:\nThe on-policy value function, $V^\\pi(s)$, estimates the expected return if we start in state s and always act according to policy $\\pi$:\n$$V ^\\pi(s) = E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe on-policy action-value function, $Q^\\pi(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to policy $\\pi$:\n$$ ^\\pi(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a] $$\nThe optimal value function, $V^*(s)$, estimates the expected return if we start in state s and always act according to the optimal policy:\n$$ V^\\pi(s) = \\max_\\pi E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe optimal action-value function, $Q^*(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to the optimal policy:\n$$ Q^\\pi(s,a) = \\max_\\pi E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a $$\nBellman Equations All four value functions above satisfy self-consistency equations known as the Bellman equations. These equations describe the relationship between the value of a state or state-action pair and the value of subsequent states.\nThe Bellman equations for on-policy value functions are:\n$$ V^\\pi(s)=E_{a \\sim \\pi, s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma V^\\pi(s\u0026rsquo;)], $$\n$$ Q^\\pi(s,a)=E_{s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma E_{a\u0026rsquo; \\sim \\pi}[Q^\\pi(s\u0026rsquo;, a\u0026rsquo;)] $$\nThe Bellman equations for optimal value functions are:\n$$ V^\\ast(s)=\\max_a E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma V^*(s\u0026rsquo;)], $$\n$$ Q^\\ast(s,a)= E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma \\max_a\u0026rsquo; Q^*(s\u0026rsquo;, a\u0026rsquo;)] $$\nThe crucial difference between the Bellman equations for on-policy value functions and optimal value functions is the absence or presence of the maximization over actions. The inclusion of this maximization reflects the fact that the agent must select the action that leads to the highest value in order to act optimally.\nAdvantage Functions In RL, there are situations where we are interested in understanding not just how good an action is in absolute terms but how much better it is compared to other available actions on average. The advantage function captures the relative advantage of taking a specified action in a state compared to randomly selecting an action. It can be defined as:\n$$ A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s) $$\nThe advantage function provides insights into the superiority of a specific action in a given state, considering the current policy\u0026rsquo;s performance.\nReferences [1] OpenAI Spinning Up. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n","permalink":"https://outlierblog.me/posts/key_concepts_rl/","summary":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return.","title":"Key Concepts In (Deep) Reinforcement Learning"},{"content":"","permalink":"https://outlierblog.me/projects/","summary":"","title":"Projects"}]