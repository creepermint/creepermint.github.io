<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Addressing Class Imbalance in Classification | Outlier Blog</title><meta name=keywords content="data-analysis,classification"><meta name=description content="Class imbalance in binary classification refers to a situation where one class (minority class) has significantly fewer instances compared to the other class (majority class). For instance, in a customer attrition prediction task, the minority class could be customers who churned or left, while the majority class is customers who stayed with the company.
Class imbalance can pose a significant challenge in machine learning, particularly for binary classification tasks because most machine learning algorithms assume that the class distribution is balanced."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/class_imbalance/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="Addressing Class Imbalance in Classification"><meta property="og:description" content="Class imbalance in binary classification refers to a situation where one class (minority class) has significantly fewer instances compared to the other class (majority class). For instance, in a customer attrition prediction task, the minority class could be customers who churned or left, while the majority class is customers who stayed with the company.
Class imbalance can pose a significant challenge in machine learning, particularly for binary classification tasks because most machine learning algorithms assume that the class distribution is balanced."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/class_imbalance/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-29T11:31:09+08:00"><meta property="article:modified_time" content="2023-05-29T11:31:09+08:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Addressing Class Imbalance in Classification"><meta name=twitter:description content="Class imbalance in binary classification refers to a situation where one class (minority class) has significantly fewer instances compared to the other class (majority class). For instance, in a customer attrition prediction task, the minority class could be customers who churned or left, while the majority class is customers who stayed with the company.
Class imbalance can pose a significant challenge in machine learning, particularly for binary classification tasks because most machine learning algorithms assume that the class distribution is balanced."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"Addressing Class Imbalance in Classification","item":"https://outlierblog.me/posts/class_imbalance/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Addressing Class Imbalance in Classification","name":"Addressing Class Imbalance in Classification","description":"Class imbalance in binary classification refers to a situation where one class (minority class) has significantly fewer instances compared to the other class (majority class). For instance, in a customer attrition prediction task, the minority class could be customers who churned or left, while the majority class is customers who stayed with the company.\nClass imbalance can pose a significant challenge in machine learning, particularly for binary classification tasks because most machine learning algorithms assume that the class distribution is balanced.","keywords":["data-analysis","classification"],"articleBody":"Class imbalance in binary classification refers to a situation where one class (minority class) has significantly fewer instances compared to the other class (majority class). For instance, in a customer attrition prediction task, the minority class could be customers who churned or left, while the majority class is customers who stayed with the company.\nClass imbalance can pose a significant challenge in machine learning, particularly for binary classification tasks because most machine learning algorithms assume that the class distribution is balanced. When the class distribution is imbalanced, the model may be biased towards the majority class. The model will quickly learn that predicting the most frequent class will lead to great results, leading to poor performance in predicting the minority class.\nIn the case of customer attrition, class imbalance occurs when the number of churned customers is significantly lower than the number of customers who have stayed. For example, if there are 1000 customers, and only 10 have churned, then the class distribution is imbalanced, and the minority class (churned customers) accounts for only 1% of the total dataset.\nIn this article, we will explore the causes and consequences of class imbalance in binary classification tasks and examine various strategies for dealing with class imbalance, such as oversampling, undersampling, and synthetic data generation, and highlight the drawbacks of each approach.\nCauses of Class Imbalance 1. Natural occurrence In some domains, the minority class may naturally occur less frequently. For example, in fraud detection, only a small percentage of transactions are fraudulent.\n2. Data collection Class imbalance can also be caused by the way data is collected. For instance, in a medical study, the number of healthy individuals may significantly outnumber those with a particular disease.\n3. Sampling bias When the data collection process is biased towards a particular group, it can result in class imbalance. For example, if a survey on job satisfaction is conducted only among current employees, the number of employees who left the organization may be significantly lower, leading to class imbalance.\n4. Human biases Human biases, such as racial or gender biases, can also contribute to class imbalance in data.\nConsequences of Class Imbalance 1. Biased models Most machine learning algorithms are designed to optimize accuracy, leading to biased models that perform poorly on the minority class. Models trained on imbalanced data may result in a high number of false negatives, i.e., cases where the minority class is misclassified as the majority class.\n2. Overfitting Overfitting occurs when the model learns the characteristic of the training data rather than the underlying patterns. In imbalanced datasets, overfitting is more likely to occur as the model tends to learn more about the majority class, leading to poor generalization performance.\n3. Misleading evaluation metrics In imbalanced datasets, accuracy can be a misleading evaluation metric. For example, if the majority class accounts for 95% of the dataset, a model that predicts only the majority class will have an accuracy of 95%. However, such a model is useless for practical purposes, as it fails to detect the minority class of interest.\n4. Increased cost of misclassification In some applications, the cost of misclassifying the minority class can be significantly higher than the majority class. For instance, in medical diagnosis, missing a positive diagnosis can have severe consequences for the patient’s health. In such cases, models trained on imbalanced data can lead to significant costs and risks.\n5. Unfairness and biases In some cases, class imbalance can be a result of human biases, leading to unfair or biased models. For instance, if a dataset on job applications contains a disproportionately low number of applications from a particular demographic, models trained on this data may be biased towards the majority group, leading to unfair hiring practices.\nDealing with Class Imbalance Several techniques can be employed to address the class imbalance, such as oversampling the minority class, undersampling the majority class, and using synthetic data generation techniques to create new samples for the minority class. To implement these techniques in Python, the imbalanced-learn library can be used. imbalanced-learn is a powerful Python library that provides a range of tools to handle class imbalance in machine learning tasks. It offers a variety of resampling techniques such as oversampling, undersampling, and hybrid methods, which can be used to balance the class distribution. In addition, the library provides a set of evaluation metrics for measuring the performance of machine learning models on imbalanced datasets, along with methods for ensemble learning and cost-sensitive learning.\nIn this example, we generated a dataset with 1000 samples, 2 features, and 2 classes, with a class imbalance of 90:10 (the majority class has a weight of 0.9, and the minority class has a weight of 0.1). We then converted the numpy array to a pandas DataFrame and plot the class distribution to verify that the dataset is imbalanced.\nimport pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_classification # generate a toy dataset with 1000 samples, 2 features, and 2 classes X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, weights=[0.9, 0.1], random_state=42) # convert the numpy array to a pandas DataFrame df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2']) df['Target'] = y sns.countplot(x='Target', data=df) plt.show()  1. Oversampling Oversampling involves creating more instances of the minority class by randomly duplicating them or creating new synthetic instances. This technique aims to balance the class distribution, making the model less biased towards the majority class. However, oversampling can lead to overfitting, where the model performs well on the training data but poorly on the test data. It can also increase the computational cost, particularly if the dataset is already large.\nfrom imblearn.over_sampling import RandomOverSampler # apply oversampling to the dataset oversampler = RandomOverSampler(random_state=42) X_resampled, y_resampled = oversampler.fit_resample(X, y) # create a new Pandas DataFrame to store the resampled dataset df_resampled = pd.DataFrame(X_resampled, columns=['Feature 1', 'Feature 2']) df_resampled['Target'] = y_resampled # plot the distribution of the resampled dataset sns.countplot(x='Target', data=df_resampled) plt.title('Oversample Minority Class') plt.show()  2. Undersampling Undersampling involves randomly removing instances from the majority class to balance the class distribution. This technique aims to reduce the bias towards the majority class and improve the performance of the model on the minority class. However, undersampling can lead to loss of information and may result in underfitting, where the model is unable to capture the complexity of the problem.\nfrom imblearn.under_sampling import RandomUnderSampler # apply undersampling to the dataset undersampler = RandomUnderSampler(random_state=42) X_resampled, y_resampled = undersampler.fit_resample(X, y) # create a new Pandas DataFrame to store the resampled dataset df_resampled = pd.DataFrame(X_resampled, columns=['Feature 1', 'Feature 2']) df_resampled['Target'] = y_resampled # plot the distribution of the resampled dataset sns.countplot(x='Target', data=df_resampled) plt.title('Undersample Majority Class') plt.show()  3. Synthetic data generation Synthetic data generation involves creating new instances of the minority class using algorithms such as SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN (Adaptive Synthetic Sampling). These algorithms generate synthetic instances by interpolating between existing minority class instances. While synthetic data generation can improve model performance on the minority class, it can also introduce noise and overfitting.\nfrom imblearn.over_sampling import SMOTE, ADASYN # apply SMOTE to the dataset smote = SMOTE(random_state=42) X_resampled_smote, y_resampled_smote = smote.fit_resample(X, y) # create a new Pandas DataFrame to store the SMOTE resampled dataset df_resampled_smote = pd.DataFrame(X_resampled_smote, columns=['Feature 1', 'Feature 2']) df_resampled_smote['Target'] = y_resampled_smote # plot the distribution of the SMOTE resampled dataset sns.countplot(x='Target', data=df_resampled_smote) plt.title('Synthetic Minority Over-sampling Technique (SMOTE)') plt.show() # apply ADASYN to the dataset adasyn = ADASYN(random_state=42) X_resampled_adasyn, y_resampled_adasyn = adasyn.fit_resample(X, y) # create a new Pandas DataFrame to store the ADASYN resampled dataset df_resampled_adasyn = pd.DataFrame(X_resampled_adasyn, columns=['Feature 1', 'Feature 2']) df_resampled_adasyn['Target'] = y_resampled_adasyn # plot the distribution of the ADASYN resampled dataset sns.countplot(x='Target', data=df_resampled_adasyn) plt.title('Adaptive Synthetic (ADASYN)') plt.show()  Conclusion In conclusion, class imbalance is a common problem in binary classification tasks, and it can lead to poor performance of machine learning models. Fortunately, several strategies can be used to deal with this problem, such as oversampling, undersampling, and synthetic data generation. Each approach has its advantages and drawbacks, and the choice of the best strategy will depend on the specific task and the characteristics of the dataset. The imbalanced-learn package provides a comprehensive set of tools for implementing these strategies in Python, along with evaluation metrics for assessing the performance of models on imbalanced datasets. By carefully selecting the most appropriate technique and using these tools, we can improve the accuracy and reliability of our models in real-world applications with imbalanced data.\n","wordCount":"1402","inLanguage":"en","datePublished":"2023-05-29T11:31:09+08:00","dateModified":"2023-05-29T11:31:09+08:00","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/class_imbalance/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Home><span>Home</span></a></li><li><a href=https://outlierblog.me/series/ title=Series><span>Series</span></a></li><li><a href=https://outlierblog.me/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Addressing Class Imbalance in Classification</h1><div class=post-meta><span title="2023-05-29 11:31:09 +0800 +08">May 29, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#causes-of-class-imbalance>Causes of Class Imbalance</a><ul><li><a href=#1-natural-occurrence>1. Natural occurrence</a></li><li><a href=#2-data-collection>2. Data collection</a></li><li><a href=#3-sampling-bias>3. Sampling bias</a></li><li><a href=#4-human-biases>4. Human biases</a></li></ul></li><li><a href=#consequences-of-class-imbalance>Consequences of Class Imbalance</a><ul><li><a href=#1-biased-models>1. Biased models</a></li><li><a href=#2-overfitting>2. Overfitting</a></li><li><a href=#3-misleading-evaluation-metrics>3. Misleading evaluation metrics</a></li><li><a href=#4-increased-cost-of-misclassification>4. Increased cost of misclassification</a></li><li><a href=#5-unfairness-and-biases>5. Unfairness and biases</a></li></ul></li><li><a href=#dealing-with-class-imbalance>Dealing with Class Imbalance</a><ul><li><a href=#1-oversampling>1. Oversampling</a></li><li><a href=#2-undersampling>2. Undersampling</a></li><li><a href=#3-synthetic-data-generation>3. Synthetic data generation</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>Class imbalance in binary classification refers to a situation where one class (minority class) has significantly fewer instances compared to the other class (majority class). For instance, in a customer attrition prediction task, the minority class could be customers who churned or left, while the majority class is customers who stayed with the company.</p><p>Class imbalance can pose a significant challenge in machine learning, particularly for binary classification tasks because most machine learning algorithms assume that the class distribution is balanced. When the class distribution is imbalanced, the model may be biased towards the majority class. The model will quickly learn that predicting the most frequent class will lead to great results, leading to poor performance in predicting the minority class.</p><p>In the case of customer attrition, class imbalance occurs when the number of churned customers is significantly lower than the number of customers who have stayed. For example, if there are 1000 customers, and only 10 have churned, then the class distribution is imbalanced, and the minority class (churned customers) accounts for only 1% of the total dataset.</p><p>In this article, we will explore the causes and consequences of class imbalance in binary classification tasks and examine various strategies for dealing with class imbalance, such as oversampling, undersampling, and synthetic data generation, and highlight the drawbacks of each approach.</p><p></p><h2 id=causes-of-class-imbalance>Causes of Class Imbalance</h2><h3 id=1-natural-occurrence>1. Natural occurrence</h3><p>In some domains, the minority class may naturally occur less frequently. For example, in fraud detection, only a small percentage of transactions are fraudulent.</p><h3 id=2-data-collection>2. Data collection</h3><p>Class imbalance can also be caused by the way data is collected. For instance, in a medical study, the number of healthy individuals may significantly outnumber those with a particular disease.</p><h3 id=3-sampling-bias>3. Sampling bias</h3><p>When the data collection process is biased towards a particular group, it can result in class imbalance. For example, if a survey on job satisfaction is conducted only among current employees, the number of employees who left the organization may be significantly lower, leading to class imbalance.</p><h3 id=4-human-biases>4. Human biases</h3><p>Human biases, such as racial or gender biases, can also contribute to class imbalance in data.</p><p></p><h2 id=consequences-of-class-imbalance>Consequences of Class Imbalance</h2><h3 id=1-biased-models>1. Biased models</h3><p>Most machine learning algorithms are designed to optimize accuracy, leading to biased models that perform poorly on the minority class. Models trained on imbalanced data may result in a high number of false negatives, i.e., cases where the minority class is misclassified as the majority class.</p><h3 id=2-overfitting>2. Overfitting</h3><p>Overfitting occurs when the model learns the characteristic of the training data rather than the underlying patterns. In imbalanced datasets, overfitting is more likely to occur as the model tends to learn more about the majority class, leading to poor generalization performance.</p><h3 id=3-misleading-evaluation-metrics>3. Misleading evaluation metrics</h3><p>In imbalanced datasets, accuracy can be a misleading evaluation metric. For example, if the majority class accounts for 95% of the dataset, a model that predicts only the majority class will have an accuracy of 95%. However, such a model is useless for practical purposes, as it fails to detect the minority class of interest.</p><h3 id=4-increased-cost-of-misclassification>4. Increased cost of misclassification</h3><p>In some applications, the cost of misclassifying the minority class can be significantly higher than the majority class. For instance, in medical diagnosis, missing a positive diagnosis can have severe consequences for the patient’s health. In such cases, models trained on imbalanced data can lead to significant costs and risks.</p><h3 id=5-unfairness-and-biases>5. Unfairness and biases</h3><p>In some cases, class imbalance can be a result of human biases, leading to unfair or biased models. For instance, if a dataset on job applications contains a disproportionately low number of applications from a particular demographic, models trained on this data may be biased towards the majority group, leading to unfair hiring practices.</p><p></p><h2 id=dealing-with-class-imbalance>Dealing with Class Imbalance</h2><p>Several techniques can be employed to address the class imbalance, such as oversampling the minority class, undersampling the majority class, and using synthetic data generation techniques to create new samples for the minority class. To implement these techniques in Python, the <code>imbalanced-learn</code> library can be used. <code>imbalanced-learn</code> is a powerful Python library that provides a range of tools to handle class imbalance in machine learning tasks. It offers a variety of resampling techniques such as oversampling, undersampling, and hybrid methods, which can be used to balance the class distribution. In addition, the library provides a set of evaluation metrics for measuring the performance of machine learning models on imbalanced datasets, along with methods for ensemble learning and cost-sensitive learning.</p><p>In this example, we generated a dataset with 1000 samples, 2 features, and 2 classes, with a class imbalance of 90:10 (the majority class has a weight of 0.9, and the minority class has a weight of 0.1). We then converted the numpy array to a pandas DataFrame and plot the class distribution to verify that the dataset is imbalanced.</p><pre><code class=language-python>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification

# generate a toy dataset with 1000 samples, 2 features, and 2 classes
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                            n_redundant=0, n_clusters_per_class=1, weights=[0.9, 0.1],
                            random_state=42)

# convert the numpy array to a pandas DataFrame
df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])
df['Target'] = y

sns.countplot(x='Target', data=df)
plt.show()
</code></pre><p><img loading=lazy src=images/original.png#center alt="Original dataset"></p><h3 id=1-oversampling>1. Oversampling</h3><p>Oversampling involves creating more instances of the minority class by randomly duplicating them or creating new synthetic instances. This technique aims to balance the class distribution, making the model less biased towards the majority class. However, oversampling can lead to overfitting, where the model performs well on the training data but poorly on the test data. It can also increase the computational cost, particularly if the dataset is already large.</p><pre><code class=language-python>from imblearn.over_sampling import RandomOverSampler

# apply oversampling to the dataset
oversampler = RandomOverSampler(random_state=42)
X_resampled, y_resampled = oversampler.fit_resample(X, y)

# create a new Pandas DataFrame to store the resampled dataset
df_resampled = pd.DataFrame(X_resampled, columns=['Feature 1', 'Feature 2'])
df_resampled['Target'] = y_resampled

# plot the distribution of the resampled dataset
sns.countplot(x='Target', data=df_resampled)
plt.title('Oversample Minority Class')
plt.show()
</code></pre><p><img loading=lazy src=images/oversample.png#center alt="Oversample minority class"></p><h3 id=2-undersampling>2. Undersampling</h3><p>Undersampling involves randomly removing instances from the majority class to balance the class distribution. This technique aims to reduce the bias towards the majority class and improve the performance of the model on the minority class. However, undersampling can lead to loss of information and may result in underfitting, where the model is unable to capture the complexity of the problem.</p><pre><code class=language-python>from imblearn.under_sampling import RandomUnderSampler

# apply undersampling to the dataset
undersampler = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = undersampler.fit_resample(X, y)

# create a new Pandas DataFrame to store the resampled dataset
df_resampled = pd.DataFrame(X_resampled, columns=['Feature 1', 'Feature 2'])
df_resampled['Target'] = y_resampled

# plot the distribution of the resampled dataset
sns.countplot(x='Target', data=df_resampled)
plt.title('Undersample Majority Class')
plt.show()
</code></pre><p><img loading=lazy src=images/undersample.png#center alt="Undersample majority class"></p><h3 id=3-synthetic-data-generation>3. Synthetic data generation</h3><p>Synthetic data generation involves creating new instances of the minority class using algorithms such as SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN (Adaptive Synthetic Sampling). These algorithms generate synthetic instances by interpolating between existing minority class instances. While synthetic data generation can improve model performance on the minority class, it can also introduce noise and overfitting.</p><pre><code class=language-python>from imblearn.over_sampling import SMOTE, ADASYN

# apply SMOTE to the dataset
smote = SMOTE(random_state=42)
X_resampled_smote, y_resampled_smote = smote.fit_resample(X, y)

# create a new Pandas DataFrame to store the SMOTE resampled dataset
df_resampled_smote = pd.DataFrame(X_resampled_smote, columns=['Feature 1', 'Feature 2'])
df_resampled_smote['Target'] = y_resampled_smote

# plot the distribution of the SMOTE resampled dataset
sns.countplot(x='Target', data=df_resampled_smote)
plt.title('Synthetic Minority Over-sampling Technique (SMOTE)')
plt.show()

# apply ADASYN to the dataset
adasyn = ADASYN(random_state=42)
X_resampled_adasyn, y_resampled_adasyn = adasyn.fit_resample(X, y)

# create a new Pandas DataFrame to store the ADASYN resampled dataset
df_resampled_adasyn = pd.DataFrame(X_resampled_adasyn, columns=['Feature 1', 'Feature 2'])
df_resampled_adasyn['Target'] = y_resampled_adasyn

# plot the distribution of the ADASYN resampled dataset
sns.countplot(x='Target', data=df_resampled_adasyn)
plt.title('Adaptive Synthetic (ADASYN)')
plt.show()
</code></pre><p><img loading=lazy src=images/smote.png#center alt=SMOTE>
<img loading=lazy src=images/adasyn.png#center alt=ADASYN></p><h2 id=conclusion>Conclusion</h2><p>In conclusion, class imbalance is a common problem in binary classification tasks, and it can lead to poor performance of machine learning models. Fortunately, several strategies can be used to deal with this problem, such as oversampling, undersampling, and synthetic data generation. Each approach has its advantages and drawbacks, and the choice of the best strategy will depend on the specific task and the characteristics of the dataset. The <code>imbalanced-learn</code> package provides a comprehensive set of tools for implementing these strategies in Python, along with evaluation metrics for assessing the performance of models on imbalanced datasets. By carefully selecting the most appropriate technique and using these tools, we can improve the accuracy and reliability of our models in real-world applications with imbalanced data.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://outlierblog.me/tags/data-analysis/>data-analysis</a></li><li><a href=https://outlierblog.me/tags/classification/>classification</a></li></ul><nav class=paginav><a class=prev href=https://outlierblog.me/posts/elbow_method/><span class=title>« Prev</span><br><span>Using Elbow Method to Determine Optimal Number of Clusters</span></a>
<a class=next href=https://outlierblog.me/posts/hyperparameters/><span class=title>Next »</span><br><span>4 Hyperparameters Optimization Frameworks for Machine Learning</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Addressing Class Imbalance in Classification on twitter" href="https://twitter.com/intent/tweet/?text=Addressing%20Class%20Imbalance%20in%20Classification&url=https%3a%2f%2foutlierblog.me%2fposts%2fclass_imbalance%2f&hashtags=data-analysis%2cclassification"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Addressing Class Imbalance in Classification on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2foutlierblog.me%2fposts%2fclass_imbalance%2f&title=Addressing%20Class%20Imbalance%20in%20Classification&summary=Addressing%20Class%20Imbalance%20in%20Classification&source=https%3a%2f%2foutlierblog.me%2fposts%2fclass_imbalance%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Addressing Class Imbalance in Classification on reddit" href="https://reddit.com/submit?url=https%3a%2f%2foutlierblog.me%2fposts%2fclass_imbalance%2f&title=Addressing%20Class%20Imbalance%20in%20Classification"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Addressing Class Imbalance in Classification on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foutlierblog.me%2fposts%2fclass_imbalance%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Addressing Class Imbalance in Classification on whatsapp" href="https://api.whatsapp.com/send?text=Addressing%20Class%20Imbalance%20in%20Classification%20-%20https%3a%2f%2foutlierblog.me%2fposts%2fclass_imbalance%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Addressing Class Imbalance in Classification on telegram" href="https://telegram.me/share/url?text=Addressing%20Class%20Imbalance%20in%20Classification&url=https%3a%2f%2foutlierblog.me%2fposts%2fclass_imbalance%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>