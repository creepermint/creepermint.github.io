<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Customer Segmentation using K-means Clustering in Python | Outlier Blog</title><meta name=keywords content="data mining,customer analytics,marketing"><meta name=description content="Introduction In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?
That&rsquo;s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/customer_segmentation/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="Customer Segmentation using K-means Clustering in Python"><meta property="og:description" content="Introduction In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?
That&rsquo;s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/customer_segmentation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-25T11:40:44+08:00"><meta property="article:modified_time" content="2023-06-25T11:40:44+08:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Customer Segmentation using K-means Clustering in Python"><meta name=twitter:description content="Introduction In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?
That&rsquo;s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"Customer Segmentation using K-means Clustering in Python","item":"https://outlierblog.me/posts/customer_segmentation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Customer Segmentation using K-means Clustering in Python","name":"Customer Segmentation using K-means Clustering in Python","description":"Introduction In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?\nThat\u0026rsquo;s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs.","keywords":["data mining","customer analytics","marketing"],"articleBody":"Introduction In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?\nThat’s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs. By effectively segmenting customers, businesses gain valuable insights into their target audience, enabling them to tailor their marketing efforts to cater to the specific desires and preferences of each segment.\nCustomer segmentation allows businesses to go beyond a one-size-fits-all approach and dive deeper into understanding their customers’ unique identities. By identifying similarities and patterns within different customer groups, organizations can develop targeted strategies that resonate on a more personal level.\nMoreover, segmentation enables businesses to allocate their resources more efficiently. Instead of scattering marketing efforts across the entire customer base, they can focus on specific segments that offer the greatest potential for growth and profitability. This approach maximizes the impact of marketing initiatives, ensuring that resources are utilized effectively to yield the best possible results.\nOne of the most widely used techniques for customer segmentation is K-means clustering. In this blog post, we will explore K-means clustering in more detail. We will learn about the assumptions of K-means clustering, how to apply it using Python, and some of the limitations of the algorithm. By the end of this post, you will have a clear understanding of what K-means clustering is and how to use it in your own customer segmentation projects.\nWhat is K-Means Clustering? K-means clustering is an unsupervised machine learning algorithm that groups data points into a predetermined number of clusters. It aims to group data points into distinct clusters based on their similarity. The “K” in K-means represents the number of clusters we want to create.\nThe algorithm works by iteratively assigning data points to the nearest cluster centroid and updating the centroids based on the newly formed clusters. It operates based on the assumption that similar data points should be closer to each other and that the centroid represents the center or average of the data points within its cluster.\nHere’s a simplified step-by-step explanation of how the K-means clustering algorithm works:\n Randomly select K centroids, which act as the initial centers of the clusters. Assign each data point to the nearest centroid based on a distance metric, often the Euclidean distance. Recalculate the centroids by taking the average of the data points within each cluster. Iterate the process of assignment and updating until convergence. Convergence occurs when the centroids no longer significantly change or a predefined number of iterations is reached. Once convergence is reached, the algorithm assigns each data point to its corresponding cluster based on the final centroids.  The outcome of K-means clustering is a partitioning of the data into K distinct clusters, where data points within the same cluster are more similar to each other than to those in other clusters.\nK-means clustering is widely used due to its simplicity, scalability, and interpretability. However, it is essential to choose the appropriate number of clusters (K) and pre-process the data adequately to obtain meaningful results. Additionally, it is worth noting that the algorithm assumes that clusters are spherical and have equal variance, which may not always hold true in real-world scenarios.\nK-Means Clustering in Python In this tutorial, we will be using Mall Customer Segmentation Data from Kaggle. This dataset contains some basic data about customers like age, gender, annual income and spending score. For this project we will only be using the numerical features (age, annual income, and spending score).\nData Preparation import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from kneed import KneeLocator from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler  # Read the CSV file containing customer data into a DataFrame df = pd.read_csv('data/Mall_Customers.csv') # Select the desired features (Age, Annual Income, and Spending Score) X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]  Now, lets visualize the distribution of the features with histograms\n# Create a 2x2 grid of subplots fig, axs = plt.subplots(2, 2) # Flatten the subplots array into a 1D array axs = axs.ravel() # Iterate over the columns of the X DataFrame and their corresponding subplot axes for i, col in enumerate(X.columns): # Plot a histogram of the current column on the corresponding subplot axis sns.histplot(X[col], ax=axs[i]) # Set the title of the subplot to indicate the column name axs[i].set_title('Histogram of ' + col) # Adjust the layout of the subplots to prevent overlapping plt.tight_layout() # Display the plot plt.show()  Since the features are on different scales, it is necessary to scale them to have zero mean and unit variance. We will use the StandardScaler from scikit-learn to perform this task.\n# Create a StandardScaler object scaler = StandardScaler() # Scale the data using the fit_transform method X_scaled = scaler.fit_transform(X)  Determining the Optimal Number of Clusters The next step is to determine the optimal number of clusters for the segmentation. For this, we will be utilizing the elbow method. The basic idea behind the elbow method is that increasing the number of clusters will result in a decrease in the within-cluster sum of squared distances (WCSS). Still, at some point, the decrease will no longer be significant enough to justify the increase in the number of clusters.\nThe elbow method works by plotting the WCSS against the number of clusters and finding the point where the decrease in WCSS slows down, creating an elbow-like shape in the plot. This elbow-like shape indicates the optimal number of clusters, with the region before the elbow being under-fitting and the region after being over-fitting.\nThe kneed library provides a simple and convenient way to determine the optimal number of clusters for a k-means clustering algorithm, without requiring manual inspection of the plot of WCSS against the number of clusters. The library provides a KneeLocator function that can be used to find the knee or elbow point in a given set of data, which can then be used to determine the optimal number of clusters.\n# compute WCSS for different values of k wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters=i, random_state=42) kmeans.fit(X_scaled) wcss.append(kmeans.inertia_) # plot WCSS vs no. of clusters sns.lineplot(x=range(1, 11), y=wcss) plt.title('Elbow Method') plt.xlabel('Number of clusters') plt.ylabel('WCSS') # find the knee location knee = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing') # plot the knee location plt.vlines(knee.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed') plt.scatter(knee.knee, knee.knee_y, color='red', s=30) plt.show()  The elbow method plot shows that the optimal number of clusters is 4, where the decrease in the within-cluster sum of squares (WCSS) is minimal.\nKMeans Clustering Then, we instantiate a KMeans object with 4 clusters and fit it to the scaled data.\n# Instantiate a KMeans object with 4 clusters and a random state of 42 kmeans = KMeans(n_clusters=4, random_state=42) # Fit the K-means model to the scaled data and obtain the cluster labels for each data point y_kmeans = kmeans.fit_predict(X_scaled)  Cluster Analysis To effectively analyze and interpret the results of the clustering process, it is essential to integrate the cluster labels obtained from the K-means algorithm with the original dataset. This integration allows us to gain a comprehensive understanding of how each data point is assigned to a specific cluster, providing valuable insights into customer segmentation. By examining average age, annual income, and spending score, we can identify distinct traits and tendencies that define each segment.\n# add the cluster labels to the original data df['cluster'] = y_kmeans # analyze the characteristics of each customer segment segment_summary = df.groupby('cluster').agg({ 'Age': 'mean', 'Annual Income (k$)': 'mean', 'Spending Score (1-100)': 'mean', 'CustomerID': 'count' }).rename(columns={ 'Age': 'Mean Age', 'Annual Income (k$)': 'Mean Annual Income (k$)', 'Spending Score (1-100)': 'Mean Spending Score (1-100)', 'CustomerID': 'Number of Customers' })  segment_summary dataframe\" /\rfig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection='3d') # Create a 3D scatter plot of the data points ax.scatter3D(df['Age'], df['Annual Income (k$)'], df['Spending Score (1-100)'], c=df['cluster'], cmap='viridis') # Set labels for each axis ax.set_xlabel('Age') ax.set_ylabel('Annual Income (k$)') ax.set_zlabel('Spending Score (1-100)') # Set a title for the plot plt.title('K-means Clustering: 3D Plot') ax.set_box_aspect(aspect=None, zoom=0.9) plt.show()  We can now analyze and interpret the clustering results. We can explore the characteristics of each cluster, and evaluate their distinctiveness.\nFor this tutorial, we can label these 4 clusters as follows:\n Conservative Spenders: This segment includes customers with a higher average age and annual income, but a lower spending score. They may be more conservative with their spending and focused on saving their money. Young and Wealthy: This segment includes customers with a lower average age and a high annual income and spending score. They are likely younger, wealthier customers who are more likely to spend on luxury goods and experiences. Budget Shoppers: This segment includes customers with a lower average age and annual income, but a relatively high spending score. They may be focused on getting the most for their money and finding deals and discounts. Middle-Aged Moderates: This segment includes customers with a higher average age and a moderate annual income and spending score. They may be more conservative with their spending than the Young and Wealthy segment, but still have more disposable income than the Budget Shoppers segment.  This is just an example on how to interpret the clusters. Remember that the interpretation of the clusters is subjective and may require domain knowledge or further analysis.\nConclusion In this blog post, we learned about customer segmentation using K-means clustering in Python. We explored the significance of understanding customers’ preferences and behaviors. Through the implementation of K-means clustering, we discovered a practical and widely-used technique to cluster customers effectively. We examined the step-by-step process of applying K-means clustering in Python. By preprocessing the data, choosing the appropriate number of clusters, and utilizing the K-means algorithm, we obtained valuable insights into our customer segments.\nIn conclusion, customer segmentation through K-means clustering equips businesses with the tools to unlock the potential within their customer data. By gaining insights into distinct customer groups, organizations can tailor their marketing strategies, make data-driven decisions, and build strong connections with their customers. As the business landscape continues to evolve, harnessing the power of data science and customer segmentation becomes increasingly crucial in achieving sustainable growth and success.\n","wordCount":"1734","inLanguage":"en","datePublished":"2023-06-25T11:40:44+08:00","dateModified":"2023-06-25T11:40:44+08:00","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/customer_segmentation/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Posts><span>Posts</span></a></li><li><a href=https://outlierblog.me/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://outlierblog.me/notebooks/ title=Notebooks><span>Notebooks</span></a></li><li><a href=https://outlierblog.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Customer Segmentation using K-means Clustering in Python</h1><div class=post-meta><span title="2023-06-25 11:40:44 +0800 +08">June 25, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#what-is-k-means-clustering>What is K-Means Clustering?</a></li><li><a href=#k-means-clustering-in-python>K-Means Clustering in Python</a><ul><li><a href=#data-preparation>Data Preparation</a></li><li><a href=#determining-the-optimal-number-of-clusters>Determining the Optimal Number of Clusters</a></li><li><a href=#kmeans-clustering>KMeans Clustering</a></li><li><a href=#cluster-analysis>Cluster Analysis</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction</h2><p>In this ever-evolving landscape of business and marketing, understanding customers has become more critical than ever before. The ability to unlock the secrets behind their preferences, behaviors, and needs holds the key to delivering personalized experiences and fostering long-lasting relationships. But how can we effectively make sense of a massive pool of customer data?</p><p>That&rsquo;s where customer segmentation comes into play. It is a fundamental aspect of marketing that involves dividing customers into distinct groups or segments based on their shared characteristics, behaviors, or needs. By effectively segmenting customers, businesses gain valuable insights into their target audience, enabling them to tailor their marketing efforts to cater to the specific desires and preferences of each segment.</p><p>Customer segmentation allows businesses to go beyond a one-size-fits-all approach and dive deeper into understanding their customers&rsquo; unique identities. By identifying similarities and patterns within different customer groups, organizations can develop targeted strategies that resonate on a more personal level.</p><p>Moreover, segmentation enables businesses to allocate their resources more efficiently. Instead of scattering marketing efforts across the entire customer base, they can focus on specific segments that offer the greatest potential for growth and profitability. This approach maximizes the impact of marketing initiatives, ensuring that resources are utilized effectively to yield the best possible results.</p><p>One of the most widely used techniques for customer segmentation is K-means clustering. In this blog post, we will explore K-means clustering in more detail. We will learn about the assumptions of K-means clustering, how to apply it using Python, and some of the limitations of the algorithm. By the end of this post, you will have a clear understanding of what K-means clustering is and how to use it in your own customer segmentation projects.</p><h2 id=what-is-k-means-clustering>What is K-Means Clustering?</h2><p>K-means clustering is an unsupervised machine learning algorithm that groups data points into a predetermined number of clusters. It aims to group data points into distinct clusters based on their similarity. The &ldquo;K&rdquo; in K-means represents the number of clusters we want to create.</p><p>The algorithm works by iteratively assigning data points to the nearest cluster centroid and updating the centroids based on the newly formed clusters. It operates based on the assumption that similar data points should be closer to each other and that the centroid represents the center or average of the data points within its cluster.</p><p>Here&rsquo;s a simplified step-by-step explanation of how the K-means clustering algorithm works:</p><ol><li>Randomly select K centroids, which act as the initial centers of the clusters.</li><li>Assign each data point to the nearest centroid based on a distance metric, often the Euclidean distance.</li><li>Recalculate the centroids by taking the average of the data points within each cluster.</li><li>Iterate the process of assignment and updating until convergence. Convergence occurs when the centroids no longer significantly change or a predefined number of iterations is reached.</li><li>Once convergence is reached, the algorithm assigns each data point to its corresponding cluster based on the final centroids.</li></ol><p>The outcome of K-means clustering is a partitioning of the data into K distinct clusters, where data points within the same cluster are more similar to each other than to those in other clusters.</p><p><img loading=lazy src=KMeans.png alt></p><p>K-means clustering is widely used due to its simplicity, scalability, and interpretability. However, it is essential to choose the appropriate number of clusters (K) and pre-process the data adequately to obtain meaningful results. Additionally, it is worth noting that the algorithm assumes that clusters are spherical and have equal variance, which may not always hold true in real-world scenarios.</p><h2 id=k-means-clustering-in-python>K-Means Clustering in Python</h2><p>In this tutorial, we will be using <a href=https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python>Mall Customer Segmentation Data</a> from Kaggle. This dataset contains some basic data about customers like age, gender, annual income and spending score. For this project we will only be using the numerical features (age, annual income, and spending score).</p><h3 id=data-preparation>Data Preparation</h3><pre><code class=language-python>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from kneed import KneeLocator
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
</code></pre><pre><code class=language-python># Read the CSV file containing customer data into a DataFrame
df = pd.read_csv('data/Mall_Customers.csv')

# Select the desired features (Age, Annual Income, and Spending Score)
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
</code></pre><p><img loading=lazy src=head.png alt="First 10 rows of the data"></p><p>Now, lets visualize the distribution of the features with histograms</p><pre><code class=language-python># Create a 2x2 grid of subplots
fig, axs = plt.subplots(2, 2)

# Flatten the subplots array into a 1D array
axs = axs.ravel()

# Iterate over the columns of the X DataFrame and their corresponding subplot axes
for i, col in enumerate(X.columns):
    # Plot a histogram of the current column on the corresponding subplot axis
    sns.histplot(X[col], ax=axs[i])
    
    # Set the title of the subplot to indicate the column name
    axs[i].set_title('Histogram of ' + col)

# Adjust the layout of the subplots to prevent overlapping
plt.tight_layout()

# Display the plot
plt.show()
</code></pre><p><img loading=lazy src=hist.png alt="Histograms of all features"></p><p>Since the features are on different scales, it is necessary to scale them to have zero mean and unit variance. We will use the <code>StandardScaler</code> from <code>scikit-learn</code> to perform this task.</p><pre><code class=language-python># Create a StandardScaler object
scaler = StandardScaler()

# Scale the data using the fit_transform method
X_scaled = scaler.fit_transform(X)
</code></pre><h3 id=determining-the-optimal-number-of-clusters>Determining the Optimal Number of Clusters</h3><p>The next step is to determine the optimal number of clusters for the segmentation. For this, we will be utilizing the elbow method. The basic idea behind the elbow method is that increasing the number of clusters will result in a decrease in the within-cluster sum of squared distances (WCSS). Still, at some point, the decrease will no longer be significant enough to justify the increase in the number of clusters.</p><p>The elbow method works by plotting the WCSS against the number of clusters and finding the point where the decrease in WCSS slows down, creating an elbow-like shape in the plot. This elbow-like shape indicates the optimal number of clusters, with the region before the elbow being under-fitting and the region after being over-fitting.</p><p>The <code>kneed</code> library provides a simple and convenient way to determine the optimal number of clusters for a k-means clustering algorithm, without requiring manual inspection of the plot of WCSS against the number of clusters. The library provides a <code>KneeLocator</code> function that can be used to find the knee or elbow point in a given set of data, which can then be used to determine the optimal number of clusters.</p><pre><code class=language-python># compute WCSS for different values of k
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# plot WCSS vs no. of clusters
sns.lineplot(x=range(1, 11), y=wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')

# find the knee location
knee = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing')

# plot the knee location
plt.vlines(knee.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')
plt.scatter(knee.knee, knee.knee_y, color='red', s=30)

plt.show()
</code></pre><p><img loading=lazy src=cluster.png alt="The elbow method indicates 4 is the optimal number of clusters"></p><p>The elbow method plot shows that the optimal number of clusters is 4, where the decrease in the within-cluster sum of squares (WCSS) is minimal.</p><h3 id=kmeans-clustering>KMeans Clustering</h3><p>Then, we instantiate a KMeans object with 4 clusters and fit it to the scaled data.</p><pre><code class=language-python># Instantiate a KMeans object with 4 clusters and a random state of 42
kmeans = KMeans(n_clusters=4, random_state=42)

# Fit the K-means model to the scaled data and obtain the cluster labels for each data point
y_kmeans = kmeans.fit_predict(X_scaled)
</code></pre><h3 id=cluster-analysis>Cluster Analysis</h3><p>To effectively analyze and interpret the results of the clustering process, it is essential to integrate the cluster labels obtained from the K-means algorithm with the original dataset. This integration allows us to gain a comprehensive understanding of how each data point is assigned to a specific cluster, providing valuable insights into customer segmentation. By examining average age, annual income, and spending score, we can identify distinct traits and tendencies that define each segment.</p><pre><code class=language-python># add the cluster labels to the original data
df['cluster'] = y_kmeans

# analyze the characteristics of each customer segment
segment_summary = df.groupby('cluster').agg({
    'Age': 'mean',
    'Annual Income (k$)': 'mean',
    'Spending Score (1-100)': 'mean',
    'CustomerID': 'count'
}).rename(columns={
    'Age': 'Mean Age',
    'Annual Income (k$)': 'Mean Annual Income (k$)',
    'Spending Score (1-100)': 'Mean Spending Score (1-100)',
    'CustomerID': 'Number of Customers'
})
</code></pre><p><img loading=lazy src=label.png alt="The resulting <code>segment_summary</code> dataframe"></p><pre><code class=language-python>fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Create a 3D scatter plot of the data points
ax.scatter3D(df['Age'], df['Annual Income (k$)'], df['Spending Score (1-100)'], c=df['cluster'], cmap='viridis')

# Set labels for each axis
ax.set_xlabel('Age')
ax.set_ylabel('Annual Income (k$)')
ax.set_zlabel('Spending Score (1-100)')

# Set a title for the plot
plt.title('K-means Clustering: 3D Plot')

ax.set_box_aspect(aspect=None, zoom=0.9)
plt.show()
</code></pre><p><img loading=lazy src=3d.png alt="The 3D plot of the data with clusters"></p><p>We can now analyze and interpret the clustering results. We can explore the characteristics of each cluster, and evaluate their distinctiveness.</p><p>For this tutorial, we can label these 4 clusters as follows:</p><ol><li>Conservative Spenders: This segment includes customers with a higher average age and annual income, but a lower spending score. They may be more conservative with their spending and focused on saving their money.</li><li>Young and Wealthy: This segment includes customers with a lower average age and a high annual income and spending score. They are likely younger, wealthier customers who are more likely to spend on luxury goods and experiences.</li><li>Budget Shoppers: This segment includes customers with a lower average age and annual income, but a relatively high spending score. They may be focused on getting the most for their money and finding deals and discounts.</li><li>Middle-Aged Moderates: This segment includes customers with a higher average age and a moderate annual income and spending score. They may be more conservative with their spending than the Young and Wealthy segment, but still have more disposable income than the Budget Shoppers segment.</li></ol><p>This is just an example on how to interpret the clusters. Remember that the interpretation of the clusters is subjective and may require domain knowledge or further analysis.</p><h2 id=conclusion>Conclusion</h2><p>In this blog post, we learned about customer segmentation using K-means clustering in Python. We explored the significance of understanding customers&rsquo; preferences and behaviors. Through the implementation of K-means clustering, we discovered a practical and widely-used technique to cluster customers effectively. We examined the step-by-step process of applying K-means clustering in Python. By preprocessing the data, choosing the appropriate number of clusters, and utilizing the K-means algorithm, we obtained valuable insights into our customer segments.</p><p>In conclusion, customer segmentation through K-means clustering equips businesses with the tools to unlock the potential within their customer data. By gaining insights into distinct customer groups, organizations can tailor their marketing strategies, make data-driven decisions, and build strong connections with their customers. As the business landscape continues to evolve, harnessing the power of data science and customer segmentation becomes increasingly crucial in achieving sustainable growth and success.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://outlierblog.me/tags/data-mining/>data mining</a></li><li><a href=https://outlierblog.me/tags/customer-analytics/>customer analytics</a></li><li><a href=https://outlierblog.me/tags/marketing/>marketing</a></li></ul><nav class=paginav><a class=next href=https://outlierblog.me/posts/market_basket_analysis/><span class=title>Next »</span><br><span>Market Basket Analysis With Apriori Algorithm</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>