<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Comprehensive Guide to Tree-Based Methods | Outlier Blog</title><meta name=keywords content><meta name=description content="Tree-based methods are powerful algorithms widely used in machine learning and data science. They provide an intuitive and interpretable approach to solving regression and classification problems. In this blog post, we will delve into the basics of decision trees, examine their advantages and disadvantages, and explore the various ensemble techniques that enhance their performance.
Basics of Decision Trees Decision trees are powerful hierarchical models that effectively partition data into subsets based on input feature values."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/guide_to_tree_method/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="A Comprehensive Guide to Tree-Based Methods"><meta property="og:description" content="Tree-based methods are powerful algorithms widely used in machine learning and data science. They provide an intuitive and interpretable approach to solving regression and classification problems. In this blog post, we will delve into the basics of decision trees, examine their advantages and disadvantages, and explore the various ensemble techniques that enhance their performance.
Basics of Decision Trees Decision trees are powerful hierarchical models that effectively partition data into subsets based on input feature values."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/guide_to_tree_method/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-29T11:29:54+08:00"><meta property="article:modified_time" content="2023-05-29T11:29:54+08:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Comprehensive Guide to Tree-Based Methods"><meta name=twitter:description content="Tree-based methods are powerful algorithms widely used in machine learning and data science. They provide an intuitive and interpretable approach to solving regression and classification problems. In this blog post, we will delve into the basics of decision trees, examine their advantages and disadvantages, and explore the various ensemble techniques that enhance their performance.
Basics of Decision Trees Decision trees are powerful hierarchical models that effectively partition data into subsets based on input feature values."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"A Comprehensive Guide to Tree-Based Methods","item":"https://outlierblog.me/posts/guide_to_tree_method/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Comprehensive Guide to Tree-Based Methods","name":"A Comprehensive Guide to Tree-Based Methods","description":"Tree-based methods are powerful algorithms widely used in machine learning and data science. They provide an intuitive and interpretable approach to solving regression and classification problems. In this blog post, we will delve into the basics of decision trees, examine their advantages and disadvantages, and explore the various ensemble techniques that enhance their performance.\nBasics of Decision Trees Decision trees are powerful hierarchical models that effectively partition data into subsets based on input feature values.","keywords":[],"articleBody":"Tree-based methods are powerful algorithms widely used in machine learning and data science. They provide an intuitive and interpretable approach to solving regression and classification problems. In this blog post, we will delve into the basics of decision trees, examine their advantages and disadvantages, and explore the various ensemble techniques that enhance their performance.\nBasics of Decision Trees Decision trees are powerful hierarchical models that effectively partition data into subsets based on input feature values. By recursively applying a set of rules or conditions, decision trees provide a structured approach to solving regression and classification problems. To illustrate their functionality, let’s explore a scenario where we need to determine whether or not to go for a picnic based on weather conditions, and how a decision tree can assist us in making this decision.\nRoot Node: The root node is the topmost node in a decision tree. In this example, the root node is “Weather”. It represents the initial question about the weather conditions and serves as the starting point for the decision tree.\nInternal Node: The internal nodes are the non-terminal nodes in the decision tree. They represent the decision points where the data is split based on the conditions. In this example, the internal nodes are “Weather”, “Sunny”, “Cloudy”, “Humid”, “Dry”, “Windy”, and “Rainy.”\nBranch: The branches connect the internal and leaf nodes, representing the possible paths or outcomes based on the conditions. In this example, we have four branches:\n From the root node “Weather,” we have two branches going to “Sunny” and “Cloudy” based on the weather conditions. From the “Sunny” node, we have two branches going to “Humid” and “Dry” based on the humidity level. From the “Cloudy” node, we have two branches going to “Windy” and “Rainy” based on the wind conditions. From the “Windy” and “Rainy” nodes, we have branches leading to the respective “No” leaf nodes.  Terminal Node: The terminal nodes or leaf nodes are the bottom-most nodes of the tree. They do not split further and represent the final decisions or outcomes. In this case, the leaf nodes are “Yes (Picnic)” and three instances of “No”, indicating whether or not to go for a picnic based on the weather conditions.\nRegression Trees Regression trees are used when the target variable is continuous. They are constructed using a top-down, greedy approach called recursive binary splitting. Each tree starts with a root node representing the entire dataset. At each step, the algorithm selects the best feature and threshold to split the data, aiming to minimize the residual sum of squares (RSS). The splitting process continues until a stopping criterion is met, such as reaching a maximum depth or having a minimum number of samples in a leaf node.\n$$ RSS = \\sum(y_i - \\hat{y})^2 $$\nLet’s see how the regression trees can be implemented in practice. We are going to use the California Housing Dataset from the scikit-learn library. The California Housing dataset comprises housing information for various regions in California. It includes features like average income, average house age, and proximity to the ocean. The target variable is the median house value, expressed in hundreds of thousands of dollars ($100,000).\nfrom sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor from sklearn.tree import plot_tree from sklearn.metrics import mean_squared_error # Load the California Housing dataset data = fetch_california_housing(as_frame=True) df = data['data'] df['MedValue'] = data['target'] X = df.iloc[:, :-1] y = df['MedValue'] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a regression tree model regression_tree = DecisionTreeRegressor(random_state=42) # Fit the model on the training data regression_tree.fit(X_train, y_train) train_mse = mean_squared_error(y_train, regression_tree.predict(X_train)) test_mse = mean_squared_error(y_test, regression_tree.predict(X_test)) print(f\"Training MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")  Training MSE: 0.00, Test MSE: 0.50  We can also visualize the tree using plot_tree method. We use the parameter max_depth=2 to limit the depth of the tree that will be visualized. Only the first two levels of the tree will be displayed.\n# Visualize the regression tree plot_tree(regression_tree, filled=True, feature_names=X.columns, max_depth=2) plt.show()  The process describes above has the potential to yield accurate predictions on the training set, but is likely to overfit the data, leading to poor test set performance. Overfitting often occurs when the resulting tree becomes overly complex, attempting to capture noise in the training data. To address the issue of overfitting, tree pruning is commonly applied. Pruning involves removing or collapsing nodes in the tree to simplify its structure and improve generalization performance. We can prune the tree by setting the max_depth parameter of DecisionTreeRegressor.\nIt’s important to note that selecting the appropriate max_depth value requires finding a balance between model simplicity and predictive power. Setting max_depth too low may result in an overly simplified model that underfits the data, while setting it too high may lead to an overly complex model that overfits the data.\nBy pruning a regression tree, we can control its complexity and prevent it from fitting the noise in the training data. Pruning helps to strike a balance between bias and variance, leading to better generalization performance. It encourages the tree to focus on the most informative features and capture the underlying patterns rather than the noise.\nClassification Trees A classification tree is very similar to a regression tree, except that it is used when the target is categorical instead of continuous. In a classification tree, the goal is to assign each observation to the most frequently occurring class among the training observations within its region. This means that the predicted class for a given observation is determined based on the majority class of similar observations in its region. This approach allows us to classify new data points based on the learned patterns from the training set. The process of growing a classification tree follows a similar recursive binary splitting approach as seen in regression trees. At each step, the algorithm selects the best feature and threshold to split the data, aiming to maximize the separation of the classes. However, unlike in regression trees where the residual sum of squares (RSS) is used as the criterion, classification trees utilize the classification error rate. The classification error rate represents the fraction of training observations within a specific region that does not belong to the most common class.\n$$ Error \\ Rate = {(no. \\ of \\ misclassified \\ samples) \\over (total \\ no. \\ of \\ samples)} $$\nHowever, it turns out that classification error is not sufficiently sensitive for tree-growing. Therefore, alternative measures such as the Gini index and entropy are often preferred due to their enhanced sensitivity and ability to handle various scenarios.\n$$ Gini \\ index = 1 - \\sum(p_i^2) $$\nThe Gini index measures how likely it is to misclassify a randomly chosen element in a dataset based on the distribution of class labels. It ranges from 0 to 1, where lower values indicate higher purity and better splits. The Gini index prefers partitions that have clear class distinctions and maximize the separation between different classes.\n$$ Entropy = - \\sum(p_i \\ log(p_i)) $$\nEntropy is a measure that tells us how much disorder or uncertainty exists in a dataset. It quantifies the average amount of information needed to determine the class label of a randomly chosen data point. The values of entropy range from 0 to 1, where lower values indicate higher purity or more certainty. In the context of decision trees, entropy helps identify the splits that reduce randomness and create subsets with similar class labels. The goal is to find the splits that result in more homogeneous subsets, leading to better classification performance.\nLet’s see how the classification trees can be implemented in practice. We are going to use the famous Iris dataset.\nfrom sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Load the Iris dataset iris = load_iris() # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42) # Build the classification tree model clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Predict on the testing set y_pred = clf.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) print(\"Accuracy:\", accuracy) # Plot the decision tree plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)  Advantages and Disadvantages of Trees Advantages:\n Decision trees are easy to understand and interpret. Their visual representation provides insights into the decision-making process. They handle both numerical and categorical features without the need for extensive data preprocessing. Decision trees can capture nonlinear relationships between features and target variables. They can handle missing values by assigning them to the most probable class or value.  Disadvantages:\n Decision trees are prone to overfitting, especially when the tree becomes deep and complex. They are sensitive to small variations in the training data and can produce different trees for slightly different datasets.  Ensemble Methods Ensemble methods are powerful techniques that leverage the collective strength of multiple simple models, often referred to as weak learners. By combining these models, ensemble methods aim to create a single, robust model with powerful predictive capabilities. In this article, we will look into four popular ensemble methods: bagging, random forest, boosting, and Bayesian additive regression trees.\nBagging The basic idea behind bagging is to create multiple subsets of the original training data through a process called bootstrapping. Bootstrapping involves randomly sampling the training data with replacement, which means that each subset can contain duplicate observations as well as missing some of the original ones. These subsets are then used to train individual models, typically using the same learning algorithm.\nOnce the models are trained, bagging combines their predictions through an averaging process. For regression problems, the predictions are averaged to obtain the final predicted value. In classification problems, a majority vote is often used to determine the final predicted class.\nBy training multiple models on different subsets of the data and aggregating their predictions, bagging reduces the impact of outliers and noise in the training set. It helps to improve the stability and robustness of the model, leading to more reliable predictions.\nRandom Forests Random Forests improve on the concept of bagging by introducing additional randomization in the tree-growing process. While bagging uses the same base model (e.g., Decision Tree) on bootstrap samples of the data, Random Forests introduce random feature selection during each split.\nThe main idea behind Random Forests is that by introducing random feature selection, the individual trees in the ensemble become more diverse and less correlated with each other.\nIn bagging, each tree is trained on a bootstrap sample of the data. In Random Forests, during each split in the tree-growing process, a random subset of features is selected. This means that instead of considering all the available features at each split, only a subset is used. By using a smaller set of features, Random Forests introduce more diversity among the trees, as they can focus on different aspects of the data.\nThe introduction of random subspace selection helps to decorrelate the trees in the ensemble. Each tree is trained on a different set of features, which reduces the correlation between their predictions.\nFinally, Random Forests employ an ensemble voting mechanism to make predictions. For classification tasks, majority voting is used, where the class that receives the most votes from the trees is selected as the final prediction. For regression tasks, the predictions from all the trees are averaged. This aggregation of predictions from multiple trees helps to reduce the variance and increase the accuracy of the ensemble model.\nBoosting The key idea behind boosting is to sequentially train weak models and give more attention to the samples that were misclassified or have high residual errors. The subsequent weak models are then built to focus on these difficult samples, learning from the mistakes made by the previous models.\nBoosting algorithms, such as AdaBoost and Gradient Boosting, assign weights to the training samples. Initially, all samples are assigned equal weights, and the first weak model is trained on the weighted data. The model’s performance is evaluated, and the weights are adjusted to give more weight to misclassified samples. This ensures that subsequent models focus on these difficult samples during training.\nUnlike bagging, which focuses on reducing variance by building independent models in parallel, boosting aims to reduce bias by iteratively improving the model’s performance on the training data. This sequential nature allows boosting to adapt and improve its performance as the iterations progress.\nBayesian Additive Regression Tree Bayesian Additive Regression Tree (BART) is based on the concept of boosting, where weak learners are combined to form a strong learner. However, BART takes a Bayesian approach and incorporates a prior distribution to model the uncertainty in the parameters.\nThe BART algorithm starts by initializing the model with a single decision tree. Then, it iteratively grows the ensemble by adding more decision trees. In each iteration, a new decision tree is added by considering the residuals (the differences between the true values and the current ensemble’s predictions). The new tree is trained to capture the patterns and relationships in the residuals that have not yet been captured by the existing ensemble.\nOne of the key advantages of BART is its ability to handle complex interactions and non-linear relationships in the data. The ensemble of decision trees allows for capturing intricate interactions between predictors and can model non-linearities in a flexible manner.\nBART also incorporates a Bayesian framework, which enables the modeling of uncertainty in the parameters. It assigns prior distributions to the parameters and updates these distributions based on the observed data. This Bayesian approach allows for quantifying uncertainty in the model predictions and provides a measure of confidence for the estimated parameters.\nIn addition, BART performs automatic variable selection by determining the relevance of each predictor in the ensemble. It assigns weights to the predictors based on their importance in the model, which can help identify the most influential variables in the data.\nSummary of Ensemble Methods Bagging: Bagging combines multiple models trained on different subsets of the training data through bootstrapping. It reduces the impact of outliers and noise, improving model stability and robustness.\nRandom Forest: Random Forest is an ensemble method that combines multiple decision trees. It introduces randomness by selecting a random subset of features for each tree and averaging their predictions. It improves accuracy and handles high-dimensional data well.\nBoosting: Boosting is an ensemble method that builds models sequentially, where each new model focuses on correcting the mistakes made by the previous models. It combines weak learners to create a strong learner and improves prediction accuracy.\nBART (Bayesian Additive Regression Trees): BART is a Bayesian ensemble method that combines decision trees. It incorporates a Bayesian framework to model uncertainty in the parameters and allows for capturing complex interactions and non-linear relationships. It performs automatic variable selection and provides measures of confidence for predictions.\nConclusion In this blog post, we explored the basics of tree-based methods and their advantages and disadvantages. Decision trees provide an interpretable approach to solving regression and classification problems, allowing us to understand the decision-making process. However, they are prone to overfitting and can be sensitive to variations in the training data.\nTo enhance the performance of decision trees, we explored ensemble methods: bagging, random forests, boosting, and Bayesian additive regression trees (BART). These techniques leverage the collective strength of multiple models to create a single, robust model with improved predictive capabilities.\nTree-based methods and their ensemble techniques offer powerful tools for solving a wide range of machine learning and data science problems. By understanding their principles and applying them effectively, we can build robust models that provide accurate predictions and valuable insights into our data.\n","wordCount":"2605","inLanguage":"en","datePublished":"2023-05-29T11:29:54+08:00","dateModified":"2023-05-29T11:29:54+08:00","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/guide_to_tree_method/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Home><span>Home</span></a></li><li><a href=https://outlierblog.me/series/ title=Series><span>Series</span></a></li><li><a href=https://outlierblog.me/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>A Comprehensive Guide to Tree-Based Methods</h1><div class=post-meta><span title="2023-05-29 11:29:54 +0800 +08">May 29, 2023</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#basics-of-decision-trees>Basics of Decision Trees</a><ul><li><a href=#regression-trees>Regression Trees</a></li><li><a href=#classification-trees>Classification Trees</a></li><li><a href=#advantages-and-disadvantages-of-trees>Advantages and Disadvantages of Trees</a></li></ul></li><li><a href=#ensemble-methods>Ensemble Methods</a><ul><li><a href=#bagging>Bagging</a></li><li><a href=#random-forests>Random Forests</a></li><li><a href=#boosting>Boosting</a></li><li><a href=#bayesian-additive-regression-tree>Bayesian Additive Regression Tree</a></li><li><a href=#summary-of-ensemble-methods>Summary of Ensemble Methods</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>Tree-based methods are powerful algorithms widely used in machine learning and data science. They provide an intuitive and interpretable approach to solving regression and classification problems. In this blog post, we will delve into the basics of decision trees, examine their advantages and disadvantages, and explore the various ensemble techniques that enhance their performance.</p><h2 id=basics-of-decision-trees>Basics of Decision Trees</h2><p>Decision trees are powerful hierarchical models that effectively partition data into subsets based on input feature values. By recursively applying a set of rules or conditions, decision trees provide a structured approach to solving regression and classification problems. To illustrate their functionality, let’s explore a scenario where we need to determine whether or not to go for a picnic based on weather conditions, and how a decision tree can assist us in making this decision.</p><p><img loading=lazy src=images/decision_tree.png alt="A decision tree for determining whether to go on a picnic based on weather conditions"></p><p><strong>Root Node:</strong> The root node is the topmost node in a decision tree. In this example, the root node is “Weather”. It represents the initial question about the weather conditions and serves as the starting point for the decision tree.</p><p><strong>Internal Node:</strong> The internal nodes are the non-terminal nodes in the decision tree. They represent the decision points where the data is split based on the conditions. In this example, the internal nodes are “Weather”, “Sunny”, “Cloudy”, “Humid”, “Dry”, “Windy”, and “Rainy.”</p><p><strong>Branch:</strong> The branches connect the internal and leaf nodes, representing the possible paths or outcomes based on the conditions. In this example, we have four branches:</p><ul><li>From the root node “Weather,” we have two branches going to “Sunny” and “Cloudy” based on the weather conditions.</li><li>From the “Sunny” node, we have two branches going to “Humid” and “Dry” based on the humidity level.</li><li>From the “Cloudy” node, we have two branches going to “Windy” and “Rainy” based on the wind conditions.</li><li>From the “Windy” and “Rainy” nodes, we have branches leading to the respective “No” leaf nodes.</li></ul><p><strong>Terminal Node:</strong> The terminal nodes or leaf nodes are the bottom-most nodes of the tree. They do not split further and represent the final decisions or outcomes. In this case, the leaf nodes are “Yes (Picnic)” and three instances of “No”, indicating whether or not to go for a picnic based on the weather conditions.</p><h3 id=regression-trees>Regression Trees</h3><p>Regression trees are used when the target variable is continuous. They are constructed using a top-down, greedy approach called recursive binary splitting. Each tree starts with a root node representing the entire dataset. At each step, the algorithm selects the best feature and threshold to split the data, aiming to minimize the residual sum of squares (RSS). The splitting process continues until a stopping criterion is met, such as reaching a maximum depth or having a minimum number of samples in a leaf node.</p><p>$$ RSS = \sum(y_i - \hat{y})^2 $$</p><p>Let’s see how the regression trees can be implemented in practice. We are going to use the California Housing Dataset from the scikit-learn library. The California Housing dataset comprises housing information for various regions in California. It includes features like average income, average house age, and proximity to the ocean. The target variable is the median house value, expressed in hundreds of thousands of dollars ($100,000).</p><pre><code class=language-python>from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import plot_tree
from sklearn.metrics import mean_squared_error

# Load the California Housing dataset
data = fetch_california_housing(as_frame=True)
df = data['data']
df['MedValue'] = data['target']

X = df.iloc[:, :-1]
y = df['MedValue']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a regression tree model
regression_tree = DecisionTreeRegressor(random_state=42)

# Fit the model on the training data
regression_tree.fit(X_train, y_train)

train_mse = mean_squared_error(y_train, regression_tree.predict(X_train))
test_mse = mean_squared_error(y_test, regression_tree.predict(X_test))

print(f&quot;Training MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}&quot;)
</code></pre><pre><code class=language-shell>Training MSE: 0.00, Test MSE: 0.50
</code></pre><p>We can also visualize the tree using <code>plot_tree</code> method. We use the parameter <code>max_depth=2</code> to limit the depth of the tree that will be visualized. Only the first two levels of the tree will be displayed.</p><pre><code class=language-python># Visualize the regression tree
plot_tree(regression_tree, filled=True, feature_names=X.columns, max_depth=2)
plt.show()
</code></pre><p><img loading=lazy src=images/regression_tree.png alt></p><p>The process describes above has the potential to yield accurate predictions on the training set, but is likely to overfit the data, leading to poor test set performance. Overfitting often occurs when the resulting tree becomes overly complex, attempting to capture noise in the training data. To address the issue of overfitting, tree pruning is commonly applied. Pruning involves removing or collapsing nodes in the tree to simplify its structure and improve generalization performance. We can prune the tree by setting the <code>max_depth</code> parameter of <code>DecisionTreeRegressor</code>.</p><p>It’s important to note that selecting the appropriate <code>max_depth</code> value requires finding a balance between model simplicity and predictive power. Setting <code>max_depth</code> too low may result in an overly simplified model that underfits the data, while setting it too high may lead to an overly complex model that overfits the data.</p><p><img loading=lazy src=images/pruned.png alt="Test MSE reaches its minimum at a tree size of 9, indicating the optimal complexity for the model, beyond which performance starts to deteriorate"></p><p>By pruning a regression tree, we can control its complexity and prevent it from fitting the noise in the training data. Pruning helps to strike a balance between bias and variance, leading to better generalization performance. It encourages the tree to focus on the most informative features and capture the underlying patterns rather than the noise.</p><h3 id=classification-trees>Classification Trees</h3><p>A classification tree is very similar to a regression tree, except that it is used when the target is categorical instead of continuous. In a classification tree, the goal is to assign each observation to the most frequently occurring class among the training observations within its region. This means that the predicted class for a given observation is determined based on the majority class of similar observations in its region. This approach allows us to classify new data points based on the learned patterns from the training set. The process of growing a classification tree follows a similar recursive binary splitting approach as seen in regression trees. At each step, the algorithm selects the best feature and threshold to split the data, aiming to maximize the separation of the classes. However, unlike in regression trees where the residual sum of squares (RSS) is used as the criterion, classification trees utilize the classification error rate. The classification error rate represents the fraction of training observations within a specific region that does not belong to the most common class.</p><p>$$ Error \ Rate = {(no. \ of \ misclassified \ samples) \over (total \ no. \ of \ samples)} $$</p><p>However, it turns out that classification error is not sufficiently sensitive for tree-growing. Therefore, alternative measures such as the Gini index and entropy are often preferred due to their enhanced sensitivity and ability to handle various scenarios.</p><p>$$ Gini \ index = 1 - \sum(p_i^2) $$</p><p>The Gini index measures how likely it is to misclassify a randomly chosen element in a dataset based on the distribution of class labels. It ranges from 0 to 1, where lower values indicate higher purity and better splits. The Gini index prefers partitions that have clear class distinctions and maximize the separation between different classes.</p><p>$$ Entropy = - \sum(p_i \ log(p_i)) $$</p><p>Entropy is a measure that tells us how much disorder or uncertainty exists in a dataset. It quantifies the average amount of information needed to determine the class label of a randomly chosen data point. The values of entropy range from 0 to 1, where lower values indicate higher purity or more certainty. In the context of decision trees, entropy helps identify the splits that reduce randomness and create subsets with similar class labels. The goal is to find the splits that result in more homogeneous subsets, leading to better classification performance.</p><p>Let’s see how the classification trees can be implemented in practice. We are going to use the famous Iris dataset.</p><pre><code class=language-python>from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

# Build the classification tree model
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict on the testing set
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(&quot;Accuracy:&quot;, accuracy)

# Plot the decision tree
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
</code></pre><p><img loading=lazy src=images/classification_tree.png alt></p><h3 id=advantages-and-disadvantages-of-trees>Advantages and Disadvantages of Trees</h3><p><strong>Advantages:</strong></p><ul><li>Decision trees are easy to understand and interpret. Their visual representation provides insights into the decision-making process.</li><li>They handle both numerical and categorical features without the need for extensive data preprocessing.</li><li>Decision trees can capture nonlinear relationships between features and target variables.</li><li>They can handle missing values by assigning them to the most probable class or value.</li></ul><p><strong>Disadvantages:</strong></p><ul><li>Decision trees are prone to overfitting, especially when the tree becomes deep and complex.</li><li>They are sensitive to small variations in the training data and can produce different trees for slightly different datasets.</li></ul><h2 id=ensemble-methods>Ensemble Methods</h2><p>Ensemble methods are powerful techniques that leverage the collective strength of multiple simple models, often referred to as weak learners. By combining these models, ensemble methods aim to create a single, robust model with powerful predictive capabilities. In this article, we will look into four popular ensemble methods: bagging, random forest, boosting, and Bayesian additive regression trees.</p><h3 id=bagging>Bagging</h3><p>The basic idea behind bagging is to create multiple subsets of the original training data through a process called bootstrapping. Bootstrapping involves randomly sampling the training data with replacement, which means that each subset can contain duplicate observations as well as missing some of the original ones. These subsets are then used to train individual models, typically using the same learning algorithm.</p><p>Once the models are trained, bagging combines their predictions through an averaging process. For regression problems, the predictions are averaged to obtain the final predicted value. In classification problems, a majority vote is often used to determine the final predicted class.</p><p>By training multiple models on different subsets of the data and aggregating their predictions, bagging reduces the impact of outliers and noise in the training set. It helps to improve the stability and robustness of the model, leading to more reliable predictions.</p><p><img loading=lazy src=images/bagging.png alt="Illustration of Bagging: Subsets of data are randomly sampled with replacement to create multiple subsets"></p><h3 id=random-forests>Random Forests</h3><p>Random Forests improve on the concept of bagging by introducing additional randomization in the tree-growing process. While bagging uses the same base model (e.g., Decision Tree) on bootstrap samples of the data, Random Forests introduce random feature selection during each split.</p><p>The main idea behind Random Forests is that by introducing random feature selection, the individual trees in the ensemble become more diverse and less correlated with each other.</p><p>In bagging, each tree is trained on a bootstrap sample of the data. In Random Forests, during each split in the tree-growing process, a random subset of features is selected. This means that instead of considering all the available features at each split, only a subset is used. By using a smaller set of features, Random Forests introduce more diversity among the trees, as they can focus on different aspects of the data.</p><p>The introduction of random subspace selection helps to decorrelate the trees in the ensemble. Each tree is trained on a different set of features, which reduces the correlation between their predictions.</p><p>Finally, Random Forests employ an ensemble voting mechanism to make predictions. For classification tasks, majority voting is used, where the class that receives the most votes from the trees is selected as the final prediction. For regression tasks, the predictions from all the trees are averaged. This aggregation of predictions from multiple trees helps to reduce the variance and increase the accuracy of the ensemble model.</p><p><img loading=lazy src=images/random_forest.png alt="Illustration of Random Forest: Multiple decision trees are trained independently on different subsets of the data using random feature subsets"></p><h3 id=boosting>Boosting</h3><p>The key idea behind boosting is to sequentially train weak models and give more attention to the samples that were misclassified or have high residual errors. The subsequent weak models are then built to focus on these difficult samples, learning from the mistakes made by the previous models.</p><p>Boosting algorithms, such as AdaBoost and Gradient Boosting, assign weights to the training samples. Initially, all samples are assigned equal weights, and the first weak model is trained on the weighted data. The model’s performance is evaluated, and the weights are adjusted to give more weight to misclassified samples. This ensures that subsequent models focus on these difficult samples during training.</p><p>Unlike bagging, which focuses on reducing variance by building independent models in parallel, boosting aims to reduce bias by iteratively improving the model’s performance on the training data. This sequential nature allows boosting to adapt and improve its performance as the iterations progress.</p><p><img loading=lazy src=images/boosting.png alt="Illustration of Boosting: Weak learners are sequentially trained on the data, with each subsequent learner focusing on the misclassified samples from the previous iteration"></p><h3 id=bayesian-additive-regression-tree>Bayesian Additive Regression Tree</h3><p>Bayesian Additive Regression Tree (BART) is based on the concept of boosting, where weak learners are combined to form a strong learner. However, BART takes a Bayesian approach and incorporates a prior distribution to model the uncertainty in the parameters.</p><p>The BART algorithm starts by initializing the model with a single decision tree. Then, it iteratively grows the ensemble by adding more decision trees. In each iteration, a new decision tree is added by considering the residuals (the differences between the true values and the current ensemble’s predictions). The new tree is trained to capture the patterns and relationships in the residuals that have not yet been captured by the existing ensemble.</p><p>One of the key advantages of BART is its ability to handle complex interactions and non-linear relationships in the data. The ensemble of decision trees allows for capturing intricate interactions between predictors and can model non-linearities in a flexible manner.</p><p>BART also incorporates a Bayesian framework, which enables the modeling of uncertainty in the parameters. It assigns prior distributions to the parameters and updates these distributions based on the observed data. This Bayesian approach allows for quantifying uncertainty in the model predictions and provides a measure of confidence for the estimated parameters.</p><p>In addition, BART performs automatic variable selection by determining the relevance of each predictor in the ensemble. It assigns weights to the predictors based on their importance in the model, which can help identify the most influential variables in the data.</p><h3 id=summary-of-ensemble-methods>Summary of Ensemble Methods</h3><p><strong>Bagging:</strong> Bagging combines multiple models trained on different subsets of the training data through bootstrapping. It reduces the impact of outliers and noise, improving model stability and robustness.</p><p><strong>Random Forest:</strong> Random Forest is an ensemble method that combines multiple decision trees. It introduces randomness by selecting a random subset of features for each tree and averaging their predictions. It improves accuracy and handles high-dimensional data well.</p><p><strong>Boosting:</strong> Boosting is an ensemble method that builds models sequentially, where each new model focuses on correcting the mistakes made by the previous models. It combines weak learners to create a strong learner and improves prediction accuracy.</p><p><strong>BART (Bayesian Additive Regression Trees):</strong> BART is a Bayesian ensemble method that combines decision trees. It incorporates a Bayesian framework to model uncertainty in the parameters and allows for capturing complex interactions and non-linear relationships. It performs automatic variable selection and provides measures of confidence for predictions.</p><h2 id=conclusion>Conclusion</h2><p>In this blog post, we explored the basics of tree-based methods and their advantages and disadvantages. Decision trees provide an interpretable approach to solving regression and classification problems, allowing us to understand the decision-making process. However, they are prone to overfitting and can be sensitive to variations in the training data.</p><p>To enhance the performance of decision trees, we explored ensemble methods: bagging, random forests, boosting, and Bayesian additive regression trees (BART). These techniques leverage the collective strength of multiple models to create a single, robust model with improved predictive capabilities.</p><p>Tree-based methods and their ensemble techniques offer powerful tools for solving a wide range of machine learning and data science problems. By understanding their principles and applying them effectively, we can build robust models that provide accurate predictions and valuable insights into our data.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://outlierblog.me/posts/addressing_outlier/><span class=title>« Prev</span><br><span>Addressing Outliers For Resilient Machine Learning Models</span></a>
<a class=next href=https://outlierblog.me/posts/linear_regression_essential/><span class=title>Next »</span><br><span>Understanding Linear Regression with Python</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share A Comprehensive Guide to Tree-Based Methods on twitter" href="https://twitter.com/intent/tweet/?text=A%20Comprehensive%20Guide%20to%20Tree-Based%20Methods&url=https%3a%2f%2foutlierblog.me%2fposts%2fguide_to_tree_method%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Comprehensive Guide to Tree-Based Methods on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2foutlierblog.me%2fposts%2fguide_to_tree_method%2f&title=A%20Comprehensive%20Guide%20to%20Tree-Based%20Methods&summary=A%20Comprehensive%20Guide%20to%20Tree-Based%20Methods&source=https%3a%2f%2foutlierblog.me%2fposts%2fguide_to_tree_method%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Comprehensive Guide to Tree-Based Methods on reddit" href="https://reddit.com/submit?url=https%3a%2f%2foutlierblog.me%2fposts%2fguide_to_tree_method%2f&title=A%20Comprehensive%20Guide%20to%20Tree-Based%20Methods"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Comprehensive Guide to Tree-Based Methods on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foutlierblog.me%2fposts%2fguide_to_tree_method%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Comprehensive Guide to Tree-Based Methods on whatsapp" href="https://api.whatsapp.com/send?text=A%20Comprehensive%20Guide%20to%20Tree-Based%20Methods%20-%20https%3a%2f%2foutlierblog.me%2fposts%2fguide_to_tree_method%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Comprehensive Guide to Tree-Based Methods on telegram" href="https://telegram.me/share/url?text=A%20Comprehensive%20Guide%20to%20Tree-Based%20Methods&url=https%3a%2f%2foutlierblog.me%2fposts%2fguide_to_tree_method%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>