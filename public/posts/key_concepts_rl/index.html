<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Key Concepts In (Deep) Reinforcement Learning | Outlier Blog</title><meta name=keywords content="reinforcement-learning"><meta name=description content="Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.
The agent also receives rewards from the environment, which indicate how well it is doing. The agent&rsquo;s ultimate goal is to maximize the total rewards it receives, called return."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/key_concepts_rl/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/javascript>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js integrity=sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="Key Concepts In (Deep) Reinforcement Learning"><meta property="og:description" content="Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.
The agent also receives rewards from the environment, which indicate how well it is doing. The agent&rsquo;s ultimate goal is to maximize the total rewards it receives, called return."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/key_concepts_rl/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-25T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-25T00:00:00+00:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Key Concepts In (Deep) Reinforcement Learning"><meta name=twitter:description content="Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.
The agent also receives rewards from the environment, which indicate how well it is doing. The agent&rsquo;s ultimate goal is to maximize the total rewards it receives, called return."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"Key Concepts In (Deep) Reinforcement Learning","item":"https://outlierblog.me/posts/key_concepts_rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Key Concepts In (Deep) Reinforcement Learning","name":"Key Concepts In (Deep) Reinforcement Learning","description":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return.","keywords":["reinforcement-learning"],"articleBody":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent’s ultimate goal is to maximize the total rewards it receives, called return. RL methods are designed to help the agent learn and improve its behaviors to achieve its objectives.\nTo better understand RL, we need to explore some key terms such as states and observations, action spaces, policies, trajectories, different ways of measuring return, the optimization problem in RL, and value functions. By understanding these fundamental concepts, we build a strong foundation to understand how RL methods work and how they can be used to solve many different real-life problems.\nStates and Observations In RL, a state represents a complete description of the world at a given point in time. It contains all the information necessary to understand the current state of the environment, including any hidden information. On the other hand, an observation provides a partial description of the state and may omit certain details. In deep RL, observations are often represented as real-valued vectors, matrices, or higher-order tensors. For example, a visual observation can be represented by an RGB matrix of pixel values, while the state of a robot can be represented by its joint angles and velocities.\nAction Spaces Different environments allow different kinds of actions to be taken by the agent. The set of valid actions is referred to as the action space. In RL, there are two main types of action spaces: discrete and continuous.\nIn discrete action spaces, such as Atari games or the game of chess, only a finite number of moves are available to the agent. The agent selects an action from a predefined set of discrete choices.\nIn continuous action spaces, such as controlling a robot in the physical world, actions are represented as real-valued vectors. This allows for a wide range of precise actions to be taken by the agent.\nPolicies A policy is a rule or strategy used by an agent to decide what actions to take in a given state. In RL, policies can be deterministic or stochastic.\nA deterministic policy selects a single action for a given state:\n$$ a_t = \\mu (s_t) $$\nThe action is directly determined by the policy’s output, which can be a computable function based on a set of parameters, such as the weights and biases of a neural network.\nA stochastic policy selects actions probabilistically:\n$$ a_t \\sim \\pi( \\cdot \\mid s_t) $$\nThe policy outputs a probability distribution over actions for a given state, and the agent samples an action based on this distribution. Stochastic policies are often used when exploring different actions or dealing with uncertainty.\nIn deep RL, parameterized policies are commonly used, where the outputs are computable functions that depend on a set of parameters. These policies can be represented by neural networks, allowing for flexible and expressive policy representations.\nTrajectories A trajectory, also known as an episode or rollout, is a sequence of states and actions experienced by an agent in the environment. It captures the agent’s interactions with the environment over a specific period. A trajectory can be represented as follows:\n$$ \\tau = (s_0, a_0, s_1, a_1, …) $$\nThe very first state of the trajectory, $s_0$, is randomly sampled from the start-state distribution, denoted as $s_0 \\sim \\rho_0 (\\cdot) $. The state transitions in an environment can be either deterministic or stochastic.\nIn deterministic state transitions, the next state, $s_{t+1}$, is solely determined by the current state and action:\n$$ s_{t+1} = f(s_t, a_t) $$\nIn stochastic state transitions, the next state, $s_{t+1}$, is sampled from a transition probability distribution:\n$$ s_{t+1} \\sim P(\\cdot|s_t, a_t) $$\nRewards and Return The reward function plays a crucial role in RL. It quantifies the immediate desirability or quality of a particular state-action-state transition. The reward function depends on the current state, the action taken, and the next state:\nThe agent’s goal is to maximize the cumulative reward over a trajectory, denoted as $R(\\tau)$. There are different types of returns in RL:\nFinite-horizon undiscounted return represents the sum of rewards obtained within a fixed window of steps:\n$$ R(\\tau) = \\sum^T_{t=0}r_t $$\nInfinite-horizon discounted return represents the sum of all rewards obtained by the agent, but discounted based on how far they are obtained in the future:\n$$ R(\\tau) = \\sum^\\infty_{t=0} \\gamma^tr_t, $$\nwhere $\\gamma \\in (0, 1)$\nThe RL Problem The primary objective in RL is to find a policy that maximizes the expected return when the agent acts according to it. Suppose the environment transitions and the policy are stochastic. In that case, the probability of a T-step trajectory can be expressed as:\n$$ P(\\tau | \\pi) = \\rho_0(s_0) \\prod_{t=0}^{T - 1} P(s_{t+1}|s_t, a_t) \\pi(a_t|s_t) $$\nThe expected return, or objective function, can be defined as:\n$$ J(\\pi) = \\int_\\tau P(\\tau | \\pi)R(\\tau) = E_{\\tau \\sim \\pi}[R(\\tau)] $$\nThe central optimization problem in RL is to find the optimal policy, denoted as π*:\n$$ \\pi^*=\\arg \\max_\\pi J(\\pi) $$\nValue Functions Value functions provide estimates of the expected return associated with states or state-action pairs. There are four main types of value functions:\nThe on-policy value function, $V^\\pi(s)$, estimates the expected return if we start in state s and always act according to policy $\\pi$:\n$$V ^\\pi(s) = E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe on-policy action-value function, $Q^\\pi(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to policy $\\pi$:\n$$ ^\\pi(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a] $$\nThe optimal value function, $V^*(s)$, estimates the expected return if we start in state s and always act according to the optimal policy:\n$$ V^\\pi(s) = \\max_\\pi E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s] $$\nThe optimal action-value function, $Q^*(s,a)$, estimates the expected return if we start in state s, take action a, and then forever act according to the optimal policy:\n$$ Q^\\pi(s,a) = \\max_\\pi E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a $$\nBellman Equations All four value functions above satisfy self-consistency equations known as the Bellman equations. These equations describe the relationship between the value of a state or state-action pair and the value of subsequent states.\nThe Bellman equations for on-policy value functions are:\n$$ V^\\pi(s)=E_{a \\sim \\pi, s’ \\sim P}[r(s,a)+ \\gamma V^\\pi(s’)], $$\n$$ Q^\\pi(s,a)=E_{s’ \\sim P}[r(s,a)+ \\gamma E_{a’ \\sim \\pi}[Q^\\pi(s’, a’)] $$\nThe Bellman equations for optimal value functions are:\n$$ V^\\ast(s)=\\max_a E_{s’ \\sim P} [r(s,a)+\\gamma V^*(s’)], $$\n$$ Q^\\ast(s,a)= E_{s’ \\sim P} [r(s,a)+\\gamma \\max_a’ Q^*(s’, a’)] $$\nThe crucial difference between the Bellman equations for on-policy value functions and optimal value functions is the absence or presence of the maximization over actions. The inclusion of this maximization reflects the fact that the agent must select the action that leads to the highest value in order to act optimally.\nAdvantage Functions In RL, there are situations where we are interested in understanding not just how good an action is in absolute terms but how much better it is compared to other available actions on average. The advantage function captures the relative advantage of taking a specified action in a state compared to randomly selecting an action. It can be defined as:\n$$ A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s) $$\nThe advantage function provides insights into the superiority of a specific action in a given state, considering the current policy’s performance.\nReferences [1] OpenAI Spinning Up. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n","wordCount":"1267","inLanguage":"en","datePublished":"2023-06-25T00:00:00Z","dateModified":"2023-06-25T00:00:00Z","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/key_concepts_rl/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Posts><span>Posts</span></a></li><li><a href=https://outlierblog.me/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://outlierblog.me/notebooks/ title=Notebooks><span>Notebooks</span></a></li><li><a href=https://outlierblog.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Key Concepts In (Deep) Reinforcement Learning</h1><div class=post-meta><span title='2023-06-25 00:00:00 +0000 UTC'>June 25, 2023</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#states-and-observations>States and Observations</a></li><li><a href=#action-spaces>Action Spaces</a></li><li><a href=#policies>Policies</a></li><li><a href=#trajectories>Trajectories</a></li><li><a href=#rewards-and-return>Rewards and Return</a></li><li><a href=#the-rl-problem>The RL Problem</a></li><li><a href=#value-functions>Value Functions</a></li><li><a href=#bellman-equations>Bellman Equations</a></li><li><a href=#advantage-functions>Advantage Functions</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.</p><p>The agent also receives rewards from the environment, which indicate how well it is doing. The agent&rsquo;s ultimate goal is to maximize the total rewards it receives, called return. RL methods are designed to help the agent learn and improve its behaviors to achieve its objectives.</p><p>To better understand RL, we need to explore some key terms such as states and observations, action spaces, policies, trajectories, different ways of measuring return, the optimization problem in RL, and value functions. By understanding these fundamental concepts, we build a strong foundation to understand how RL methods work and how they can be used to solve many different real-life problems.</p><h2 id=states-and-observations>States and Observations</h2><p>In RL, a <strong>state</strong> represents a complete description of the world at a given point in time. It contains all the information necessary to understand the current state of the environment, including any hidden information. On the other hand, an <strong>observation</strong> provides a partial description of the state and may omit certain details. In deep RL, observations are often represented as real-valued vectors, matrices, or higher-order tensors. For example, a visual observation can be represented by an RGB matrix of pixel values, while the state of a robot can be represented by its joint angles and velocities.</p><h2 id=action-spaces>Action Spaces</h2><p>Different environments allow different kinds of actions to be taken by the agent. The set of valid actions is referred to as the <strong>action space</strong>. In RL, there are two main types of action spaces: discrete and continuous.</p><p>In <strong>discrete action spaces</strong>, such as Atari games or the game of chess, only a finite number of moves are available to the agent. The agent selects an action from a predefined set of discrete choices.</p><p>In <strong>continuous action spaces</strong>, such as controlling a robot in the physical world, actions are represented as real-valued vectors. This allows for a wide range of precise actions to be taken by the agent.</p><h2 id=policies>Policies</h2><p>A <strong>policy</strong> is a rule or strategy used by an agent to decide what actions to take in a given state. In RL, policies can be deterministic or stochastic.</p><p>A <strong>deterministic policy</strong> selects a single action for a given state:</p><p>$$
a_t = \mu (s_t)
$$</p><p>The action is directly determined by the policy&rsquo;s output, which can be a computable function based on a set of parameters, such as the weights and biases of a neural network.</p><p>A <strong>stochastic policy</strong> selects actions probabilistically:</p><p>$$
a_t \sim \pi( \cdot \mid s_t)
$$</p><p>The policy outputs a probability distribution over actions for a given state, and the agent samples an action based on this distribution. Stochastic policies are often used when exploring different actions or dealing with uncertainty.</p><p>In deep RL, parameterized policies are commonly used, where the outputs are computable functions that depend on a set of parameters. These policies can be represented by neural networks, allowing for flexible and expressive policy representations.</p><h2 id=trajectories>Trajectories</h2><p>A trajectory, also known as an episode or rollout, is a sequence of states and actions experienced by an agent in the environment. It captures the agent&rsquo;s interactions with the environment over a specific period. A trajectory can be represented as follows:</p><p>$$
\tau = (s_0, a_0, s_1, a_1, &mldr;)
$$</p><p>The very first state of the trajectory, $s_0$, is randomly sampled from the start-state distribution, denoted as $s_0 \sim \rho_0 (\cdot) $. The state transitions in an environment can be either deterministic or stochastic.</p><p>In <strong>deterministic state transitions</strong>, the next state, $s_{t+1}$, is solely determined by the current state and action:</p><p>$$
s_{t+1} = f(s_t, a_t)
$$</p><p>In <strong>stochastic state transitions</strong>, the next state, $s_{t+1}$, is sampled from a transition probability distribution:</p><p>$$
s_{t+1} \sim P(\cdot|s_t, a_t)
$$</p><h2 id=rewards-and-return>Rewards and Return</h2><p>The reward function plays a crucial role in RL. It quantifies the immediate desirability or quality of a particular state-action-state transition. The reward function depends on the current state, the action taken, and the next state:</p><p>The agent&rsquo;s goal is to maximize the cumulative reward over a trajectory, denoted as $R(\tau)$. There are different types of returns in RL:</p><p><strong>Finite-horizon undiscounted return</strong> represents the sum of rewards obtained within a fixed window of steps:</p><p>$$
R(\tau) = \sum^T_{t=0}r_t
$$</p><p><strong>Infinite-horizon discounted return</strong> represents the sum of all rewards obtained by the agent, but discounted based on how far they are obtained in the future:</p><p>$$
R(\tau) = \sum^\infty_{t=0} \gamma^tr_t,
$$</p><p>where $\gamma \in (0, 1)$</p><h2 id=the-rl-problem>The RL Problem</h2><p>The primary objective in RL is to find a policy that maximizes the expected return when the agent acts according to it. Suppose the environment transitions and the policy are stochastic. In that case, the probability of a T-step trajectory can be expressed as:</p><p>$$
P(\tau | \pi) = \rho_0(s_0) \prod_{t=0}^{T - 1} P(s_{t+1}|s_t, a_t) \pi(a_t|s_t)
$$</p><p>The expected return, or objective function, can be defined as:</p><p>$$
J(\pi) = \int_\tau P(\tau | \pi)R(\tau) = E_{\tau \sim \pi}[R(\tau)]
$$</p><p>The central optimization problem in RL is to find the optimal policy, denoted as <strong><code>π*</code></strong>:</p><p>$$
\pi^*=\arg \max_\pi J(\pi)
$$</p><h2 id=value-functions>Value Functions</h2><p>Value functions provide estimates of the expected return associated with states or state-action pairs. There are four main types of value functions:</p><p>The <strong>on-policy value function</strong>, $V^\pi(s)$, estimates the expected return if we start in state <em>s</em> and always act according to policy $\pi$:</p><p>$$V
^\pi(s) = E_{\tau \sim \pi} [R(\tau)|s_0 = s]
$$</p><p>The <strong>on-policy action-value function</strong>, $Q^\pi(s,a)$, estimates the expected return if we start in state <em>s</em>, take action <em>a</em>, and then forever act according to policy $\pi$:</p><p>$$
^\pi(s,a) = E_{\tau \sim \pi}[R(\tau)|s_0=s, a_0=a]
$$</p><p>The <strong>optimal value function</strong>, $V^*(s)$, estimates the expected return if we start in state <em>s</em> and always act according to the <em>optimal</em> policy:</p><p>$$
V^\pi(s) = \max_\pi E_{\tau \sim \pi} [R(\tau)|s_0 = s]
$$</p><p>The <strong>optimal action-value function</strong>, $Q^*(s,a)$, estimates the expected return if we start in state <em>s</em>, take action <em>a</em>, and then forever act according to the <em>optimal</em> policy:</p><p>$$
Q^\pi(s,a) = \max_\pi E_{\tau \sim \pi}[R(\tau)|s_0=s, a_0=a
$$</p><h2 id=bellman-equations>Bellman Equations</h2><p>All four value functions above satisfy self-consistency equations known as the Bellman equations. These equations describe the relationship between the value of a state or state-action pair and the value of subsequent states.</p><p>The Bellman equations for on-policy value functions are:</p><p>$$
V^\pi(s)=E_{a \sim \pi, s&rsquo; \sim P}[r(s,a)+ \gamma V^\pi(s&rsquo;)],
$$</p><p>$$
Q^\pi(s,a)=E_{s&rsquo; \sim P}[r(s,a)+ \gamma E_{a&rsquo; \sim \pi}[Q^\pi(s&rsquo;, a&rsquo;)]
$$</p><p>The Bellman equations for optimal value functions are:</p><p>$$
V^\ast(s)=\max_a E_{s&rsquo; \sim P} [r(s,a)+\gamma V^*(s&rsquo;)],
$$</p><p>$$
Q^\ast(s,a)= E_{s&rsquo; \sim P} [r(s,a)+\gamma \max_a&rsquo; Q^*(s&rsquo;, a&rsquo;)]
$$</p><p>The crucial difference between the Bellman equations for on-policy value functions and optimal value functions is the absence or presence of the maximization over actions. The inclusion of this maximization reflects the fact that the agent must select the action that leads to the highest value in order to act optimally.</p><h2 id=advantage-functions>Advantage Functions</h2><p>In RL, there are situations where we are interested in understanding not just how good an action is in absolute terms but how much better it is compared to other available actions on average. The <strong>advantage function</strong> captures the relative advantage of taking a specified action in a state compared to randomly selecting an action. It can be defined as:</p><p>$$
A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)
$$</p><p>The advantage function provides insights into the superiority of a specific action in a given state, considering the current policy&rsquo;s performance.</p><h2 id=references>References</h2><p>[1] OpenAI Spinning Up. <a href=https://spinningup.openai.com/en/latest/spinningup/rl_intro.html>https://spinningup.openai.com/en/latest/spinningup/rl_intro.html</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://outlierblog.me/tags/reinforcement-learning/>reinforcement-learning</a></li></ul><nav class=paginav><a class=prev href=https://outlierblog.me/posts/multibandits/><span class=title>« Prev</span><br><span>Multi-Armed Bandit Problem and Its Solutions</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>