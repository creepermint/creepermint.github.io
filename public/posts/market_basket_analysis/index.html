<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Market Basket Analysis With Apriori Algorithm | Outlier Blog</title><meta name=keywords content="data mining,predictive analytics"><meta name=description content="Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.
Market Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/market_basket_analysis/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="Market Basket Analysis With Apriori Algorithm"><meta property="og:description" content="Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.
Market Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/market_basket_analysis/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-29T11:31:18+08:00"><meta property="article:modified_time" content="2023-05-29T11:31:18+08:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Market Basket Analysis With Apriori Algorithm"><meta name=twitter:description content="Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.
Market Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"Market Basket Analysis With Apriori Algorithm","item":"https://outlierblog.me/posts/market_basket_analysis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Market Basket Analysis With Apriori Algorithm","name":"Market Basket Analysis With Apriori Algorithm","description":"Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.\nMarket Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors.","keywords":["data mining","predictive analytics"],"articleBody":"Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.\nMarket Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors. By identifying patterns and relationships between products, businesses can improve sales and customer satisfaction.\nIn this blog post, we will explore the use of association rule mining in Market Basket Analysis, with a focus on one of the most popular algorithms - the Apriori Algorithm. We will delve into the inner workings of the algorithm and understand how it can be applied to reveal patterns in customer purchase behavior.\nAssociation Rule Mining One of the key techniques used in Market Basket Analysis is Association Rule Mining. Association Rule Mining is a powerful technique that can be used to identify interesting relationships between items in large datasets. It involves identifying frequent itemsets, which are sets of items that are often purchased or appear together in transactions, and then generating association rules that describe the relationships between these itemsets.\nAn association rule has two parts: an antecedent and a consequent. The antecedent is the set of items that appear on the left-hand side of the rule, while the consequent is the set of items that appear on the right-hand side. The antecedent and consequent are connected by the symbol ‚Äú=‚Äù which means ‚Äúimplies‚Äù.\nFor example, consider the following association rule: {üçû, üç∞} = {‚òï}. In this rule, {üçû, üç∞} is the antecedent and {‚òï} is the consequent. This rule implies that customers who purchase bread and cake together are also likely to purchase coffee.\nTo determine which itemsets are frequent, we typically use three metrics: support, confidence, and lift\nSupport Support measures the frequency of occurrence of a particular itemset in the dataset. It is calculated as the number of transactions containing the itemset divided by the total number of transactions in the dataset. A high support value indicates that the itemset occurs frequently in the dataset and is therefore a good candidate for generating association rules.\nFor example, if we have 100 transactions in total and 20 of them include ü•ê and ü•ñ together, the support for the association rule ‚Äúü•ê - ü•ñ‚Äù would be 20/100 = 0.2 or 20%.\nConfidence Confidence measures the strength of the relationship between two items in an association rule. It is calculated as the number of transactions containing both items in the rule divided by the total number of transactions containing the first item in the rule. A high confidence value indicates that the second item is likely to be purchased when the first item is purchased.\nFor example, if we have 50 transactions that include ü•ê, and 20 of those transactions also include ü•ñ, the confidence for the association rule ‚Äúü•ê - ü•ñ‚Äù would be 20/50 = 0.4 or 40%.\nLift Lift measures the degree of association between two items in an association rule, relative to the frequency of occurrence of both items. It is calculated as the support of the itemset containing both items divided by the product of the supports of the individual items in the rule. A lift value greater than 1 indicates a positive association between the items, while a value less than 1 indicates a negative association.\nFor example, if the support for ‚Äúü•ê - ü•ñ‚Äù is 0.2 (as in the example above), and the support for ü•ê alone is 0.6 and the support for ü•ñ alone is 0.5, then the lift for ‚Äúü•ê - ü•ñ‚Äù would be (0.2) / (0.6 x 0.5) = 0.67.\nApriori Algorithm The Apriori algorithm is widely used in association rule mining because of its efficiency in discovering interesting relationships between items in large datasets. It employs a bottom-up approach to mine frequent itemsets and generates association rules that describe the relationships between them.\nThe Apriori algorithm prunes the search space of itemsets to focus only on those that are frequent, which significantly reduces the computational resources and time required to find frequent itemsets. This pruning helps to avoid the generation of irrelevant rules that would not be useful in practical applications.\nThe algorithm has two phases, where the first phase involves identifying all frequent itemsets that occur above a minimum threshold, also known as the support threshold. The second phase involves generating association rules between these frequent itemsets based on their support and confidence.\nThe algorithm starts by identifying all individual items in the dataset and their frequency of occurrence. It then generates candidate itemsets of length two by joining pairs of frequent items. It continues to generate candidate itemsets of increasing length until no new frequent itemsets are found.\nIn the second phase, the algorithm generates association rules between the frequent itemsets. An association rule is a statement of the form A - B, where A and B are itemsets. The support of an association rule is the proportion of transactions in the dataset that contain both A and B, while the confidence of the rule is the proportion of transactions containing A that also contain B.\nThe Apriori algorithm uses support and confidence thresholds to filter out weak association rules. A high support threshold ensures that only frequent itemsets are considered, while a high confidence threshold ensures that only strong association rules are generated.\nPython Implementation We will use ‚ÄúThe Bread Basket‚Äù dataset from Kaggle. The dataset belongs to a bakery located in Edinburgh. The dataset has over 9000 transactions. For this analysis, we will only be using the Transaction and Item columns.\nimport pandas as pd\rfrom mlxtend.frequent_patterns import apriori, association_rules\rdf = pd.read_csv('data/bread basket.csv')\rdf = df[['Transaction', 'Item']]\r The resulting dataframe is pivoted to create a matrix where the rows are transactions, the columns are unique items, and the values are the counts of each item in each transaction.\n# group the df by transaction and item, count the number of each item in each transaction\rbasket = df.groupby(\rby=['Transaction', 'Item']\r)['Item'].count().reset_index(name='Count')\r# pivot the table to have transaction as rows and item names as columns\rbasket = basket.pivot_table(\rindex='Transaction', columns='Item', values='Count', aggfunc='sum').fillna(0).applymap(lambda x: 1 if x  0 else 0)\r Next, we use the Apriori algorithm from the mlxtend library to generate frequent itemsets. The min_support parameter is set to 0.01, which means that an itemset must appear in at least 1% of all transactions to be considered frequent. After generating frequent itemsets, we use the association_rules function from the same library to generate association rules between the frequent itemsets. The min_threshold parameter is set to 0.5, which means that only association rules with a confidence score of 50% or higher will be considered. Finally, we sort the association rules by lift and select the top 10 rules based on the highest lift score.\n# create frequent itemsets with a support of at least 0.01\rfrequent_itemsets = apriori(basket, min_support=0.01, use_colnames=True)\r# generate association rules with a minimum confidence of 0.5\rassociation_rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\r# print the top 10 association rules by lift\rtop_10_rules = association_rules.sort_values(by='lift', ascending=False).head(10)\rprint(top_10_rules)\r Based on the market basket analysis performed on the dataset, we were able to generate a set of association rules that have a confidence greater than 0.5 and a positive lift value. Among the rules generated, we identified the top 10 rules that were particularly important and interesting.\nTakeaways 1. Market Basket Analysis is a powerful data mining technique that can reveal patterns and relationships between products and customer preferences by analyzing large datasets such as purchase history.\n2. Association Rule Mining is a key technique used in Market Basket Analysis to identify frequent itemsets and generate association rules that describe the relationships between them.\n3. An association rule has two parts: an antecedent and a consequent, which are connected by the symbol ‚Äú=‚Äù.\n4. The Apriori algorithm is a widely used algorithm in association rule mining, as it efficiently discovers interesting relationships between items in large datasets.\n5. The Apriori algorithm prunes the search space of itemsets to focus only on those that are frequent, which significantly reduces the computational resources and time required to find frequent itemsets.\n6. The algorithm has two phases: identifying all frequent itemsets that occur above a minimum threshold and generating association rules between these frequent itemsets based on their support and confidence.\n7. Support, confidence, and lift are metrics used to determine which itemsets are frequent and to filter out weak association rules.\n8. A high support value indicates that the itemset occurs frequently in the dataset, while a high confidence value indicates that the second item is likely to be purchased when the first item is purchased.\n9. Lift measures the degree of association between two items in an association rule, relative to the frequency of occurrence of both items. A lift value greater than 1 indicates a positive association between the items, while a value less than 1 indicates a negative association.\n10. The Apriori algorithm uses support and confidence thresholds to filter out weak association rules, ensuring that only frequent and strong association rules are generated.\n","wordCount":"1551","inLanguage":"en","datePublished":"2023-05-29T11:31:18+08:00","dateModified":"2023-05-29T11:31:18+08:00","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/market_basket_analysis/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Posts><span>Posts</span></a></li><li><a href=https://outlierblog.me/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://outlierblog.me/notebooks/ title=Notebooks><span>Notebooks</span></a></li><li><a href=https://outlierblog.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Market Basket Analysis With Apriori Algorithm</h1><div class=post-meta><span title="2023-05-29 11:31:18 +0800 +08">May 29, 2023</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#association-rule-mining>Association Rule Mining</a><ul><li><a href=#support>Support</a></li><li><a href=#confidence>Confidence</a></li><li><a href=#lift>Lift</a></li></ul></li><li><a href=#apriori-algorithm>Apriori Algorithm</a></li><li><a href=#python-implementation>Python Implementation</a></li><li><a href=#takeaways>Takeaways</a></li></ul></nav></div></details></div><div class=post-content><p>Market Basket Analysis is a powerful data mining technique that enables businesses to gain insights into customer purchasing patterns. By analyzing large datasets, such as purchase history, businesses can reveal product groupings and customer preferences. This technique became even more accessible with the adoption of electronic point-of-sale (POS) systems, which made it easier to collect transactional data and analyze customer behavior.</p><p>Market Basket Analysis is especially useful in industries like retail, e-commerce, and grocery, where customer behavior is complex and influenced by various factors. By identifying patterns and relationships between products, businesses can improve sales and customer satisfaction.</p><p>In this blog post, we will explore the use of association rule mining in Market Basket Analysis, with a focus on one of the most popular algorithms - the Apriori Algorithm. We will delve into the inner workings of the algorithm and understand how it can be applied to reveal patterns in customer purchase behavior.</p><h2 id=association-rule-mining>Association Rule Mining</h2><p>One of the key techniques used in Market Basket Analysis is Association Rule Mining. Association Rule Mining is a powerful technique that can be used to identify interesting relationships between items in large datasets. It involves identifying frequent itemsets, which are sets of items that are often purchased or appear together in transactions, and then generating association rules that describe the relationships between these itemsets.</p><p>An association rule has two parts: an antecedent and a consequent. The antecedent is the set of items that appear on the left-hand side of the rule, while the consequent is the set of items that appear on the right-hand side. The antecedent and consequent are connected by the symbol &ldquo;=>&rdquo; which means &ldquo;implies&rdquo;.</p><p>For example, consider the following association rule: {üçû, üç∞} => {‚òï}. In this rule, {üçû, üç∞} is the antecedent and {‚òï} is the consequent. This rule implies that customers who purchase bread and cake together are also likely to purchase coffee.</p><p>To determine which itemsets are frequent, we typically use three metrics: support, confidence, and lift</p><h3 id=support>Support</h3><p>Support measures the frequency of occurrence of a particular itemset in the dataset. It is calculated as the number of transactions containing the itemset divided by the total number of transactions in the dataset. A high support value indicates that the itemset occurs frequently in the dataset and is therefore a good candidate for generating association rules.</p><p><img loading=lazy src=support.png alt></p><p>For example, if we have 100 transactions in total and 20 of them include ü•ê and ü•ñ together, the support for the association rule &ldquo;ü•ê -> ü•ñ&rdquo; would be 20/100 = 0.2 or 20%.</p><h3 id=confidence>Confidence</h3><p>Confidence measures the strength of the relationship between two items in an association rule. It is calculated as the number of transactions containing both items in the rule divided by the total number of transactions containing the first item in the rule. A high confidence value indicates that the second item is likely to be purchased when the first item is purchased.</p><p><img loading=lazy src=confidence.png alt></p><p>For example, if we have 50 transactions that include ü•ê, and 20 of those transactions also include ü•ñ, the confidence for the association rule &ldquo;ü•ê -> ü•ñ&rdquo; would be 20/50 = 0.4 or 40%.</p><h3 id=lift>Lift</h3><p>Lift measures the degree of association between two items in an association rule, relative to the frequency of occurrence of both items. It is calculated as the support of the itemset containing both items divided by the product of the supports of the individual items in the rule. A lift value greater than 1 indicates a positive association between the items, while a value less than 1 indicates a negative association.</p><p><img loading=lazy src=lift.png alt></p><p>For example, if the support for &ldquo;ü•ê -> ü•ñ&rdquo; is 0.2 (as in the example above), and the support for ü•ê alone is 0.6 and the support for ü•ñ alone is 0.5, then the lift for &ldquo;ü•ê -> ü•ñ&rdquo; would be (0.2) / (0.6 x 0.5) = 0.67.</p><h2 id=apriori-algorithm>Apriori Algorithm</h2><p>The Apriori algorithm is widely used in association rule mining because of its efficiency in discovering interesting relationships between items in large datasets. It employs a bottom-up approach to mine frequent itemsets and generates association rules that describe the relationships between them.</p><p>The Apriori algorithm prunes the search space of itemsets to focus only on those that are frequent, which significantly reduces the computational resources and time required to find frequent itemsets. This pruning helps to avoid the generation of irrelevant rules that would not be useful in practical applications.</p><p>The algorithm has two phases, where the first phase involves identifying all frequent itemsets that occur above a minimum threshold, also known as the support threshold. The second phase involves generating association rules between these frequent itemsets based on their support and confidence.</p><p>The algorithm starts by identifying all individual items in the dataset and their frequency of occurrence. It then generates candidate itemsets of length two by joining pairs of frequent items. It continues to generate candidate itemsets of increasing length until no new frequent itemsets are found.</p><p>In the second phase, the algorithm generates association rules between the frequent itemsets. An association rule is a statement of the form A -> B, where A and B are itemsets. The support of an association rule is the proportion of transactions in the dataset that contain both A and B, while the confidence of the rule is the proportion of transactions containing A that also contain B.</p><p>The Apriori algorithm uses support and confidence thresholds to filter out weak association rules. A high support threshold ensures that only frequent itemsets are considered, while a high confidence threshold ensures that only strong association rules are generated.</p><h2 id=python-implementation>Python Implementation</h2><p>We will use ‚ÄúThe Bread Basket‚Äù dataset from Kaggle. The dataset belongs to a bakery located in Edinburgh. The dataset has over 9000 transactions. For this analysis, we will only be using the <code>Transaction</code> and <code>Item</code> columns.</p><pre><code class=language-python>import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

df = pd.read_csv('data/bread basket.csv')

df = df[['Transaction', 'Item']]
</code></pre><p>The resulting dataframe is pivoted to create a matrix where the rows are transactions, the columns are unique items, and the values are the counts of each item in each transaction.</p><pre><code class=language-python># group the df by transaction and item, count the number of each item in each transaction
basket = df.groupby(
    by=['Transaction', 'Item']
    )['Item'].count().reset_index(name='Count')

# pivot the table to have transaction as rows and item names as columns
basket = basket.pivot_table(
    index='Transaction', 
    columns='Item', 
    values='Count', 
    aggfunc='sum').fillna(0).applymap(lambda x: 1 if x &gt; 0 else 0)
</code></pre><p><img loading=lazy src=sample.png alt></p><p>Next, we use the Apriori algorithm from the <code>mlxtend</code> library to generate frequent itemsets. The <code>min_support</code> parameter is set to 0.01, which means that an itemset must appear in at least 1% of all transactions to be considered frequent. After generating frequent itemsets, we use the <code>association_rules</code> function from the same library to generate association rules between the frequent itemsets. The <code>min_threshold</code> parameter is set to 0.5, which means that only association rules with a confidence score of 50% or higher will be considered. Finally, we sort the association rules by lift and select the top 10 rules based on the highest lift score.</p><pre><code class=language-python># create frequent itemsets with a support of at least 0.01
frequent_itemsets = apriori(basket, min_support=0.01, use_colnames=True)

# generate association rules with a minimum confidence of 0.5
association_rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)

# print the top 10 association rules by lift
top_10_rules = association_rules.sort_values(by='lift', ascending=False).head(10)
print(top_10_rules)
</code></pre><p><img loading=lazy src=apriori.png alt></p><p>Based on the market basket analysis performed on the dataset, we were able to generate a set of association rules that have a confidence greater than 0.5 and a positive lift value. Among the rules generated, we identified the top 10 rules that were particularly important and interesting.</p><h2 id=takeaways>Takeaways</h2><p><strong>1.</strong> Market Basket Analysis is a powerful data mining technique that can reveal patterns and relationships between products and customer preferences by analyzing large datasets such as purchase history.</p><p><strong>2.</strong> Association Rule Mining is a key technique used in Market Basket Analysis to identify frequent itemsets and generate association rules that describe the relationships between them.</p><p><strong>3.</strong> An association rule has two parts: an antecedent and a consequent, which are connected by the symbol &ldquo;=>&rdquo;.</p><p><strong>4.</strong> The Apriori algorithm is a widely used algorithm in association rule mining, as it efficiently discovers interesting relationships between items in large datasets.</p><p><strong>5.</strong> The Apriori algorithm prunes the search space of itemsets to focus only on those that are frequent, which significantly reduces the computational resources and time required to find frequent itemsets.</p><p><strong>6.</strong> The algorithm has two phases: identifying all frequent itemsets that occur above a minimum threshold and generating association rules between these frequent itemsets based on their support and confidence.</p><p><strong>7.</strong> Support, confidence, and lift are metrics used to determine which itemsets are frequent and to filter out weak association rules.</p><p><strong>8.</strong> A high support value indicates that the itemset occurs frequently in the dataset, while a high confidence value indicates that the second item is likely to be purchased when the first item is purchased.</p><p><strong>9.</strong> Lift measures the degree of association between two items in an association rule, relative to the frequency of occurrence of both items. A lift value greater than 1 indicates a positive association between the items, while a value less than 1 indicates a negative association.</p><p><strong>10.</strong> The Apriori algorithm uses support and confidence thresholds to filter out weak association rules, ensuring that only frequent and strong association rules are generated.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://outlierblog.me/tags/data-mining/>data mining</a></li><li><a href=https://outlierblog.me/tags/predictive-analytics/>predictive analytics</a></li></ul><nav class=paginav><a class=prev href=https://outlierblog.me/posts/customer_segmentation/><span class=title>¬´ Prev</span><br><span>Customer Segmentation using K-means Clustering in Python</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>