<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Monte Carlo Method | Outlier Blog</title><meta name=keywords content="reinforcement-learning"><meta name=description content="The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/monte_carlo_method/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/javascript>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js integrity=sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="Monte Carlo Method"><meta property="og:description" content="The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/monte_carlo_method/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-11T12:36:24+08:00"><meta property="article:modified_time" content="2023-09-11T12:36:24+08:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Monte Carlo Method"><meta name=twitter:description content="The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"Monte Carlo Method","item":"https://outlierblog.me/posts/monte_carlo_method/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Monte Carlo Method","name":"Monte Carlo Method","description":"The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment.","keywords":["reinforcement-learning"],"articleBody":"The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment. Learning from actual experience doesn’t require prior knowledge of the environment’s dynamics and can lead to optimal behavior. Learning from simulated experience can also be powerful, requiring only the generation of sample transitions, not the complete probability distributions needed for dynamic programming (DP).\nMonte Carlo Prediction When estimating the value of a state under a policy $\\pi$, Monte Carlo methods typically average the returns observed after visiting that state. This averaging process converges to the expected value as more returns are collected. This concept forms the basis for all Monte Carlo methods.\nFor example, when estimating $v_\\pi(s)$, the value of state $s$ under policy $\\pi$, based on a set of episodes following $\\pi$ and passing through $s$. Each occurrence of state $s$ in an episode is called a visit to $s$. The first-visit MC method estimates $v_\\pi(s)$ as the average of the returns following first visits to $s$, whereas the every-visit MC method averages the returns following all visits to $s$. Both first-visit MC and every-visit MC converge to $v_\\pi(s)$ as the number of visits to $s$tends toward infinity.\nMonte Carlo Estimation For action values, the policy evaluation problem involves estimating $q_\\pi(s, a)$, the expected return when starting in state $s$, taking action $a$, and following policy $\\pi$. The methods for this are similar to those used for state values but now involve state-action pairs.\nOne complication of this method is that many state-action pairs may never be visited. If $\\pi$ is a deterministic policy, then in following $\\pi$, we will observe returns only for one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. This is a serious problem because the purpose of learning action values is to help in choosing among the actions available in each state. To compare alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor.\nFor policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. This is called the assumption of exploring starts.\nMonte Carlo Control Monte Carlo estimation can be applied to control problems to approximate optimal policies. This involves maintaining both an approximate policy and an approximate value function. The value function is iteratively updated to better approximate the value function for the current policy, while the policy is improved with respect to the current value function. These two processes may have opposing effects, but they collectively guide both policy and value function towards optimality.\n$$ \\pi_0 \\xrightarrow{\\text{E}} q_{\\pi0} \\xrightarrow{\\text{I}} \\pi_1 \\xrightarrow{\\text{E}} q_{\\pi1} \\xrightarrow{\\text{I}} \\pi_2 \\xrightarrow{\\text{E}} … \\xrightarrow{\\text{I}} \\pi_* \\xrightarrow{\\text{E}} q_* $$\nwhere $\\xrightarrow{\\text{E}}$ denotes a complete policy evaluation and $\\xrightarrow{\\text{I}}$ denotes a complete policy improvement. Policy evaluation is done as described in the preceding section. Policy improvement is done by making the policy greedy with respect to the current value function. For any action-value function $q$, the corresponding greedy policy is the one that deterministically chooses an action with maximum action-value.\n$$ \\pi(s) = \\arg \\max_a q(s, a) $$\nPolicy improvement then can be done by constructing each $\\pi_{k+1}$ as the greedy policy with respect to $q_{\\pi_k}$. The policy improvement theorem then applies to $\\pi_k$ and $\\pi_{k+1}$ because, for all $s \\in S$,\n$$ \\begin{align} q_{\\pi_k}(s, \\pi_{k+1}(s)) \u0026= q_{\\pi_k}(s, \\arg \\max_a q_{\\pi_k}(s, a)) \\\\ \u0026= \\max_a q_{\\pi_k}(s, a) \\\\ \u0026\\geq q_{\\pi_k}(s, \\pi_k(s)) \\\\ \u0026\\geq v_{\\pi_k}(s) \\end{align} $$\nThe policy improvement theorem ensures that each updated policy ($\\pi_{k+1}$) is uniformly better or equally good as the previous one ($\\pi_k$). This process continues until convergence to the optimal policy and optimal value function is achieved.\nControl without Exploring Starts Control without the need for exploring initial states is a goal in reinforcement learning to avoid relying on the impractical assumption of starting from all possible initial conditions. Achieving this entails ensuring that the agent continues to select all possible actions throughout its learning process. There are two principal approaches to guaranteeing this: on-policy methods and off-policy methods. On-policy methods aim to evaluate or improve the policy used to make decisions, while off-policy methods involve evaluating or improving a policy that differs from the one used to collect the data.\nIn on-policy methods, the policy is typically “soft,”, meaning that $\\pi(a \\mid s) \\gt 0$ for all $s \\in S$ and $a \\in A(s)$. Over time, the policy gradually transitions closer to a deterministic optimal policy. An example of a soft policy is the $\\epsilon\\text{-greedy}$ policies, where most of the time, the agent selects the action with the highest estimated value, but occasionally, with a probability $\\epsilon$, the agent choose random action instead. All non-greedy actions are given the minimal probability of selection, $\\epsilon \\over \\left|A(s)\\right|$ , while the remaining probability, $1 - \\epsilon + {\\epsilon \\over \\left|A(s)\\right|}$, is given to the greedy action.\nOff-policy Prediction via Importance Sampling All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? A more straightforward approach is to employ two policies, one that is being learned (the target policy) and becomes optimal over time, and another that is more exploratory and generates the agent’s behavior (the behavior policy). Learning occurs “off” the target policy, and this overall process is termed off-policy learning.\nWe start by considering the prediction problem, in which both target and behavior policies are fixed. Suppose we wish to estimate $v_\\pi$ or $q_\\pi$, but all we have are episodes following another policy $b$, where $b \\neq \\pi$. In this case, $\\pi$ is the target policy, and $b$ is the behavior policy. In order to use episodes from $b$ to estimate values for $\\pi$, we require that every action taken under $\\pi$ is also taken, at least occasionally, under $b$. This is called the assumption of coverage. It follows from coverage that $b$ must be stochastic in states where it is not identical to $\\pi$. The target policy $\\pi$, on the other hand, may be deterministic.\nAlmost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state $S_t$, the probability of the subsequent state-action trajectory, $A_t, S_{t+1}, A_{t+1}, …, S_T$, occurring under any policy $\\pi$ is,\n$$ \\begin{align} Pr{A_t, S_{t+1}, A_{t+1}, …, S_T \\mid S_t, A_{t:T-1} \\sim \\pi} \u0026= \\pi(A_t \\mid S_t)p(S_{t+1} \\mid S_t, A_t)\\pi(A_{t+1} \\mid S_{t+1})…p(S_T \\mid S_{T-1}, A_{T-1}) \\\\ \u0026= \\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k) \\end{align} $$\nwhere $p$ is the state-transition probability function. Thus, the importance sampling ratio is,\n$$ \\begin{align} \\rho_{t:T-1} \u0026= {\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k) \\over \\prod_{k=t}^{T-1} b(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k)} \\\\ \u0026= \\prod_{k=t}^{T-1}{\\pi(A_k \\mid S_k) \\over b(A_k \\mid S_k)} \\end{align} $$\nRecall that we wish to estimate the expected returns (values) under the target policy, but all we have are returns $G_t$ due to the behavior policy. These returns have the wrong expectation and cannot be averaged to obtain $V_\\pi$. This is where importance sampling comes in. The ratio $\\rho_{t:T-1}$transforms the returns to have the right expected value:\n$$ \\mathbb{E}[\\rho_{t:T-1}G_t \\mid S_t=s] = v_\\pi(s) $$\nThere are two main types of importance sampling: ordinary importance sampling and weighted importance sampling. In ordinary importance sampling, we use the sampled values to estimate expected values or probabilities under the target distribution. Let $\\mathcal{T}(s)$ denote the set of all time steps in which state $s$ is visited. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up through $T(t)$. Then ${G_t}{t \\in \\mathcal{T}(s)}$ are the returns that pertain to state $s$, and ${\\rho{t:T(t)-1}}{t \\in \\mathcal{T}(s)}$ are the importance-sampling ratios. To estimate $v\\pi(s)$, we scale the returns by the ratios and average the results.\n$$ V(S) = {\\sum_{t \\in \\mathcal{T}(s) } \\rho_{t:T(t)-1} G_t \\over \\left|\\mathcal{T}(s)\\right|} $$\nWeighted importance sampling improves upon ordinary importance sampling by introducing weights that adjust for the discrepancies between the importance distribution and the target distribution. These weights ensure that samples drawn from the importance distribution contribute appropriately to the estimate.\n$$ V(S) = {\\sum_{t \\in \\mathcal{T}(s) } \\rho_{t:T(t)-1} G_t \\over \\sum_{t \\in \\mathcal{T}(s) } \\rho_{t:T(t)-1}} $$\nOrdinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one.\nReferences [1] Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto\n","wordCount":"1604","inLanguage":"en","datePublished":"2023-09-11T12:36:24+08:00","dateModified":"2023-09-11T12:36:24+08:00","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/monte_carlo_method/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Posts><span>Posts</span></a></li><li><a href=https://outlierblog.me/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://outlierblog.me/notebooks/ title=Notebooks><span>Notebooks</span></a></li><li><a href=https://outlierblog.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Monte Carlo Method</h1><div class=post-meta><span title='2023-09-11 12:36:24 +0800 +08'>September 11, 2023</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#monte-carlo-prediction>Monte Carlo Prediction</a></li><li><a href=#monte-carlo-estimation>Monte Carlo Estimation</a></li><li><a href=#monte-carlo-control>Monte Carlo Control</a></li><li><a href=#control-without-exploring-starts>Control without Exploring Starts</a></li><li><a href=#off-policy-prediction-via-importance-sampling>Off-policy Prediction via Importance Sampling</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>The Monte Carlo method represents a reinforcement learning technique used to estimate the value of a state or state-action pair by averaging the returns achieved from those states or state-actions. Unlike traditional methods that rely on iterative updates, Monte Carlo methods learn from complete episodes, making them well-suited for tasks with a defined endpoint. These methods accumulate knowledge from experience, collecting sequences of states, actions, and rewards from real or simulated interactions with an environment. Learning from actual experience doesn&rsquo;t require prior knowledge of the environment&rsquo;s dynamics and can lead to optimal behavior. Learning from simulated experience can also be powerful, requiring only the generation of sample transitions, not the complete probability distributions needed for dynamic programming (DP).</p><h3 id=monte-carlo-prediction>Monte Carlo Prediction</h3><p>When estimating the value of a state under a policy $\pi$, Monte Carlo methods typically average the returns observed after visiting that state. This averaging process converges to the expected value as more returns are collected. This concept forms the basis for all Monte Carlo methods.</p><p>For example, when estimating $v_\pi(s)$, the value of state $s$ under policy $\pi$, based on a set of episodes following $\pi$ and passing through $s$. Each occurrence of state $s$ in an episode is called a <em>visit</em> to $s$. The first-visit MC method estimates $v_\pi(s)$ as the average of the returns following
first visits to $s$, whereas the every-visit MC method averages the returns following all
visits to $s$. Both first-visit MC and every-visit MC converge to $v_\pi(s)$ as the number of visits to $s$tends toward infinity.</p><h3 id=monte-carlo-estimation>Monte Carlo Estimation</h3><p>For action values, the policy evaluation problem involves estimating $q_\pi(s, a)$, the expected return when starting in state $s$, taking action $a$, and following policy $\pi$. The methods for this are similar to those used for state values but now involve state-action pairs.</p><p>One complication of this method is that many state-action pairs may never be visited. If $\pi$ is
a deterministic policy, then in following $\pi$, we will observe returns only for one of the
actions from each state. With no returns to average, the Monte Carlo estimates of the
other actions will not improve with experience. This is a serious problem because the
purpose of learning action values is to help in choosing among the actions available in
each state. To compare alternatives we need to estimate the value of all the actions from
each state, not just the one we currently favor.</p><p>For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. This is called the assumption of exploring starts.</p><p><img loading=lazy src=images/exploring_start.png alt></p><h3 id=monte-carlo-control>Monte Carlo Control</h3><p>Monte Carlo estimation can be applied to control problems to approximate optimal policies. This involves maintaining both an approximate policy and an approximate value function. The value function is iteratively updated to better approximate the value function for the current policy, while the policy is improved with respect to the current value function. These two processes may have opposing effects, but they collectively guide both policy and value function towards optimality.</p><p>$$
\pi_0 \xrightarrow{\text{E}} q_{\pi0} \xrightarrow{\text{I}} \pi_1 \xrightarrow{\text{E}} q_{\pi1} \xrightarrow{\text{I}} \pi_2 \xrightarrow{\text{E}} &mldr;
\xrightarrow{\text{I}} \pi_* \xrightarrow{\text{E}} q_*
$$</p><p>where $\xrightarrow{\text{E}}$ denotes a complete policy evaluation and $\xrightarrow{\text{I}}$ denotes a complete policy improvement. Policy evaluation is done as described in the preceding section. Policy improvement is done by making the policy greedy with respect to the current value function. For any action-value function $q$, the corresponding greedy policy is the one that deterministically chooses an action with maximum action-value.</p><p>$$
\pi(s) = \arg \max_a q(s, a)
$$</p><p>Policy improvement then can be done by constructing each $\pi_{k+1}$ as the greedy policy with respect to $q_{\pi_k}$. The policy improvement theorem then applies to $\pi_k$ and $\pi_{k+1}$ because, for all $s \in S$,</p><p>$$
\begin{align}
q_{\pi_k}(s, \pi_{k+1}(s)) &= q_{\pi_k}(s, \arg \max_a q_{\pi_k}(s, a)) \\
&= \max_a q_{\pi_k}(s, a) \\
&\geq q_{\pi_k}(s, \pi_k(s)) \\
&\geq v_{\pi_k}(s)
\end{align}
$$</p><p>The policy improvement theorem ensures that each updated policy ($\pi_{k+1}$) is uniformly better or equally good as the previous one ($\pi_k$). This process continues until convergence to the optimal policy and optimal value function is achieved.</p><h3 id=control-without-exploring-starts>Control without Exploring Starts</h3><p>Control without the need for exploring initial states is a goal in reinforcement learning to avoid relying on the impractical assumption of starting from all possible initial conditions. Achieving this entails ensuring that the agent continues to select all possible actions throughout its learning process. There are two principal approaches to guaranteeing this: on-policy methods and off-policy methods. On-policy methods aim to evaluate or improve the policy used to make decisions, while off-policy methods involve evaluating or improving a policy that differs from the one used to collect the data.</p><p>In on-policy methods, the policy is typically &ldquo;soft,”, meaning that $\pi(a \mid s) \gt 0$ for all $s \in S$ and $a \in A(s)$. Over time, the policy gradually transitions closer to a deterministic optimal policy. An example of a soft policy is the $\epsilon\text{-greedy}$ policies, where most of the time, the agent selects the action with the highest estimated value, but occasionally, with a probability $\epsilon$, the agent choose random action instead. All non-greedy actions are given the minimal probability of selection, $\epsilon \over \left|A(s)\right|$ , while the remaining probability, $1 - \epsilon + {\epsilon \over \left|A(s)\right|}$, is given to the greedy action.</p><p><img loading=lazy src=images/soft_policy.png alt></p><h3 id=off-policy-prediction-via-importance-sampling>Off-policy Prediction via Importance Sampling</h3><p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? A more straightforward approach is to employ two policies, one that is being learned (the target policy) and becomes optimal over time, and another that is more exploratory and generates the agent&rsquo;s behavior (the behavior policy). Learning occurs &ldquo;off&rdquo; the target policy, and this overall process is termed off-policy learning.</p><p>We start by considering the prediction problem, in which both target and behavior policies are fixed. Suppose we wish to estimate $v_\pi$ or $q_\pi$, but all we have are episodes following another policy $b$, where $b \neq \pi$. In this case, $\pi$ is the target policy, and $b$ is the behavior policy. In order to use episodes from $b$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $b$. This is called the assumption of coverage. It follows from coverage that $b$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic.</p><p>Almost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state $S_t$, the probability of the subsequent state-action trajectory, $A_t, S_{t+1}, A_{t+1}, &mldr;, S_T$, occurring under any policy $\pi$ is,</p><p>$$
\begin{align}
Pr{A_t, S_{t+1}, A_{t+1}, &mldr;, S_T \mid S_t, A_{t:T-1} \sim \pi}
&= \pi(A_t \mid S_t)p(S_{t+1} \mid S_t, A_t)\pi(A_{t+1} \mid S_{t+1})&mldr;p(S_T \mid S_{T-1}, A_{T-1}) \\
&= \prod_{k=t}^{T-1} \pi(A_k \mid S_k)p(S_{k+1} \mid S_k, A_k)
\end{align}
$$</p><p>where $p$ is the state-transition probability function. Thus, the importance sampling ratio is,</p><p>$$
\begin{align}
\rho_{t:T-1} &= {\prod_{k=t}^{T-1} \pi(A_k \mid S_k)p(S_{k+1} \mid S_k, A_k) \over \prod_{k=t}^{T-1} b(A_k \mid S_k)p(S_{k+1} \mid S_k, A_k)} \\
&= \prod_{k=t}^{T-1}{\pi(A_k \mid S_k) \over b(A_k \mid S_k)}
\end{align}
$$</p><p>Recall that we wish to estimate the expected returns (values) under the target policy, but all we have are returns $G_t$ due to the behavior policy. These returns have the wrong expectation and cannot be averaged to obtain $V_\pi$. This is where importance sampling comes in. The ratio $\rho_{t:T-1}$transforms the returns to have the right expected value:</p><p>$$
\mathbb{E}[\rho_{t:T-1}G_t \mid S_t=s] = v_\pi(s)
$$</p><p>There are two main types of importance sampling: ordinary importance sampling and weighted importance sampling. In ordinary importance sampling, we use the sampled values to estimate expected values or probabilities under the target distribution. Let $\mathcal{T}(s)$ denote the set of all time steps in which state $s$ is visited. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up through $T(t)$. Then ${G_t}<em>{t \in \mathcal{T}(s)}$ are the returns that pertain to state $s$, and ${\rho</em>{t:T(t)-1}}<em>{t \in \mathcal{T}(s)}$ are the importance-sampling ratios. To estimate $v</em>\pi(s)$, we scale the returns by the ratios and average the results.</p><p>$$
V(S) = {\sum_{t \in \mathcal{T}(s) } \rho_{t:T(t)-1} G_t \over \left|\mathcal{T}(s)\right|}
$$</p><p>Weighted importance sampling improves upon ordinary importance sampling by introducing weights that adjust for the discrepancies between the importance distribution and the target distribution. These weights ensure that samples drawn from the importance distribution contribute appropriately to the estimate.</p><p>$$
V(S) = {\sum_{t \in \mathcal{T}(s) } \rho_{t:T(t)-1} G_t \over \sum_{t \in \mathcal{T}(s) } \rho_{t:T(t)-1}}
$$</p><p>Ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one.</p><h2 id=references>References</h2><p>[1] Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://outlierblog.me/tags/reinforcement-learning/>reinforcement-learning</a></li></ul><nav class=paginav><a class=next href=https://outlierblog.me/posts/multibandits/><span class=title>Next »</span><br><span>Multi-Armed Bandit Problem and Its Solutions</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>