<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Multi-Armed Bandit Problem and Its Solutions | Outlier Blog</title><meta name=keywords content="reinforcement-learning,exploration-exploitation"><meta name=description content="In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.
Imagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/multibandits/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/javascript>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js integrity=sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="Multi-Armed Bandit Problem and Its Solutions"><meta property="og:description" content="In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.
Imagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/multibandits/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-08T14:21:44+08:00"><meta property="article:modified_time" content="2023-10-08T14:21:44+08:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Multi-Armed Bandit Problem and Its Solutions"><meta name=twitter:description content="In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.
Imagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"Multi-Armed Bandit Problem and Its Solutions","item":"https://outlierblog.me/posts/multibandits/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Multi-Armed Bandit Problem and Its Solutions","name":"Multi-Armed Bandit Problem and Its Solutions","description":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.\nImagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one.","keywords":["reinforcement-learning","exploration-exploitation"],"articleBody":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.\nImagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one. In this setup, each arm provides a random reward from an unknown probability distribution. Our primary objective is to maximize the total reward obtained over a series of plays.\nAs we do not know the probability distributions, a straightforward strategy is to simply select the arm given a uniform distribution; that is, select each arm with the same probability. Over time, we will eventually manage to estimate the true reward probability according to the law of large numbers. But here’s the catch: we need to spend enormous time trying out every action. Why not we only focus on the most promising actions given the reward we received so far?\nExploration vs Exploitation We want to play only the good actions; so just keep playing the actions that have given us the best reward so far. However, at first, we do not have information to tell us what the best actions are. We need strategies that exploit what we think are the best actions so far, but still explore other actions.\nNow, the big question is: how much should we exploit and how much should we explore? This is known as the exploration vs exploitation dilemma. It’s tricky because we don’t have all the information we need. We want to gather enough data to make smart decisions overall while keeping the risks in check. Exploitation means using what we know works best, while exploration involves taking some risks to learn about actions we’re not familiar with.\nIn the context of the multi-armed bandit problem, we want exploration strategies that minimize the regret, which is the expected loss from not taking the best action. A zero-regret strategy is a strategy where the average regret of each round approaches zero as the number of rounds approaches infinity. This means, a zero-regret strategy will converge to an optimal strategy given enough rounds.\nBernoulli Bandit We are going to implement several exploration strategies for the simplest multi-armed bandit problem: Bernoulli Bandit. The bandit has $K$ actions. The action produces a reward, $r=1$, with probability $0 \\le \\theta_k \\le 1$, which is unknown to the agent, but fixed over time. The objective of the agent is to minimize regret over a fixed number of action selections, $T$.\n$$ \\rho = T \\theta^* - \\sum_{t=1}^T \\theta_{\\alpha_t} $$\n$$ \\text{where } \\theta^* = \\max_k{\\theta_k} \\text{; and } \\\\theta_{\\alpha_t} \\text{ corresponds to the chosen action } \\alpha_t \\text{ at each step} $$\nclass BernoulliBandit:\rdef __init__(self, n_actions=5):\rself._probs = np.random.random(n_actions)\r@property\rdef action_count(self):\rreturn len(self._probs)\rdef pull(self, action):\rif np.any(np.random.random() \u003e self._probs[action]):\rreturn 0.0\rreturn 1.0\rdef optimal_reward(self):\rreturn np.max(self._probs)\rdef action_value(self, action):\rreturn self._probs[action]\rdef step(self):\rpass\rdef reset(self):\rpass\rThe implementation for each strategy that will be discuss inherits from the AbstractAgent class:\nclass AbstractAgent(metaclass=ABCMeta):\rdef init_actions(self, n_actions):\rself._successes = np.zeros(n_actions)\rself._failures = np.zeros(n_actions)\rself._total_pulls = 0\r@abstractmethod\rdef get_action(self):\rpass\rdef update(self, action, reward):\rself._total_pulls += 1\rif reward == 1:\rself._successes[action] += 1\relse:\rself._failures[action] += 1\r@property\rdef name(self):\rreturn self.__class__.__name__\rEpsilon-greedy The epsilon-greedy strategy is a simple and effective way to balance exploration and exploitation. The parameter $\\epsilon \\in [0,1]$ controls how much the agent explores and how much will it exploit.\nAccording to this strategy, with a small probability $\\epsilon$, the agent takes a random action, but most of the time, with probability $1 - \\epsilon$, the agent will pick the best action learned so far. The best $\\epsilon$ value depends on the particular problem, but typically, values around 0.05 to 0.1 work very well.\nclass EpsilonGreedyAgent(AbstractAgent):\rdef __init__(self, epsilon=0.01):\rself._epsilon = epsilon\rdef get_action(self):\rif np.random.random() \u003c self._epsilon:\rreturn np.random.randint(len(self._successes))\relse:\rreturn np.argmax(self._successes / (self._successes + self._failures + 0.1))\rThe following plot shows the regret for each step, averaged over 10 trials.\nHigher values of epsilon tend to have a higher regret over time. Higher value means more exploration, so the agent spends more time exploring less valuable actions, even though it already has a good estimate of the value of actions. In this particular problem, the epsilon value of 0.05 to 0.1 is a reasonable choice.\nUpper Confidence Bound The epsilon-greedy strategy has no preference for actions and is inefficient in exploration. The agent might explore a bad action which is already been confirmed as a bad action in the past. It would be better to select among actions that are uncertain or have the potential to be optimal. One can come up with an idea of index for each action that represents optimality and uncertainty at the same time. One efficient way to do it is to use the UCB1 algorithm.\nIn each iteration, the agent assesses each available action’s potential by calculating a weight ($w_k$) that combines estimates of both optimality and uncertainty.\n$$ w_k = {\\alpha_k \\over \\alpha_k + \\beta_k} + \\sqrt{2 \\log t \\over \\alpha_k + \\beta_k} $$\nThe first term ${\\alpha_k \\over \\alpha_k + \\beta_k}$ represents the estimated success probability (optimality). The second term $\\sqrt{2 \\log t \\over \\alpha_k + \\beta_k}$ represents the uncertainty, encouraging exploration.\nAfter calculating weights for all actions, the agent then will choose with the maximum weight.\nclass UCBAgent(AbstractAgent):\rdef get_action(self):\rpulls = self._successes + self._failures + 0.1\rreturn np.argmax(self._successes / pulls + np.sqrt(2 * np.log(self._total_pulls + 0.1) / pulls))\rIn a static environment, epsilon-greedy might outperform UCB1 initially because epsilon-greedy is straightforward and tends to quickly focus on the arm with the highest estimated mean reward. UCB1, in contrast, might spend more time exploring and being cautious due to its confidence bounds.\nBut, in many real problems, the underlying probability distributions are not static. For example, suppose we employ a recommendation system for streaming content, using multi-armed bandit approach to decide which shows to suggest to users. In this scenario, the reward is measured by user engagement, specifically whether they watch the suggested show. The viewing preferences of our audience may evolve over time, influenced by factors such as trending genres, seasonal changes, and more.\nHere is an example of a nonstationary bandit where the reward probabilities change over time.\nclass DriftingBandit(BernoulliBandit):\rdef __init__(self, n_actions=5, gamma=0.01):\rsuper().__init__(n_actions)\rself._gamma = gamma\rself._successes = None\rself._failures = None\rself._steps = 0\rself.reset()\rdef reset(self):\rself._successes = np.zeros(self.action_count) + 1.0\rself._failures = np.zeros(self.action_count) + 1.0\rself._steps = 0\rdef step(self):\raction = np.random.randint(self.action_count)\rreward = self.pull(action)\rself._step(action, reward)\rdef _step(self, action, reward):\rself._successes = self._successes * (1 - self._gamma) + self._gamma\rself._failures = self._failures * (1 - self._gamma) + self._gamma\rself._steps += 1\rself._successes[action] += reward\rself._failures[action] += 1.0 - reward\rself._probs = np.random.beta(self._successes, self._failures)\rWe can see from the plot how the reward probabilities change over time.\nUCB1 shines in a changing environment because of its ability to adapt. As the distribution of rewards changes over time, UCB1 continues to explore arms with uncertain estimates, preventing it from getting stuck on a suboptimal arm.\nThompson Sampling Unlike the UCB1 algorithm, Thompson Sampling incorporates the actual distribution of rewards by sampling from a Beta distribution for each action. The Beta distribution is a flexible choice, as it is defined on the interval $[0, 1]$, making it suitable for representing probabilities.\nIn each iteration, the algorithm samples from a Beta distribution for each available action. These samples provide estimates of the true success probability for each action. The algorithm then selects the action with the highest sampled value. This approach allows Thompson Sampling to adapt to the true underlying distribution of rewards and make more informed decisions over time.\nclass ThompsonSamplingAgent(AbstractAgent):\rdef get_action(self):\rreturn np.argmax(np.random.beta(self._successes + 1, self._failures + 1))\rFrom these comparison plots, we can see that Thompson Sampling performs really well compared to epsilon-greedy and UCB1.\nIn a static environment, the algorithm continuously refines its probability distributions based on observed outcomes. As it converges to the true underlying distribution, the algorithm becomes adept at exploiting the arm with the highest expected reward.\nIn a dynamic environment, its ability to update beliefs in a Bayesian manner allows it to swiftly adapt to changes in the reward distribution.\n","wordCount":"1422","inLanguage":"en","datePublished":"2023-10-08T14:21:44+08:00","dateModified":"2023-10-08T14:21:44+08:00","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/multibandits/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Posts><span>Posts</span></a></li><li><a href=https://outlierblog.me/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://outlierblog.me/notebooks/ title=Notebooks><span>Notebooks</span></a></li><li><a href=https://outlierblog.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Multi-Armed Bandit Problem and Its Solutions</h1><div class=post-meta><span title='2023-10-08 14:21:44 +0800 +08'>October 8, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#exploration-vs-exploitation>Exploration vs Exploitation</a></li><li><a href=#bernoulli-bandit>Bernoulli Bandit</a></li><li><a href=#epsilon-greedy>Epsilon-greedy</a></li><li><a href=#upper-confidence-bound>Upper Confidence Bound</a></li><li><a href=#thompson-sampling>Thompson Sampling</a></li></ul></nav></div></details></div><div class=post-content><p>In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the <em>exploration vs exploitation dilemma</em>.</p><p>Imagine we are facing a row of slot machines (also called <a href=https://en.wiktionary.org/wiki/one-armed_bandit>one-armed bandits</a>). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one. In this setup, each arm provides a random reward from an unknown probability distribution. Our primary objective is to maximize the total reward obtained over a series of plays.</p><p>As we do not know the probability distributions, a straightforward strategy is to simply select the arm given a uniform distribution; that is, select each arm with the same probability. Over time, we will eventually manage to estimate the true reward probability according to the <a href=https://en.wikipedia.org/wiki/Law_of_large_numbers>law of large numbers</a>. But here&rsquo;s the catch: we need to spend enormous time trying out every action. Why not we only focus on the most promising actions given the reward we received so far?</p><h2 id=exploration-vs-exploitation>Exploration vs Exploitation</h2><p>We want to play only the good actions; so just keep playing the actions that have given us the best reward so far. However, at first, we do not have information to tell us what the best actions are. We need strategies that <em>exploit</em> what we think are the best actions so far, but still <em>explore</em> other actions.</p><p>Now, the big question is: how much should we exploit and how much should we explore? This is known as the exploration vs exploitation dilemma. It&rsquo;s tricky because we don&rsquo;t have all the information we need. We want to gather enough data to make smart decisions overall while keeping the risks in check. Exploitation means using what we know works best, while exploration involves taking some risks to learn about actions we&rsquo;re not familiar with.</p><p>In the context of the multi-armed bandit problem, we want exploration strategies that minimize the regret, which is the expected loss from not taking the best action. A zero-regret strategy is a strategy where the average regret of each round approaches zero as the number of rounds approaches infinity. This means, a zero-regret strategy will converge to an optimal strategy given enough rounds.</p><h2 id=bernoulli-bandit>Bernoulli Bandit</h2><p>We are going to implement several exploration strategies for the simplest multi-armed bandit problem: Bernoulli Bandit. The bandit has $K$ actions. The action produces a reward, $r=1$, with probability $0 \le \theta_k \le 1$, which is unknown to the agent, but fixed over time. The objective of the agent is to minimize regret over a fixed number of action selections, $T$.</p><p>$$
\rho = T \theta^* - \sum_{t=1}^T \theta_{\alpha_t}
$$</p><p>$$
\text{where } \theta^* = \max_k{\theta_k} \text{; and } \\theta_{\alpha_t} \text{ corresponds to the chosen action } \alpha_t \text{ at each step}
$$</p><pre><code class=language-python>class BernoulliBandit:
    def __init__(self, n_actions=5):
        self._probs = np.random.random(n_actions)

    @property
    def action_count(self):
        return len(self._probs)

    def pull(self, action):
        if np.any(np.random.random() &gt; self._probs[action]):
            return 0.0
        return 1.0

    def optimal_reward(self):
        return np.max(self._probs)

    def action_value(self, action):
        return self._probs[action]
    
    def step(self):
        pass

    def reset(self):
        pass
</code></pre><p>The implementation for each strategy that will be discuss inherits from the <code>AbstractAgent</code> class:</p><pre><code class=language-python>class AbstractAgent(metaclass=ABCMeta):
    def init_actions(self, n_actions):
        self._successes = np.zeros(n_actions)
        self._failures = np.zeros(n_actions)
        self._total_pulls = 0

    @abstractmethod
    def get_action(self):
        pass

    def update(self, action, reward):
        self._total_pulls += 1
        if reward == 1:
            self._successes[action] += 1
        else:
            self._failures[action] += 1

    @property
    def name(self):
        return self.__class__.__name__
</code></pre><h2 id=epsilon-greedy>Epsilon-greedy</h2><p>The epsilon-greedy strategy is a simple and effective way to balance exploration and exploitation. The parameter $\epsilon \in [0,1]$ controls how much the agent explores and how much will it exploit.</p><p>According to this strategy, with a small probability $\epsilon$, the agent takes a random action, but most of the time, with probability $1 - \epsilon$, the agent will pick the best action learned so far. The best $\epsilon$ value depends on the particular problem, but typically, values around 0.05 to 0.1 work very well.</p><pre><code class=language-python>class EpsilonGreedyAgent(AbstractAgent):
    def __init__(self, epsilon=0.01):
        self._epsilon = epsilon

    def get_action(self):
        if np.random.random() &lt; self._epsilon:
            return np.random.randint(len(self._successes))
        else:
            return np.argmax(self._successes / (self._successes + self._failures + 0.1))
</code></pre><p>The following plot shows the regret for each step, averaged over 10 trials.</p><p><img loading=lazy src=images/epsilon_greedy.png alt></p><p>Higher values of epsilon tend to have a higher regret over time. Higher value means more exploration, so the agent spends more time exploring less valuable actions, even though it already has a good estimate of the value of actions. In this particular problem, the epsilon value of 0.05 to 0.1 is a reasonable choice.</p><h2 id=upper-confidence-bound>Upper Confidence Bound</h2><p>The epsilon-greedy strategy has no preference for actions and is inefficient in exploration. The agent might explore a bad action which is already been confirmed as a bad action in the past. It would be better to select among actions that are uncertain or have the potential to be optimal. One can come up with an idea of index for each action that represents optimality and uncertainty at the same time. One efficient way to do it is to use the UCB1 algorithm.</p><p>In each iteration, the agent assesses each available action&rsquo;s potential by calculating a weight ($w_k$) that combines estimates of both optimality and uncertainty.</p><p>$$
w_k = {\alpha_k \over \alpha_k + \beta_k} + \sqrt{2 \log t \over \alpha_k + \beta_k}
$$</p><p>The first term ${\alpha_k \over \alpha_k + \beta_k}$ represents the estimated success probability (optimality). The second term $\sqrt{2 \log t \over \alpha_k + \beta_k}$ represents the uncertainty, encouraging exploration.</p><p>After calculating weights for all actions, the agent then will choose with the maximum weight.</p><pre><code class=language-python>class UCBAgent(AbstractAgent):
    def get_action(self):
        pulls = self._successes + self._failures + 0.1
        return np.argmax(self._successes / pulls + np.sqrt(2 * np.log(self._total_pulls + 0.1) / pulls))
</code></pre><p>In a static environment, epsilon-greedy might outperform UCB1 initially because epsilon-greedy is straightforward and tends to quickly focus on the arm with the highest estimated mean reward. UCB1, in contrast, might spend more time exploring and being cautious due to its confidence bounds.</p><p><img loading=lazy src=images/ucb_epsilon.png alt></p><p>But, in many real problems, the underlying probability distributions are not static. For example, suppose we employ a recommendation system for streaming content, using multi-armed bandit approach to decide which shows to suggest to users. In this scenario, the reward is measured by user engagement, specifically whether they watch the suggested show. The viewing preferences of our audience may evolve over time, influenced by factors such as trending genres, seasonal changes, and more.</p><p>Here is an example of a nonstationary bandit where the reward probabilities change over time.</p><pre><code class=language-python>class DriftingBandit(BernoulliBandit):
    def __init__(self, n_actions=5, gamma=0.01):
        super().__init__(n_actions)

        self._gamma = gamma

        self._successes = None
        self._failures = None
        self._steps = 0

        self.reset()

    def reset(self):
        self._successes = np.zeros(self.action_count) + 1.0
        self._failures = np.zeros(self.action_count) + 1.0
        self._steps = 0

    def step(self):
        action = np.random.randint(self.action_count)
        reward = self.pull(action)
        self._step(action, reward)

    def _step(self, action, reward):
        self._successes = self._successes * (1 - self._gamma) + self._gamma
        self._failures = self._failures * (1 - self._gamma) + self._gamma
        self._steps += 1

        self._successes[action] += reward
        self._failures[action] += 1.0 - reward

        self._probs = np.random.beta(self._successes, self._failures)
</code></pre><p><img loading=lazy src=images/drift.png alt></p><p>We can see from the plot how the reward probabilities change over time.</p><p><img loading=lazy src=images/epsilon_ucb.png alt></p><p>UCB1 shines in a changing environment because of its ability to adapt. As the distribution of rewards changes over time, UCB1 continues to explore arms with uncertain estimates, preventing it from getting stuck on a suboptimal arm.</p><h2 id=thompson-sampling>Thompson Sampling</h2><p>Unlike the UCB1 algorithm, Thompson Sampling incorporates the actual distribution of rewards by sampling from a Beta distribution for each action. The Beta distribution is a flexible choice, as it is defined on the interval $[0, 1]$, making it suitable for representing probabilities.</p><p>In each iteration, the algorithm samples from a Beta distribution for each available action. These samples provide estimates of the true success probability for each action. The algorithm then selects the action with the highest sampled value. This approach allows Thompson Sampling to adapt to the true underlying distribution of rewards and make more informed decisions over time.</p><pre><code class=language-python>class ThompsonSamplingAgent(AbstractAgent):
    def get_action(self):
        return np.argmax(np.random.beta(self._successes + 1, self._failures + 1))
</code></pre><p>From these comparison plots, we can see that Thompson Sampling performs really well compared to epsilon-greedy and UCB1.</p><p><img loading=lazy src=images/all_static.png alt></p><p>In a static environment, the algorithm continuously refines its probability distributions based on observed outcomes. As it converges to the true underlying distribution, the algorithm becomes adept at exploiting the arm with the highest expected reward.</p><p><img loading=lazy src=images/all_drift.png alt></p><p>In a dynamic environment, its ability to update beliefs in a Bayesian manner allows it to swiftly adapt to changes in the reward distribution.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://outlierblog.me/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://outlierblog.me/tags/exploration-exploitation/>exploration-exploitation</a></li></ul><nav class=paginav><a class=next href=https://outlierblog.me/posts/key_concepts_rl/><span class=title>Next »</span><br><span>Key Concepts In (Deep) Reinforcement Learning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>