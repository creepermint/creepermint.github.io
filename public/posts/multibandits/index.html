<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Multi-Armed Bandit Problem and Its Solutions | Outlier Blog</title><meta name=keywords content="reinforcement-learning,exploration-exploitation"><meta name=description content="In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.
Imagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one."><meta name=author content="Thomas Shaw"><link rel=canonical href=https://outlierblog.me/posts/multibandits/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css integrity="sha256-v1+fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://outlierblog.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://outlierblog.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://outlierblog.me/favicon-32x32.png><link rel=apple-touch-icon href=https://outlierblog.me/apple-touch-icon.png><link rel=mask-icon href=https://outlierblog.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/javascript>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js integrity=sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-018FG8S7HC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-018FG8S7HC",{anonymize_ip:!1})}</script><meta property="og:title" content="Multi-Armed Bandit Problem and Its Solutions"><meta property="og:description" content="In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.
Imagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one."><meta property="og:type" content="article"><meta property="og:url" content="https://outlierblog.me/posts/multibandits/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-02T14:21:44+08:00"><meta property="article:modified_time" content="2023-08-02T14:21:44+08:00"><meta property="og:site_name" content="Outlier Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Multi-Armed Bandit Problem and Its Solutions"><meta name=twitter:description content="In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.
Imagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://outlierblog.me/posts/"},{"@type":"ListItem","position":2,"name":"Multi-Armed Bandit Problem and Its Solutions","item":"https://outlierblog.me/posts/multibandits/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Multi-Armed Bandit Problem and Its Solutions","name":"Multi-Armed Bandit Problem and Its Solutions","description":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.\nImagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one.","keywords":["reinforcement-learning","exploration-exploitation"],"articleBody":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.\nImagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one. In this setup, each machine provides a random reward from an unknown probability distribution. The primary objective of the gambler (or agent) is to maximize the total reward obtained over a series of plays.\nAt each trial, the agent faces a crucial tradeoff: exploitation or exploration. In exploration, the agent tries out different arms to gather information about their rewards and estimate their true potential. The more exploration is done, the better the agent can learn about each arm’s payoffs. In contrast, exploitation involves making decisions based on the current knowledge to choose the arm expected to yield the highest reward.\nEffectively balancing exploration and exploitation is the key challenge to maximize cumulative rewards over time. If the agent exploits too much, it may miss out on higher-reward arms. On the other hand, excessive exploration could lead to missed opportunities to gain higher rewards from known better arms.\nTo address the multi-armed bandit problem, various strategies and algorithms have been developed, including epsilon-greedy, optimistic initialization, upper confidence bound (UCB), Thompson sampling, and gradient bandit methods. Each of these approaches aims to tackle the exploration-exploitation dilemma and optimize the agent’s decision-making.\nEpsilon-Greedy The action value is estimated according to past experience by averaging the rewards associated with the target action $a$ that we have observed so far (up to the current time step t).\n$$ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^t r_\\tau \\mathbb{1}[a_\\tau = a] $$\nwhere $\\mathbb{1}$ is a binary indicator function and $N_t(a)$ is how many times the action a has been selected so far, $N_t(a) = \\sum_{\\tau=1}^t \\mathbb{1}[a_\\tau = a]$.\nThe simplest action that the agent can make is to select one of the actions with the highest estimated value, that is, one of the greedy actions defined as:\n$$ A_t = \\arg \\max_a \\hat{Q}_t(a) $$\nThe greedy action prioritizes exploiting existing knowledge to achieve the maximum immediate reward and does not invest any time in sampling other actions to explore if they could yield better results.\nOne simple alternative to this is to take greedy action most of the time, but with a small probability (say $\\epsilon$), select random action instead. This method is known as the epsilon-greedy method.\n$$ A_t = \\begin{cases} \\arg \\max_a \\hat{Q}_t(a) \u0026 \\text{with probability 1 - } \\epsilon \\\\ \\text{a random action} \u0026 \\text{with probability } \\epsilon \\ \\end{cases} $$\nOptimistic Initialization The idea behind this method is to initialize the estimated action value, $Q_0(a)$ to a high value, higher than their actual estimated action value. The agent then updates action value by incremental averaging, starting with $N_0(a) \\ge 0$ for all $a \\in \\mathcal{A}$,\n$$ \\hat{Q}_t(A_t) = \\hat{Q}_{t-1}(A_t) + {1 \\over N_t(a)}(r_t - \\hat{Q}_{t-1}(A_t)), \\text{ and} $$ $$ \\hat{Q}_t(a) = \\hat{Q}_{t-1}(a) \\text{ for all } a \\ne A_t $$\nThis method promotes systematic exploration in the initial stages. When the agent chooses actions initially, the rewards received are lower than the starting estimates. Consequently, the agent switches to other actions as it becomes “disappointed” with the received rewards. This leads to repeated trials of all actions before the value estimates eventually converge.\nUpper Confidence Bound Exploration is important because we are not always sure how accurate our action-value estimates are. Greedy method choose an action that looks best at time $t$, but there might be better options among the other actions. Epsilon-greedy forces us to try non-greedy actions, but it does so randomly, without considering which non-greedy actions are almost as good as the greedy ones or which ones have more uncertainty.\nA better approach would be to choose non-greedy actions based on their potential to be the best choices. This means considering how close their estimates are to the highest possible values and how uncertain those estimates are.\nOne effective way of doing this is to select actions according to:\n$$ A_t = \\arg \\max_a [Q_t(a) + c \\sqrt{\\ln t \\over N_t(a)}] $$\nwhere $\\ln t$ denotes the natural logarithm of t, and the number $c \\gt 0$ control degree of exploration.\nThe square-root term is a measure of the uncertainty in the estimate of action $a$’s value. Each time action $a$ selected, the uncertainty decrease and, conversely, each time action other than $a$ is selected, the uncertainty increase. The use of the natural logarithm means that the increments become progressively smaller over time, but they have no upper limit. As a result, all actions will eventually be chosen, but actions with lower value estimates or those that have been frequently selected before will be picked less frequently as time goes on.\nThompson Sampling Thompson sampling model the uncertainty in the reward distributions of arms using probability distributions, particularly the Beta distribution. Each arm is associated with a Beta distribution that represents the agent’s belief or uncertainty about the true mean reward of that arm.\nThe algorithm work as follow:\nFor each arm, initialize the parameters of the Beta distribution based on some prior belief. The choice of prior can influence the algorithm’s behavior, but common choices are uniform or optimistic priors. For example, we can set α = 1 and β = 1, where we expect the reward probability to be 50% but we are not very confident.\nAt each time step, sample an expected reward from each arm’s $\\text{Beta}(\\alpha_i, \\beta_i)$ distribution independently. the best action is selected among samples: $a_t = \\arg\\max \\tilde{Q}(a)$\nThe sampled reward is used to update the parameters of the corresponding Beta distribution for that arm, incorporating the new information.\n$$ \\alpha_i \\leftarrow \\alpha_i + r_t \\mathbb{1}[a_t = a_i] $$ $$ \\beta_i \\leftarrow \\beta_i + (1-r_t) \\mathbb{1}[a_t = a_i] $$\nContinue the process by selecting arms, pulling, observing rewards, and updating the Beta distributions at each time step.\nThompson sampling implements the idea of probability matching. Because its reward estimations $\\tilde{Q}$ are sampled from posterior distributions, each of these probabilities is equivalent to the probability that the corresponding action is optimal, conditioned on observed history.\nGradient Bandit Algorithm So far, we have explored methods that estimate action values and use those estimates to select actions. Another approach is to consider learning a numerical preference for each action $a$, which we denote as $H_t(a)$. A higher preference value corresponds to a more frequent selection of the action, but the preference itself does not hold any direct reward interpretation.\nInitially, all action preferences are set equally, ensuring that all actions have an equal probability of being chosen. After each step (where action $A_t$ is selected, and reward $R_t$ is received), the action preferences are updated by:\n$$ H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R_t})(1 - \\pi_t(A_t)), \\text{ and} $$ $$ H_{t+1}(a) = H_t(a) - \\alpha(R_t, -\\bar{R_t})\\pi_t(a),\\text{ for all } \\ a \\ne A_t $$\nwhere $\\pi_t(a) = {e^{H_t(a)} \\over \\sum_{b=1}^ke^{H_t(b)}}$ is the probability of taking action $a$ at time $t$, $\\alpha \\gt 0$ is a step-size parameter, and $\\bar{R_t} \\in \\mathbb{R}$ is the average of all the rewards up to but not including time $t$.\nIf the reward ($R_t$) is higher than the baseline ($\\bar{R_t}$), the probability of taking $A_t$ in the future increase and vice versa. The non-selected actions move in the opposite direction.\nReferences [1] RL Course by David Silver - Lecture 9: Exploration and Exploitation\n[2] Stanford CME 241 slide - Multi-Armed Bandits: Exploration versus Exploitation\n[3] Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto\n[4] Lilian Weng - The Multi-Armed Bandit Problem and Its Solutions\n","wordCount":"1322","inLanguage":"en","datePublished":"2023-08-02T14:21:44+08:00","dateModified":"2023-08-02T14:21:44+08:00","author":{"@type":"Person","name":"Thomas Shaw"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://outlierblog.me/posts/multibandits/"},"publisher":{"@type":"Organization","name":"Outlier Blog","logo":{"@type":"ImageObject","url":"https://outlierblog.me/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://outlierblog.me/ accesskey=h title="Outlier Blog (Alt + H)"><img src=https://outlierblog.me/apple-touch-icon.png alt aria-label=logo height=35>Outlier Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://outlierblog.me/ title=Posts><span>Posts</span></a></li><li><a href=https://outlierblog.me/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://outlierblog.me/notebooks/ title=Notebooks><span>Notebooks</span></a></li><li><a href=https://outlierblog.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://outlierblog.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Multi-Armed Bandit Problem and Its Solutions</h1><div class=post-meta><span title='2023-08-02 14:21:44 +0800 +08'>August 2, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Thomas Shaw</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#epsilon-greedy>Epsilon-Greedy</a></li><li><a href=#optimistic-initialization>Optimistic Initialization</a></li><li><a href=#upper-confidence-bound>Upper Confidence Bound</a></li><li><a href=#thompson-sampling>Thompson Sampling</a></li><li><a href=#gradient-bandit-algorithm>Gradient Bandit Algorithm</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.</p><p>Imagine a gambler facing a row of slot machines (also called <a href=https://en.wiktionary.org/wiki/one-armed_bandit>one-armed bandits</a>). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one. In this setup, each machine provides a random reward from an unknown probability distribution. The primary objective of the gambler (or agent) is to maximize the total reward obtained over a series of plays.</p><p>At each trial, the agent faces a crucial tradeoff: exploitation or exploration. In exploration, the agent tries out different arms to gather information about their rewards and estimate their true potential. The more exploration is done, the better the agent can learn about each arm&rsquo;s payoffs. In contrast, exploitation involves making decisions based on the current knowledge to choose the arm expected to yield the highest reward.</p><p>Effectively balancing exploration and exploitation is the key challenge to maximize cumulative rewards over time. If the agent exploits too much, it may miss out on higher-reward arms. On the other hand, excessive exploration could lead to missed opportunities to gain higher rewards from known better arms.</p><p>To address the multi-armed bandit problem, various strategies and algorithms have been developed, including epsilon-greedy, optimistic initialization, upper confidence bound (UCB), Thompson sampling, and gradient bandit methods. Each of these approaches aims to tackle the exploration-exploitation dilemma and optimize the agent&rsquo;s decision-making.</p><h2 id=epsilon-greedy>Epsilon-Greedy</h2><p>The action value is estimated according to past experience by averaging the rewards associated with the target action $a$ that we have observed so far (up to the current time step t).</p><p>$$
\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \mathbb{1}[a_\tau = a]
$$</p><p>where $\mathbb{1}$ is a binary indicator function and $N_t(a)$ is how many times the action a has been selected so far, $N_t(a) = \sum_{\tau=1}^t \mathbb{1}[a_\tau = a]$.</p><p>The simplest action that the agent can make is to select one of the actions with the highest estimated value, that is, one of the greedy actions defined as:</p><p>$$
A_t = \arg \max_a \hat{Q}_t(a)
$$</p><p>The greedy action prioritizes exploiting existing knowledge to achieve the maximum immediate reward and does not invest any time in sampling other actions to explore if they could yield better results.</p><p>One simple alternative to this is to take greedy action most of the time, but with a small probability (say $\epsilon$), select random action instead. This method is known as the epsilon-greedy method.</p><p>$$
A_t = \begin{cases}
\arg \max_a \hat{Q}_t(a) & \text{with probability 1 - } \epsilon \\
\text{a random action} & \text{with probability } \epsilon \
\end{cases}
$$</p><h2 id=optimistic-initialization>Optimistic Initialization</h2><p>The idea behind this method is to initialize the estimated action value, $Q_0(a)$ to a high value, higher than their actual estimated action value. The agent then updates action value by incremental averaging, starting with $N_0(a) \ge 0$ for all $a \in \mathcal{A}$,</p><p>$$
\hat{Q}_t(A_t) = \hat{Q}_{t-1}(A_t) + {1 \over N_t(a)}(r_t - \hat{Q}_{t-1}(A_t)), \text{ and}
$$
$$
\hat{Q}_t(a) = \hat{Q}_{t-1}(a) \text{ for all } a \ne A_t
$$</p><p>This method promotes systematic exploration in the initial stages. When the agent chooses actions initially, the rewards received are lower than the starting estimates. Consequently, the agent switches to other actions as it becomes &ldquo;disappointed&rdquo; with the received rewards. This leads to repeated trials of all actions before the value estimates eventually converge.</p><h2 id=upper-confidence-bound>Upper Confidence Bound</h2><p>Exploration is important because we are not always sure how accurate our action-value estimates are. Greedy method choose an action that looks best at time $t$, but there might be better options among the other actions. Epsilon-greedy forces us to try non-greedy actions, but it does so randomly, without considering which non-greedy actions are almost as good as the greedy ones or which ones have more uncertainty.</p><p>A better approach would be to choose non-greedy actions based on their potential to be the best choices. This means considering how close their estimates are to the highest possible values and how uncertain those estimates are.</p><p>One effective way of doing this is to select actions according to:</p><p>$$
A_t = \arg \max_a [Q_t(a) + c \sqrt{\ln t \over N_t(a)}]
$$</p><p>where $\ln t$ denotes the natural logarithm of t, and the number $c \gt 0$ control degree of exploration.</p><p>The square-root term is a measure of the uncertainty in the estimate of action $a$’s value. Each time action $a$ selected, the uncertainty decrease and, conversely, each time action other than $a$ is selected, the uncertainty increase. The use of the natural logarithm means that the increments become progressively smaller over time, but they have no upper limit. As a result, all actions will eventually be chosen, but actions with lower value estimates or those that have been frequently selected before will be picked less frequently as time goes on.</p><h2 id=thompson-sampling>Thompson Sampling</h2><p>Thompson sampling model the uncertainty in the reward distributions of arms using probability distributions, particularly the Beta distribution. Each arm is associated with a Beta distribution that represents the agent&rsquo;s belief or uncertainty about the true mean reward of that arm.</p><p>The algorithm work as follow:</p><ol><li><p>For each arm, initialize the parameters of the Beta distribution based on some prior belief. The choice of prior can influence the algorithm&rsquo;s behavior, but common choices are uniform or optimistic priors. For example, we can set α = 1 and β = 1, where we expect the reward probability to be 50% but we are not very confident.</p></li><li><p>At each time step, sample an expected reward from each arm&rsquo;s $\text{Beta}(\alpha_i, \beta_i)$ distribution independently. the best action is selected among samples: $a_t = \arg\max \tilde{Q}(a)$</p></li><li><p>The sampled reward is used to update the parameters of the corresponding Beta distribution for that arm, incorporating the new information.</p><p>$$
\alpha_i \leftarrow \alpha_i + r_t \mathbb{1}[a_t = a_i]
$$
$$
\beta_i \leftarrow \beta_i + (1-r_t) \mathbb{1}[a_t = a_i]
$$</p></li><li><p>Continue the process by selecting arms, pulling, observing rewards, and updating the Beta distributions at each time step.</p></li></ol><p>Thompson sampling implements the idea of <a href=https://en.wikipedia.org/wiki/Probability_matching>probability matching</a>. Because its reward estimations $\tilde{Q}$ are sampled from posterior distributions, each of these probabilities is equivalent to the probability that the corresponding action is optimal, conditioned on observed history.</p><h2 id=gradient-bandit-algorithm>Gradient Bandit Algorithm</h2><p>So far, we have explored methods that estimate action values and use those estimates to select actions. Another approach is to consider learning a numerical preference for each action $a$, which we denote as $H_t(a)$. A higher preference value corresponds to a more frequent selection of the action, but the preference itself does not hold any direct reward interpretation.</p><p>Initially, all action preferences are set equally, ensuring that all actions have an equal probability of being chosen. After each step (where action $A_t$ is selected, and reward $R_t$ is received), the action preferences are updated by:</p><p>$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R_t})(1 - \pi_t(A_t)), \text{ and}
$$
$$
H_{t+1}(a) = H_t(a) - \alpha(R_t, -\bar{R_t})\pi_t(a),\text{ for all } \ a \ne A_t
$$</p><p>where $\pi_t(a) = {e^{H_t(a)} \over \sum_{b=1}^ke^{H_t(b)}}$ is the probability of taking action $a$ at time $t$, $\alpha \gt 0$ is a step-size parameter, and $\bar{R_t} \in \mathbb{R}$ is the average of all the rewards up to but not including time $t$.</p><p>If the reward ($R_t$) is higher than the baseline ($\bar{R_t}$), the probability of taking $A_t$ in the future increase and vice versa. The non-selected actions move in the opposite direction.</p><h2 id=references>References</h2><p>[1] RL Course by David Silver - Lecture 9: <a href=https://youtu.be/sGuiWX07sKw>Exploration and Exploitation</a></p><p>[2] Stanford CME 241 slide - <a href=https://stanford.edu/~ashlearn/RLForFinanceBook/MultiArmedBandits.pdf>Multi-Armed Bandits: Exploration versus Exploitation</a></p><p>[3] Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</p><p>[4] Lilian Weng - <a href=https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/>The Multi-Armed Bandit Problem and Its Solutions</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://outlierblog.me/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://outlierblog.me/tags/exploration-exploitation/>exploration-exploitation</a></li></ul><nav class=paginav><a class=prev href=https://outlierblog.me/posts/monte_carlo_method/><span class=title>« Prev</span><br><span>Monte Carlo Method</span></a>
<a class=next href=https://outlierblog.me/posts/key_concepts_rl/><span class=title>Next »</span><br><span>Key Concepts In (Deep) Reinforcement Learning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://outlierblog.me/>Outlier Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>