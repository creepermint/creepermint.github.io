<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>exploration-exploitation on Outlier Blog</title>
    <link>https://outlierblog.me/tags/exploration-exploitation/</link>
    <description>Recent content in exploration-exploitation on Outlier Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 02 Aug 2023 14:21:44 +0800</lastBuildDate><atom:link href="https://outlierblog.me/tags/exploration-exploitation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Multi-Armed Bandit Problem and Its Solutions</title>
      <link>https://outlierblog.me/posts/multibandits/</link>
      <pubDate>Wed, 02 Aug 2023 14:21:44 +0800</pubDate>
      
      <guid>https://outlierblog.me/posts/multibandits/</guid>
      <description>In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma.
Imagine a gambler facing a row of slot machines (also called one-armed bandits). The gambler must make a series of decisions: which machines to play, how many times to play each machine, the order in which to play them, and whether to stick with the current machine or switch to another one.</description>
    </item>
    
  </channel>
</rss>
