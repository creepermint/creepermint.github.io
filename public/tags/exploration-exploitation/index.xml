<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>exploration-exploitation on Outlier Blog</title>
    <link>https://outlierblog.me/tags/exploration-exploitation/</link>
    <description>Recent content in exploration-exploitation on Outlier Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 08 Oct 2023 14:21:44 +0800</lastBuildDate><atom:link href="https://outlierblog.me/tags/exploration-exploitation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Multi-Armed Bandit Problem and Its Solutions</title>
      <link>https://outlierblog.me/posts/multibandits/</link>
      <pubDate>Sun, 08 Oct 2023 14:21:44 +0800</pubDate>
      
      <guid>https://outlierblog.me/posts/multibandits/</guid>
      <description>In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.
Imagine we are facing a row of slot machines (also calledÂ one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one.</description>
    </item>
    
  </channel>
</rss>
